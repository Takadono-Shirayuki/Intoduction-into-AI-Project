{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c8b7dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các thư viện cần thiết\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces import Discrete\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "604b9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các hằng số sử dụng\n",
    "class Action:\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "class Maze:\n",
    "    WALL = -1\n",
    "    PATH = 0\n",
    "    UNEXPLORED = 1\n",
    "    GOAL = 2\n",
    "    AGENT_POSITION = 3\n",
    "class Buff:\n",
    "    NONE = 0\n",
    "    SENRIGAN = 1\n",
    "    TOU_NO_HIKARI = 2\n",
    "    SLIME_SAN_ONEGAI = 3\n",
    "    UNMEI_NO_MICHI = 4\n",
    "class Debuff:\n",
    "    NONE = 0\n",
    "    WAAMU_HOURU = 1\n",
    "    SHIN_NO_MEIRO = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b796e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo tên model\n",
    "while True:\n",
    "    model_name = input(\"Enter model name: \")\n",
    "    if model_name == \"\":\n",
    "        print(\"Model name cannot be empty. Please enter a valid name.\")\n",
    "    else:\n",
    "        os.makedirs(model_name, exist_ok=True)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404baae-6398-4dd8-9e8c-74c0e6c92fc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Môi trường Mê cung\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    # Môi trường mê cung\n",
    "    # Maze: Mê cung được tạo ra ngẫu nhiên với các ô đường và tường\n",
    "    # Với các ô đường được đánh dấu là 0 và các ô tường được đánh dấu là -1, ô đích được đánh dấu là 2\n",
    "    # Discovered_maze: Mê cung đã được khám phá trong quá trình di chuyển của tác tử\n",
    "    # Với các ô đường được đánh dấu là 0, các ô tường được đánh dấu là -5, đích được đánh dấu là 10, ô chưa được khám phá được đánh dấu là 1\n",
    "    # Giá trị các ô đường giảm dần mỗi lần tác tử đi qua\n",
    "    # Agent_position: Vị trí hiện tại của tác tử trong mê cung\n",
    "    # Base_position: Vị trí ban đầu của tác tử trong mê cung\n",
    "    # Goal_position: Vị trí đích trong mê cung\n",
    "    # Buff: Biến để xác định xem tác tử có đang sử dụng buff senrigan hay không\n",
    "    # Debuff: Biến để xác định xem tác tử có đang bị debuff shin no meiro hay không\n",
    "\n",
    "    def __init__(self, maze_size, local_obs_size = 3, max_steps = 15, path_percent = 70):\n",
    "        \"\"\"\n",
    "        Khởi tạo môi trường Mê cung.\n",
    "\n",
    "        Args:\n",
    "        - maze_size (int): Kích thước của mê cung (ví dụ: 50x50).\n",
    "        - max_steps (int): Số bước tối đa cho mỗi tập.\n",
    "        - path_percent (int): Tỷ lệ phần trăm ô đường trong mê cung (0-100).\n",
    "        \"\"\"\n",
    "        # Đặt các thông số của môi trường\n",
    "        super(MazeEnv, self).__init__()  # Kế thừa từ gym.Env\n",
    "        \n",
    "        self.maze_size = maze_size\n",
    "        self.max_steps = max_steps\n",
    "        self.path_percent = path_percent\n",
    "        self.local_obs_size = local_obs_size\n",
    "        \n",
    "        self.maze = np.ones((maze_size, maze_size), dtype=int)  # Mặc định là chưa biết (1)\n",
    "        self.goal_position = (maze_size * 5 // 6, maze_size * 5 // 6)  # Đích cố định tại giữa khu vực đích\n",
    "        self.senrigan_buff = False\n",
    "        self.shin_no_meiro_debuff = False\n",
    "\n",
    "        # Định nghĩa action_space (0: lên, 1: xuống, 2: trái, 3: phải)\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "    # Các phương thức liên quan đến tái tạo mê cung\n",
    "    def reset(self, buff = Buff.NONE, debuff = Debuff.NONE):\n",
    "        self.base_position = (self.maze_size // 6 - 1, self.maze_size // 6 - 1)\n",
    "        self.agent_position = self.base_position\n",
    "        return self.regenerate_maze(buff, debuff)\n",
    "    \n",
    "    def regenerate_maze(self, buff = Buff.NONE, debuff = Debuff.NONE):\n",
    "        # Kích hoạt buff\n",
    "        if buff == Buff.SENRIGAN:\n",
    "            self.senrigan_buff = True\n",
    "        else:\n",
    "            self.senrigan_buff = False\n",
    "        if buff == Buff.UNMEI_NO_MICHI:\n",
    "            adder = 1\n",
    "        else:\n",
    "            adder = 0\n",
    "        # Kích hoạt debuff\n",
    "        if debuff == Debuff.WAAMU_HOURU:\n",
    "            self.agent_position = (random.randint(self.maze_size // 3, self.maze_size), random.randint(self.maze_size // 3, self.maze_size))\n",
    "        if debuff == Debuff.SHIN_NO_MEIRO:\n",
    "            self.shin_no_meiro_debuff = True\n",
    "        else:\n",
    "            self.shin_no_meiro_debuff = False\n",
    "        # Tạo mê cung mới\n",
    "        self.generate_maze(adder)\n",
    "        self.create_discovered_maze()\n",
    "\n",
    "        # Kích hoạt buff\n",
    "        if buff == Buff.SLIME_SAN_ONEGAI:\n",
    "            self.bfs(self.agent_position, 50)\n",
    "        if buff == Buff.TOU_NO_HIKARI:\n",
    "            local_obs_size = 10\n",
    "        else:\n",
    "            local_obs_size = 3\n",
    "        self.discover_maze(local_obs_size)\n",
    "        return (self.get_observation(), self.discovered_maze, self.agent_position)\n",
    "\n",
    "    def generate_maze(self, adder = 0):\n",
    "        if self.agent_position[0] >= self.maze_size * 2 // 3 and self.agent_position[1] >= self.maze_size * 2 // 3:\n",
    "            number_of_path = 1 + adder\n",
    "        else:\n",
    "            number_of_path = 2 + adder\n",
    "\n",
    "        total_cells = self.maze_size * self.maze_size\n",
    "        num_paths = int(total_cells * self.path_percent / 100)\n",
    "        num_walls = total_cells - num_paths\n",
    "        maze_values = [Maze.PATH] * num_paths + [Maze.WALL] * num_walls\n",
    "\n",
    "        while True:  # Sử dụng vòng lặp để tạo lại mê cung nếu không hợp lệ\n",
    "            random.shuffle(maze_values)  # Xáo trộn các giá trị\n",
    "    \n",
    "            # Điền vào mê cung\n",
    "            self.maze = np.array(maze_values).reshape(self.maze_size, self.maze_size)\n",
    "    \n",
    "            # Đặt điểm bắt đầu và đích\n",
    "            self.maze[self.agent_position] = Maze.PATH # Đặt vị trí tác tử là đường \n",
    "            self.maze[self.goal_position] = Maze.GOAL # Đặt vị trí đích\n",
    "    \n",
    "            # Kiểm tra tính hợp lệ\n",
    "            if self.validate_maze(number_of_path):\n",
    "                break  # Nếu mê cung hợp lệ, thoát vòng lặp\n",
    "\n",
    "    def validate_maze(self, number_of_path = 0):\n",
    "        stack = [self.agent_position]\n",
    "        visited = set()\n",
    "        d = 0\n",
    "        \n",
    "        while stack:\n",
    "            x, y = stack.pop()\n",
    "            if self.shin_no_meiro_debuff:\n",
    "                if (x, y) == self.goal_position:\n",
    "                    return False\n",
    "            \n",
    "            if (x, y) == self.goal_position:\n",
    "                d += 1\n",
    "                if d == number_of_path:\n",
    "                    return True\n",
    "                continue\n",
    "            \n",
    "            if (x, y) in visited:\n",
    "                continue\n",
    "            visited.add((x, y))\n",
    "\n",
    "            # Thêm các ô lân cận vào stack\n",
    "            neighbors = self.get_neighbors(x, y)\n",
    "            for nx, ny in neighbors:\n",
    "                if self.maze[nx, ny] != Maze.WALL and (nx, ny) not in visited:  # Chỉ đi qua đường\n",
    "                    stack.append((nx, ny))\n",
    "                    \n",
    "        return self.shin_no_meiro_debuff  # Không có đường tới đích\n",
    "\n",
    "    def get_neighbors(self, x, y):\n",
    "        \"\"\"\n",
    "        Lấy danh sách các ô lân cận.\n",
    "        \"\"\"\n",
    "        neighbors = []\n",
    "        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.maze_size and 0 <= ny < self.maze_size:\n",
    "                neighbors.append((nx, ny))\n",
    "        return neighbors\n",
    "    \n",
    "    # Các phương thức liên quan đến mê cung được khám phá\n",
    "    def create_discovered_maze(self):\n",
    "        self.discovered_maze = np.zeros((self.maze_size + 10, self.maze_size + 10), dtype=int)  # Tạo mê cung đã khám phá\n",
    "        self.discovered_maze.fill(Maze.UNEXPLORED)  # Đánh dấu tất cả ô là chưa khám phá (1)\n",
    "        self.discovered_maze[0 : 5, :] = 5 * Maze.WALL\n",
    "        self.discovered_maze[:, 0 : 5] = 5 * Maze.WALL\n",
    "        self.discovered_maze[self.maze_size + 5 : self.maze_size + 10, :] = 5 * Maze.WALL\n",
    "        self.discovered_maze[:, self.maze_size + 5 : self.maze_size + 10] = 5 * Maze.WALL\n",
    "        self.discovered_maze[self.goal_position[0] + 5, self.goal_position[1] + 5] = 10  # Đích được đánh dấu là 10\n",
    "\n",
    "    def discover_maze(self, local_obs_size = 3):\n",
    "        x, y = self.agent_position\n",
    "        if self.senrigan_buff:\n",
    "            half_size = max(5, local_obs_size)  \n",
    "        else:\n",
    "            half_size = max(3, local_obs_size)  \n",
    "\n",
    "        # Xác định giới hạn của vùng quan sát trong mê cung\n",
    "        min_x = max(0, x - half_size)\n",
    "        max_x = min(self.maze_size, x + half_size + 1)\n",
    "        min_y = max(0, y - half_size)\n",
    "        max_y = min(self.maze_size, y + half_size + 1)\n",
    "\n",
    "        # Điền dữ liệu từ mê cung vào vùng khám phá\n",
    "        for x in range(min_x, max_x):\n",
    "            for y in range(min_y, max_y):\n",
    "                if self.discovered_maze[x + 5, y + 5] == Maze.UNEXPLORED:\n",
    "                    self.discovered_maze[x + 5, y + 5] = self.maze[x, y] * 5\n",
    "        \n",
    "    # Phương thức chính để thực hiện hành động trong môi trường\n",
    "    def step(self, action, take_action = True):\n",
    "        \"\"\"\n",
    "        Thực hiện hành động và cập nhật trạng thái của môi trường.\n",
    "    \n",
    "        Args:\n",
    "        - action (int): Hành động \n",
    "    \n",
    "        Returns:\n",
    "        - local_obs (np.array): Quan sát hiện tại của tác tử\n",
    "        - global_obs (np.array): Mê cung đã khám phá\n",
    "        - reward (float): Phần thưởng\n",
    "        - done (bool): Trạng thái kết thúc\n",
    "        \"\"\"\n",
    "        # Lưu lại trạng thái nếu không thực hiện hành động\n",
    "        if not take_action:\n",
    "            saved_position = self.agent_position\n",
    "            saved_discovered_maze = self.discovered_maze.copy()\n",
    "\n",
    "        # Lấy vị trí hiện tại của tác tử\n",
    "        x, y = self.agent_position\n",
    "    \n",
    "        # Xác định vị trí mới dựa trên hành động\n",
    "        if action == Action.UP:  # Lên\n",
    "            new_x, new_y = x - 1, y\n",
    "        elif action == Action.DOWN:  # Xuống\n",
    "            new_x, new_y = x + 1, y\n",
    "        elif action == Action.LEFT:  # Trái\n",
    "            new_x, new_y = x, y - 1\n",
    "        elif action == Action.RIGHT:  # Phải\n",
    "            new_x, new_y = x, y + 1\n",
    "    \n",
    "        # Khởi tạo biến phần thưởng\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Cập nhật vị trí tác tử\n",
    "        if self.valid_check((new_x, new_y)) and self.discovered_maze[new_x + 5, new_y + 5] != 5 * Maze.WALL:  # Nếu vị trí mới hợp lệ và không phải là tường\n",
    "            # Nếu vị trí mới hợp lệ, cập nhật vị trí tác tử\n",
    "            self.agent_position = (new_x, new_y)\n",
    "            self.discover_maze()\n",
    "\n",
    "        if self.discovered_maze[new_x + 5, new_y + 5] == 5 * Maze.WALL:\n",
    "            reward -= 2 * self.maze_size  # Phạt lớn khi va chạm với tường\n",
    "        elif self.agent_position == self.goal_position:\n",
    "            reward += 3 * self.maze_size  # Thưởng lớn khi đến đích\n",
    "            done = True\n",
    "        else:\n",
    "            # Tính khoảng cách Manhattan từ vị trí hiện tại tới đích\n",
    "            current_distance = abs(self.agent_position[0] - self.goal_position[0]) + abs(self.agent_position[1] - self.goal_position[1])\n",
    "            \n",
    "            # Tính khoảng cách trước đó\n",
    "            previous_distance = abs(x - self.goal_position[0]) + abs(y - self.goal_position[1])\n",
    "\n",
    "            # Tính khoảng cách đã đi được\n",
    "            moved_distance = 1 + abs(new_x - self.agent_position[0]) + abs(new_y - self.agent_position[1])\n",
    "\n",
    "            # Tăng thưởng nếu đến gần đích hơn\n",
    "            if self.discovered_maze[new_x + 5, new_y + 5] == Maze.PATH:  # Nếu ô chưa được khám phá\n",
    "                if current_distance < previous_distance:\n",
    "                    reward += 2 * self.maze_size // current_distance  # Thưởng khi di chuyển đến gần đích hơn\n",
    "                else:\n",
    "                    reward -= 2 * self.maze_size // moved_distance # Phạt nếu tác tử đi xa hơn\n",
    "            else:\n",
    "                # Phạt khi đi vào ô đã khám phá\n",
    "                reward += self.discovered_maze[new_x + 5, new_y + 5] * self.maze_size // current_distance + self.discovered_maze[new_x + 5, new_y + 5] * self.maze_size // moved_distance\n",
    "            \n",
    "            # Giảm giá trị ô đã khám phá\n",
    "            self.discovered_maze[new_x + 5, new_y + 5] -= 1\n",
    "        \n",
    "        # Tạo quan sát hiện tại\n",
    "        local_obs = self.get_observation()\n",
    "\n",
    "        #Tạo quan sát toàn mê cung\n",
    "        return_discovered_maze = self.discovered_maze.copy()\n",
    "        return_agent_position = self.agent_position\n",
    "        # Trả lại các giá trị đã lưu nếu không thực hiện hành động\n",
    "        if not take_action:\n",
    "            self.agent_position = saved_position\n",
    "            self.discovered_maze = saved_discovered_maze.copy()\n",
    "        return local_obs, return_discovered_maze, return_agent_position, reward, done\n",
    "\n",
    "    # Xuất dữ liệu mê cung\n",
    "    def render(self):\n",
    "        render_maze = np.zeros((self.maze_size, self.maze_size), dtype = int) \n",
    "        for i in range(self.maze_size):\n",
    "            for j in range(self.maze_size):\n",
    "                if self.maze[i][j] == Maze.WALL:\n",
    "                    render_maze[i][j] = 1\n",
    "                    continue\n",
    "                if self.maze[i][j] == Maze.GOAL:\n",
    "                    render_maze[i][j] = 10\n",
    "                    continue\n",
    "                if self.discovered_maze[i + 5, j + 5] < Maze.PATH:\n",
    "                    render_maze[i][j] = 2\n",
    "        print(render_maze)\n",
    "    \n",
    "    # Phương thức để lấy quan sát hiện tại của tác tử\n",
    "    def get_observation(self):\n",
    "        x, y = self.agent_position\n",
    "        observation = np.zeros((2 * self.local_obs_size + 1, 2*self.local_obs_size + 1), dtype = int)\n",
    "        observation[:, :] = self.discovered_maze[x + 5 - self.local_obs_size: x + 6 + self.local_obs_size, y + 5 - self.local_obs_size : y + 6 + self.local_obs_size]\n",
    "        return observation\n",
    "\n",
    "    def bfs(self, position, step = 50):\n",
    "        \"\"\"\n",
    "        Thuật toán BFS (Breadth-First Search)\n",
    "    \n",
    "        Args:\n",
    "        - position: Đỉnh bắt đầu tìm kiếm.\n",
    "        - step: số bược di chuyển\n",
    "    \n",
    "        \"\"\"\n",
    "        # Tập các đỉnh đã duyệt\n",
    "        visited = set()\n",
    "        visited_counter = 0\n",
    "    \n",
    "        # Hàng đợi (FIFO) để quản lý các đỉnh\n",
    "        queue = deque([position])\n",
    "    \n",
    "        # Bắt đầu duyệt đồ thị\n",
    "        while queue:\n",
    "            # Lấy một đỉnh từ hàng đợi\n",
    "            current = queue.popleft()\n",
    "            x,y = current\n",
    "            \n",
    "            # Kiểm tra nếu đỉnh chưa được duyệt\n",
    "            if current not in visited:\n",
    "                visited.add(current)\n",
    "                visited_counter += 1\n",
    "                if self.discovered_maze[x + 5, y + 5] == Maze.UNEXPLORED:\n",
    "                    self.discovered_maze[5 + x, 5 + y] = 5 * self.maze[x, y]\n",
    "                    \n",
    "                # Nếu đã duyệt đủ số bước, dừng lại\n",
    "                if visited_counter >= step:\n",
    "                    break\n",
    "                neighbors = self.get_neighbors(x, y)\n",
    "                for nx, ny in neighbors:\n",
    "                    if self.maze[nx, ny] != Maze.WALL and (nx, ny) not in visited:  # Chỉ đi qua đường\n",
    "                        queue.append((nx, ny))\n",
    "    # Phương thức để kiểm tra tính hợp lệ của một vị trí\n",
    "    def valid_check(self, p1):\n",
    "        if 0 <= p1[0] < self.maze_size and 0 <= p1[1] < self.maze_size and self.maze[p1] != Maze.WALL:\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000311c-41eb-4e8c-9b70-d640e978f328",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Mạng Nơ ron \n",
    "\n",
    "class MazeNetCombined(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MazeNetCombined, self).__init__()\n",
    "        \n",
    "        # Quan sát cục bộ\n",
    "        self.conv1_local = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2_local = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3_local = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4_local = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully Connected cho vị trí hiện tại\n",
    "        self.fc_position = nn.Linear(2, 32)\n",
    "\n",
    "        # Tầng Fully Connected cuối cùng\n",
    "        self.fc1 = nn.LazyLinear(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout_fc = nn.Dropout(p=0.5)  # Dropout trước tầng FC3\n",
    "        self.fc3 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, local_obs,    , position):\n",
    "        # Xử lý local_obs\n",
    "        x_local = F.relu(self.conv1_local(local_obs))\n",
    "        x_local = F.relu(self.conv2_local(x_local))\n",
    "        x_local = F.relu(self.conv3_local(x_local))\n",
    "        x_local = F.relu(self.conv4_local(x_local))\n",
    "        x_local = x_local.view(x_local.size(0), -1)\n",
    "        \n",
    "        # Xử lý vị trí hiện tại\n",
    "        x_position = F.relu(self.fc_position(position))\n",
    "\n",
    "        # Kết hợp tất cả\n",
    "        x = torch.cat((x_local, x_position), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout_fc(x)  # Dropout trước FC3\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c3e95ff-7bb2-422c-9292-206a310eb14f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "\n",
    "    def push(self, experience):\n",
    "        \"\"\"Thêm một trải nghiệm (local_obs, global_obs, position, action, reward, next_local_obs, next_global_obs, next_position, done) vào replay buffer.\"\"\"\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)  # Loại bỏ trải nghiệm cũ nhất nếu đầy\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Trích xuất batch mẫu từ replay buffer.\"\"\"\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        # Tách dữ liệu thành các phần riêng biệt\n",
    "        local_obs, global_obs, position, actions, rewards, next_local_obs, next_global_obs, next_position, dones = zip(*batch)\n",
    "        return (np.array(local_obs), np.array(global_obs), np.array(position), \n",
    "                np.array(actions), np.array(rewards), np.array(next_local_obs), \n",
    "                np.array(next_global_obs), np.array(next_position), np.array(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def extend(self, buffer):\n",
    "        \"\"\"Mở rộng replay buffer bằng cách thêm một buffer khác.\"\"\"\n",
    "        if len(self.buffer) + len(buffer.buffer) > self.capacity:\n",
    "            self.capacity = len(self.buffer) + len(buffer.buffer)\n",
    "        self.buffer.extend(buffer.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7c6359cf-4e38-4694-8f99-cb823c37e1c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cập nhật model\n",
    "def update_model(policy_net, target_net, replay_buffer, optimizer, batch_size, gamma, device):\n",
    "    \"\"\"\n",
    "    Cập nhật mô hình chính (policy network) cho mạng có đầu vào đa dạng (local_obs, global_obs, position).\n",
    "\n",
    "    Args:\n",
    "    - policy_net (nn.Module): Mạng chính dự đoán giá trị Q(s, a).\n",
    "    - target_net (nn.Module): Mạng mục tiêu dùng để tính Q_target.\n",
    "    - replay_buffer (ReplayBuffer): Bộ nhớ replay chứa các kinh nghiệm (local_obs, global_obs, position, action, reward, next_local_obs, next_global_obs, next_position, done).\n",
    "    - optimizer (torch.optim.Optimizer): Trình tối ưu hóa (Adam, SGD, ...).\n",
    "    - batch_size (int): Kích thước batch mẫu từ replay buffer.\n",
    "    - gamma (float): Hệ số chiết khấu (discount factor).\n",
    "    - device (torch.device): Thiết bị thực thi (CPU hoặc GPU).\n",
    "\n",
    "    Returns:\n",
    "    - loss (float): Giá trị mất mát (loss) sau khi cập nhật.\n",
    "    \"\"\"\n",
    "    # Kiểm tra nếu replay buffer chưa đủ dữ liệu\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # 1. Lấy mẫu từ replay buffer\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    (local_obs, global_obs, position, actions, rewards, \n",
    "     next_local_obs, next_global_obs, next_position, dones) = batch\n",
    "\n",
    "    # 2. Chuyển đổi dữ liệu sang Tensor và đưa vào thiết bị (CPU/GPU)\n",
    "    local_obs = torch.tensor(local_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh cho CNN\n",
    "    global_obs = torch.tensor(global_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh\n",
    "    position = torch.tensor(position, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_local_obs = torch.tensor(next_local_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh\n",
    "    next_global_obs = torch.tensor(next_global_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh\n",
    "    next_position = torch.tensor(next_position, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 3. Dự đoán Q(s, a) từ policy_net\n",
    "    q_values = policy_net(local_obs, global_obs, position)  # Đầu ra: (batch_size, num_actions)\n",
    "    q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Lấy giá trị Q(s, a) cho hành động đã thực hiện\n",
    "\n",
    "    # 4. Tính toán Q_target bằng target_net\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_local_obs, next_global_obs, next_position)  # Dự đoán Q(s', a') từ target_net\n",
    "        max_next_q_values = next_q_values.max(1)[0]  # Lấy giá trị lớn nhất Q(s', a')\n",
    "        q_targets = rewards + gamma * max_next_q_values * (1 - dones)  # Hàm Bellman\n",
    "\n",
    "    # 5. Tính hàm mất mát   \n",
    "    loss = F.mse_loss(q_values, q_targets)\n",
    "\n",
    "    # 6. Tối ưu hóa mô hình\n",
    "    optimizer.zero_grad()  # Xóa gradient cũ\n",
    "    loss.backward()  # Lan truyền ngược (backpropagation)\n",
    "    optimizer.step()  # Cập nhật trọng số\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "682b6dba-bdca-4cc4-a4fb-b21725ba7e30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Chọn hành động\n",
    "def select_action(env, policy_net, local_obs, global_obs, position, device):\n",
    "    \"\"\"\n",
    "    Chọn hành động dựa trên chiến lược Boltzmann Exploration.\n",
    "\n",
    "    Args:\n",
    "    - policy_net (nn.Module): Mạng chính để dự đoán giá trị Q(s, a).\n",
    "    - local_obs (np.array): Quan sát cục bộ (ví dụ: 11x11).\n",
    "    - global_obs (np.array): Quan sát toàn bộ mê cung (ví dụ: 50x50).\n",
    "    - position (list or np.array): Vị trí hiện tại của tác tử (dx, dy).\n",
    "    - device (torch.device): Thiết bị thực thi (CPU hoặc GPU).\n",
    "\n",
    "    Returns:\n",
    "    - action (int): Hành động được chọn (0, 1, 2, 3).\n",
    "    \"\"\"\n",
    "    p1 = (position[0] - 1, position[1])\n",
    "    p2 = (position[0] + 1, position[1])\n",
    "    p3 = (position[0], position[1] - 1)\n",
    "    p4 = (position[0], position[1] + 1)\n",
    "\n",
    "    # Loại bỏ các hành động không hợp lệ (nếu cần)\n",
    "    valid_actions = []\n",
    "    if env.valid_check(p1): valid_actions.append(Action.UP)  # Lên\n",
    "    if env.valid_check(p2): valid_actions.append(Action.DOWN)  # Xuống\n",
    "    if env.valid_check(p3): valid_actions.append(Action.LEFT)  # Trái\n",
    "    if env.valid_check(p4): valid_actions.append(Action.RIGHT)  # Phải\n",
    "\n",
    "    # Chuyển đổi các quan sát thành Tensor để đưa vào mạng\n",
    "    local_obs_tensor = torch.tensor(local_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    global_obs_tensor = torch.tensor(global_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    position_tensor = torch.tensor(position, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # Dự đoán giá trị Q(s, a) cho tất cả các hành động\n",
    "    q_values = policy_net(local_obs_tensor, global_obs_tensor, position_tensor)\n",
    "\n",
    "    valid_q_values = q_values.squeeze()[valid_actions]  # Chỉ giữ Q của các hành động hợp lệ\n",
    "    selected_index = valid_q_values.argmax().item()  # Chọn hành động có giá trị Q lớn nhất\n",
    "    action = valid_actions[selected_index]\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0b4b6527-7fe7-4c1b-bcbc-894d259e3afd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Đồng bộ với mạng mục tiêu\n",
    "def sync_target_network(q_network, target_network):\n",
    "    target_network.load_state_dict(q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b5a797c7-f96e-4e97-a03b-824daaae83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu, tải trọng số của mô hình\n",
    "\n",
    "# Lưu trọng số của mô hình\n",
    "def save_model(model, path):\n",
    "    torch.save(model, path)\n",
    "    print(f\"Model saved to: {path}\")\n",
    "# Tải trọng số của mô hình\n",
    "def load_model(path, device = \"cuda\"):\n",
    "    policy_model = torch.load(path, map_location=device, weights_only=False)  # Tải mô hình từ file\n",
    "    target_model = policy_model\n",
    "    target_model.eval() # Đặt target_model ở chế độ đánh giá\n",
    "    print(F\"Model loaded from: {path}\")\n",
    "    return policy_model, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "83b2070f-2f24-41e0-bd60-d400df0fbe81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Model loaded from: model01/model.pth\n",
      "Initialization complete!\n",
      "Bot info file created!\n",
      "Model info file created!\n",
      "Training info file created!\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo các siêu tham số\n",
    "\n",
    "# Thiết bị thực thi (CPU hoặc GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.85  # Hệ số chiết khấu (discount factor)\n",
    "learning_rate = 1e-3  # Learning rate cho optimizer\n",
    "weight_decay = 1e-3 # weight decay cho optimizer\n",
    "batch_size = 64  # Kích thước batch khi lấy mẫu từ replay buffer\n",
    "target_update_frequency = 100  # Số vòng lặp huấn luyện trước khi đồng bộ target_net với policy_net\n",
    "max_episodes = 10000  # Số lượng tập (episodes) tối đa\n",
    "replay_buffer_capacity = 5000  # Dung lượng bộ nhớ replay buffer\n",
    "\n",
    "# Thông tin về môi trường\n",
    "maze_size = 30 # Kích thước mê cung (30x30)\n",
    "max_steps = 15  # Số bước tối đa trong mỗi tập (episode)\n",
    "path_percent = 70  # Tỷ lệ phần trăm ô đường đi trong mê cung (70% đường đi, 30% tường)\n",
    "buff = Buff.NONE  # Buff cho tác tử (nếu có)\n",
    "\n",
    "# Mạng chính và mạng mục tiêu\n",
    "local_obs_size = 3\n",
    "\n",
    "load = True and os.path.exists(model_name + \"/model.pth\")\n",
    "# Nếu có trọng số đã lưu, tải chúng vào mô hình\n",
    "if load:\n",
    "    policy_net, target_net = load_model(model_name + \"/model.pth\", device)\n",
    "else:\n",
    "    # Khởi tạo mạng chính (policy_net) và mạng mục tiêu (target_net)\n",
    "    policy_net = MazeNetCombined().to(device)\n",
    "    target_net = MazeNetCombined().to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())  # Đồng bộ hóa trọng số ban đầu\n",
    "    target_net.eval()  # Đặt target_net ở chế độ đánh giá\n",
    "\n",
    "replay_buffer = ReplayBuffer(capacity=replay_buffer_capacity)\n",
    "\n",
    "save_rb = ReplayBuffer(capacity=10000)  # Lưu replay buffer để huấn luyện sau này\n",
    "save_rate = 0.15  # Tỷ lệ lưu replay buffer (15%)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "print(\"Initialization complete!\")\n",
    "\n",
    "# Lưu thông tin các siêu tham số vào file\n",
    "with open(model_name + \"/bot.info\", \"w\") as file:\n",
    "    file.write(\"Maze Environment Configuration:\\n\")\n",
    "    file.write(f\"maze_size: {maze_size}\\n\")\n",
    "    file.write(f\"max_steps: {max_steps}\\n\")\n",
    "    file.write(f\"path_percent: {path_percent}\\n\")\n",
    "    file.write(f\"buff: {buff}\\n\")\n",
    "    \n",
    "    file.write(\"\\nNeural Network Configuration:\\n\")\n",
    "    file.write(f\"local_obs_size: {local_obs_size}\\n\")\n",
    "\n",
    "    file.write(\"\\nTraining Configuration:\\n\")\n",
    "    file.write(f\"gamma: {gamma}\\n\")\n",
    "    file.write(f\"learning_rate: {learning_rate}\\n\")\n",
    "    file.write(f\"weight_decay: {weight_decay}\\n\")\n",
    "    file.write(f\"batch_size: {batch_size}\\n\")\n",
    "    file.write(f\"target_update_frequency: {target_update_frequency}\\n\")\n",
    "    file.write(f\"max_episodes: {max_episodes}\\n\")\n",
    "    file.write(f\"replay_buffer_capacity: {replay_buffer_capacity}\\n\")\n",
    "\n",
    "    file.write(\"\\nReplay Buffer Configuration:\\n\")\n",
    "    file.write(f\"save_rate: {save_rate}\\n\")\n",
    "print(\"Bot info file created!\")\n",
    "\n",
    "# Tạo model info\n",
    "model_info = {}\n",
    "model_info['model_path'] = model_name + \"/model.pth\"\n",
    "model_info['buff'] = buff\n",
    "\n",
    "with open(model_name + \"/model_info.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model_info, file)\n",
    "print(\"Model info file created!\")\n",
    "\n",
    "# Tạo training_info\n",
    "training_info = {}\n",
    "training_info['gamma'] = gamma\n",
    "training_info['learning_rate'] = learning_rate\n",
    "training_info['weight_decay'] = weight_decay\n",
    "\n",
    "with open(model_name + \"/training_info.pkl\", \"wb\") as file:\n",
    "    pickle.dump(training_info, file)\n",
    "print(\"Training info file created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9db2b6af-b40a-41ae-81d5-8f390bc365ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done counter: 1\n",
      "Number of steps: 54\n",
      "Done counter: 2\n",
      "Number of steps: 76\n",
      "Done counter: 3\n",
      "Number of steps: 42\n",
      "Done counter: 4\n",
      "Number of steps: 48\n",
      "Done counter: 5\n",
      "Number of steps: 66\n",
      "Done counter: 6\n",
      "Number of steps: 80\n",
      "Done counter: 7\n",
      "Number of steps: 52\n",
      "Done counter: 8\n",
      "Number of steps: 50\n",
      "Done counter: 9\n",
      "Number of steps: 70\n",
      "Done counter: 10\n",
      "Number of steps: 42\n",
      "Done counter: 11\n",
      "Number of steps: 60\n",
      "Done counter: 12\n",
      "Number of steps: 46\n",
      "Done counter: 13\n",
      "Number of steps: 74\n",
      "Done counter: 14\n",
      "Number of steps: 42\n",
      "Done counter: 15\n",
      "Number of steps: 46\n",
      "Done counter: 16\n",
      "Number of steps: 66\n",
      "Done counter: 17\n",
      "Number of steps: 48\n",
      "Done counter: 18\n",
      "Number of steps: 44\n",
      "Done counter: 19\n",
      "Number of steps: 44\n",
      "Done counter: 20\n",
      "Number of steps: 68\n",
      "Done counter: 21\n",
      "Number of steps: 44\n",
      "Done counter: 22\n",
      "Number of steps: 60\n",
      "Done counter: 23\n",
      "Number of steps: 48\n",
      "Done counter: 24\n",
      "Number of steps: 48\n",
      "Done counter: 25\n",
      "Number of steps: 52\n",
      "Done counter: 26\n",
      "Number of steps: 46\n",
      "Done counter: 27\n",
      "Number of steps: 60\n",
      "Done counter: 28\n",
      "Number of steps: 74\n",
      "Done counter: 29\n",
      "Number of steps: 74\n",
      "Done counter: 30\n",
      "Number of steps: 50\n",
      "Done counter: 31\n",
      "Number of steps: 60\n",
      "Done counter: 32\n",
      "Number of steps: 48\n",
      "Done counter: 33\n",
      "Number of steps: 48\n",
      "Done counter: 34\n",
      "Number of steps: 56\n",
      "Done counter: 35\n",
      "Number of steps: 54\n",
      "Done counter: 36\n",
      "Number of steps: 54\n",
      "Done counter: 37\n",
      "Number of steps: 48\n",
      "Done counter: 38\n",
      "Number of steps: 54\n",
      "Done counter: 39\n",
      "Number of steps: 66\n",
      "Done counter: 40\n",
      "Number of steps: 46\n",
      "Done counter: 41\n",
      "Number of steps: 52\n",
      "Done counter: 42\n",
      "Number of steps: 44\n",
      "Done counter: 43\n",
      "Number of steps: 56\n",
      "Done counter: 44\n",
      "Number of steps: 74\n",
      "Done counter: 45\n",
      "Number of steps: 56\n",
      "Done counter: 46\n",
      "Number of steps: 66\n",
      "Done counter: 47\n",
      "Number of steps: 58\n",
      "Done counter: 48\n",
      "Number of steps: 46\n",
      "Done counter: 49\n",
      "Number of steps: 54\n",
      "Done counter: 50\n",
      "Number of steps: 64\n",
      "Done counter: 51\n",
      "Number of steps: 46\n",
      "Done counter: 52\n",
      "Number of steps: 76\n",
      "Done counter: 53\n",
      "Number of steps: 48\n",
      "Done counter: 54\n",
      "Number of steps: 46\n",
      "Done counter: 55\n",
      "Number of steps: 66\n",
      "Done counter: 56\n",
      "Number of steps: 92\n",
      "Done counter: 57\n",
      "Number of steps: 42\n",
      "Done counter: 58\n",
      "Number of steps: 64\n",
      "Done counter: 59\n",
      "Number of steps: 62\n",
      "Done counter: 60\n",
      "Number of steps: 66\n",
      "Done counter: 61\n",
      "Number of steps: 80\n",
      "Done counter: 62\n",
      "Number of steps: 52\n",
      "Done counter: 63\n",
      "Number of steps: 126\n",
      "Done counter: 64\n",
      "Number of steps: 80\n",
      "Done counter: 65\n",
      "Number of steps: 46\n",
      "Done counter: 66\n",
      "Number of steps: 44\n",
      "Done counter: 67\n",
      "Number of steps: 64\n",
      "Done counter: 68\n",
      "Number of steps: 46\n",
      "Done counter: 69\n",
      "Number of steps: 50\n",
      "Done counter: 70\n",
      "Number of steps: 42\n",
      "Done counter: 71\n",
      "Number of steps: 44\n",
      "Done counter: 72\n",
      "Number of steps: 60\n",
      "Done counter: 73\n",
      "Number of steps: 44\n",
      "Done counter: 74\n",
      "Number of steps: 82\n",
      "Done counter: 75\n",
      "Number of steps: 54\n",
      "Done counter: 76\n",
      "Number of steps: 42\n",
      "Done counter: 77\n",
      "Number of steps: 42\n",
      "Done counter: 78\n",
      "Number of steps: 64\n",
      "Done counter: 79\n",
      "Number of steps: 50\n",
      "Done counter: 80\n",
      "Number of steps: 44\n",
      "Done counter: 81\n",
      "Number of steps: 62\n",
      "Done counter: 82\n",
      "Number of steps: 44\n",
      "Done counter: 83\n",
      "Number of steps: 52\n",
      "Done counter: 84\n",
      "Number of steps: 58\n",
      "Done counter: 85\n",
      "Number of steps: 44\n",
      "Done counter: 86\n",
      "Number of steps: 46\n",
      "Done counter: 87\n",
      "Number of steps: 50\n",
      "Done counter: 88\n",
      "Number of steps: 42\n",
      "Done counter: 89\n",
      "Number of steps: 46\n",
      "Done counter: 90\n",
      "Number of steps: 46\n",
      "Done counter: 91\n",
      "Number of steps: 42\n",
      "Done counter: 92\n",
      "Number of steps: 48\n",
      "Done counter: 93\n",
      "Number of steps: 44\n",
      "Done counter: 94\n",
      "Number of steps: 46\n",
      "Done counter: 95\n",
      "Number of steps: 44\n",
      "Done counter: 96\n",
      "Number of steps: 62\n",
      "Done counter: 97\n",
      "Number of steps: 56\n",
      "Done counter: 98\n",
      "Number of steps: 44\n",
      "Done counter: 99\n",
      "Number of steps: 44\n",
      "Done counter: 100\n",
      "Number of steps: 60\n",
      "Done counter: 101\n",
      "Number of steps: 44\n",
      "Done counter: 102\n",
      "Number of steps: 48\n",
      "Done counter: 103\n",
      "Number of steps: 56\n",
      "Done counter: 104\n",
      "Number of steps: 46\n",
      "Done counter: 105\n",
      "Number of steps: 44\n",
      "Done counter: 106\n",
      "Number of steps: 64\n",
      "Done counter: 107\n",
      "Number of steps: 66\n",
      "Done counter: 108\n",
      "Number of steps: 52\n",
      "Done counter: 109\n",
      "Number of steps: 46\n",
      "Done counter: 110\n",
      "Number of steps: 70\n",
      "Done counter: 111\n",
      "Number of steps: 72\n",
      "Done counter: 112\n",
      "Number of steps: 66\n",
      "Done counter: 113\n",
      "Number of steps: 46\n",
      "Done counter: 114\n",
      "Number of steps: 46\n",
      "Done counter: 115\n",
      "Number of steps: 64\n",
      "Done counter: 116\n",
      "Number of steps: 54\n",
      "Done counter: 117\n",
      "Number of steps: 56\n",
      "Done counter: 118\n",
      "Number of steps: 56\n",
      "Done counter: 119\n",
      "Number of steps: 64\n",
      "Done counter: 120\n",
      "Number of steps: 46\n",
      "Done counter: 121\n",
      "Number of steps: 62\n",
      "Done counter: 122\n",
      "Number of steps: 42\n",
      "Done counter: 123\n",
      "Number of steps: 50\n",
      "Done counter: 124\n",
      "Number of steps: 50\n",
      "Done counter: 125\n",
      "Number of steps: 48\n",
      "Done counter: 126\n",
      "Number of steps: 52\n",
      "Done counter: 127\n",
      "Number of steps: 44\n",
      "Done counter: 128\n",
      "Number of steps: 44\n",
      "Done counter: 129\n",
      "Number of steps: 50\n",
      "Done counter: 130\n",
      "Number of steps: 56\n",
      "Done counter: 131\n",
      "Number of steps: 48\n",
      "Done counter: 132\n",
      "Number of steps: 44\n",
      "Done counter: 133\n",
      "Number of steps: 46\n",
      "Done counter: 134\n",
      "Number of steps: 54\n",
      "Done counter: 135\n",
      "Number of steps: 44\n",
      "Done counter: 136\n",
      "Number of steps: 48\n",
      "Done counter: 137\n",
      "Number of steps: 54\n",
      "Done counter: 138\n",
      "Number of steps: 60\n",
      "Done counter: 139\n",
      "Number of steps: 44\n",
      "Done counter: 140\n",
      "Number of steps: 56\n",
      "Done counter: 141\n",
      "Number of steps: 48\n",
      "Done counter: 142\n",
      "Number of steps: 80\n",
      "Done counter: 143\n",
      "Number of steps: 44\n",
      "Done counter: 144\n",
      "Number of steps: 62\n",
      "Done counter: 145\n",
      "Number of steps: 46\n",
      "Done counter: 146\n",
      "Number of steps: 50\n",
      "Done counter: 147\n",
      "Number of steps: 52\n",
      "Done counter: 148\n",
      "Number of steps: 56\n",
      "Done counter: 149\n",
      "Number of steps: 54\n",
      "Done counter: 150\n",
      "Number of steps: 78\n",
      "Done counter: 151\n",
      "Number of steps: 50\n",
      "Done counter: 152\n",
      "Number of steps: 46\n",
      "Done counter: 153\n",
      "Number of steps: 52\n",
      "Done counter: 154\n",
      "Number of steps: 50\n",
      "Done counter: 155\n",
      "Number of steps: 58\n",
      "Done counter: 156\n",
      "Number of steps: 58\n",
      "Done counter: 157\n",
      "Number of steps: 86\n",
      "Done counter: 158\n",
      "Number of steps: 46\n",
      "Done counter: 159\n",
      "Number of steps: 42\n",
      "Done counter: 160\n",
      "Number of steps: 46\n",
      "Done counter: 161\n",
      "Number of steps: 56\n",
      "Done counter: 162\n",
      "Number of steps: 44\n",
      "Done counter: 163\n",
      "Number of steps: 44\n",
      "Done counter: 164\n",
      "Number of steps: 70\n",
      "Done counter: 165\n",
      "Number of steps: 54\n",
      "Done counter: 166\n",
      "Number of steps: 44\n",
      "Done counter: 167\n",
      "Number of steps: 48\n",
      "Done counter: 168\n",
      "Number of steps: 54\n",
      "Done counter: 169\n",
      "Number of steps: 42\n",
      "Done counter: 170\n",
      "Number of steps: 48\n",
      "Done counter: 171\n",
      "Number of steps: 84\n",
      "Done counter: 172\n",
      "Number of steps: 52\n",
      "Done counter: 173\n",
      "Number of steps: 42\n",
      "Done counter: 174\n",
      "Number of steps: 64\n",
      "Done counter: 175\n",
      "Number of steps: 46\n",
      "Done counter: 176\n",
      "Number of steps: 80\n",
      "Done counter: 177\n",
      "Number of steps: 50\n",
      "Done counter: 178\n",
      "Number of steps: 50\n",
      "Done counter: 179\n",
      "Number of steps: 52\n",
      "Done counter: 180\n",
      "Number of steps: 42\n",
      "Done counter: 181\n",
      "Number of steps: 68\n",
      "Done counter: 182\n",
      "Number of steps: 42\n",
      "Done counter: 183\n",
      "Number of steps: 52\n",
      "Done counter: 184\n",
      "Number of steps: 44\n",
      "average number of steps: 54\n",
      "max: 126\n",
      "min: 42\n",
      "Model saved to: model01/model.pth\n"
     ]
    }
   ],
   "source": [
    "# Huấn luyện mô hình\n",
    "\n",
    "#Khởi tạo môi trường \n",
    "counter = 0\n",
    "step_min = 1000\n",
    "step_max = 0\n",
    "done_count = 0\n",
    "done_statistical = []\n",
    "env = MazeEnv(maze_size=maze_size, local_obs_size=local_obs_size, max_steps=max_steps, path_percent=path_percent)\n",
    "env.reset()\n",
    "# Vòng lặp huấn luyện\n",
    "for episode in range(max_episodes):\n",
    "    if counter % env.max_steps == 0:\n",
    "        # Khởi tạo trạng thái môi trường\n",
    "        local_obs, global_obs, position = env.regenerate_maze(buff)\n",
    "    counter += 1\n",
    "    \n",
    "    # Chọn hành động\n",
    "    action = select_action(env, policy_net, local_obs, global_obs, position, device)\n",
    "\n",
    "    # Tạo buffer cho các hành động\n",
    "    for i in range(4):\n",
    "        next_local_obs, next_global_obs, next_position, reward, d = env.step(i, (action==i))\n",
    "\n",
    "        # Lưu trải nghiệm vào replay buffer\n",
    "        replay_buffer.push((local_obs, global_obs, position, i, reward,\n",
    "                            next_local_obs, next_global_obs, next_position, d))\n",
    "\n",
    "        # Lưu trải nghiệm vào replay buffer để huấn luyện sau này\n",
    "        if np.random.rand() < save_rate:\n",
    "            save_rb.push((local_obs, global_obs, position, i, reward,\n",
    "                        next_local_obs, next_global_obs, next_position, d))\n",
    "        \n",
    "        # Cập nhật trạng thái\n",
    "        if i == action:\n",
    "            local_obs, global_obs, position = next_local_obs, next_global_obs, next_position\n",
    "            done = d\n",
    "\n",
    "    # Huấn luyện mô hình nếu buffer đủ dữ liệu\n",
    "    if len(replay_buffer) >= batch_size:\n",
    "        update_model(policy_net, target_net, replay_buffer, optimizer, batch_size, gamma, device)\n",
    "\n",
    "    # reset môi trường nếu đạt được mục tiêu\n",
    "    if done:\n",
    "        done_count += 1\n",
    "        print(f\"Done counter: {done_count}\")\n",
    "        print(f\"Number of steps: {counter}\")\n",
    "        if counter > step_max:\n",
    "            step_max = counter\n",
    "        if counter < step_min: \n",
    "            step_min = counter\n",
    "        done_statistical.append(counter)\n",
    "        counter = 0\n",
    "        local_obs, global_obs, position = env.reset(buff)\n",
    "    # Đồng bộ target_net định kỳ\n",
    "    if (episode + 1) % target_update_frequency == 0:\n",
    "        sync_target_network(policy_net, target_net)\n",
    "\n",
    "print(f\"average number of steps: {(max_episodes - counter) // done_count}\")\n",
    "print(f\"max: {step_max}\")\n",
    "print(f\"min: {step_min}\")\n",
    "save_model(policy_net, model_name + \"/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer was saved to model01/replay_buffer.pkl\n",
      "Replay buffer size: 6161\n",
      "Replay buffer was cleared\n"
     ]
    }
   ],
   "source": [
    "# Lưu replay buffer\n",
    "def save_replay_buffer(replay_buffer, filename=\"replay_buffer.pkl\"):\n",
    "    \"\"\"\n",
    "    Lưu replay buffer vào tệp.\n",
    "\n",
    "    Args:\n",
    "    - replay_buffer (ReplayBuffer): Replay buffer cần lưu.\n",
    "    - filename (str): Tên tệp để lưu replay buffer.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    with open(filename, 'ab') as f:\n",
    "        pickle.dump(replay_buffer, f)\n",
    "        print(f\"Replay buffer was saved to {filename}\")\n",
    "        print(f\"Replay buffer size: {len(replay_buffer)}\")\n",
    "        replay_buffer.buffer = []\n",
    "        print(\"Replay buffer was cleared\")\n",
    "save_replay_buffer(save_rb, filename=model_name + \"/replay_buffer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb40e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hiện biểu đồ thống kê\n",
    "done_statistical = Counter(done_statistical)\n",
    "values = list(done_statistical.keys())  # Các giá trị duy nhất\n",
    "frequencies = list(done_statistical.values())\n",
    "\n",
    "# Vẽ biểu đồ cột\n",
    "plt.bar(values, frequencies, color=\"skyblue\")\n",
    "\n",
    "# Thêm nhãn và tiêu đề\n",
    "plt.xlabel(\"Số bước\")\n",
    "plt.ylabel(\"Tần suất\")\n",
    "plt.title(\"\")\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
