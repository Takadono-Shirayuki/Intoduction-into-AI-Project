{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a44f45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các thư viện cần thiết\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47fa6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo tên meta model\n",
    "while True:\n",
    "    model_name = input(\"Enter meta model name: \")\n",
    "    if model_name.strip() != \"\":\n",
    "        break\n",
    "    else:\n",
    "        print(\"Model name cannot be empty. Please enter a valid name.\")\n",
    "os.makedirs(model_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e08c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lớp TrainingData \n",
    "# Lớp này dùng để lưu trữ dữ liệu huấn luyện cho mô hình\n",
    "\n",
    "class TrainingData:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Khởi tạo lớp TrainingData với dữ liệu và nhãn.\n",
    "        \n",
    "        :param data: Dữ liệu huấn luyện (danh sách các mẫu).\n",
    "        :param labels: Nhãn tương ứng với dữ liệu (danh sách các nhãn).\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "    \n",
    "    def add(self, sample):\n",
    "        \"\"\"\n",
    "        Thêm một mẫu vào dữ liệu huấn luyện.\n",
    "        \n",
    "        :param sample: Mẫu dữ liệu cần thêm.\n",
    "        \"\"\"\n",
    "        self.data.append(sample)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Lấy một mẫu ngẫu nhiên từ dữ liệu huấn luyện với kích thước batch_size.\n",
    "        \n",
    "        :param batch_size: Kích thước của mẫu cần lấy.\n",
    "        :return: Một danh sách chứa các mẫu ngẫu nhiên.\n",
    "        \"\"\"\n",
    "        return random.sample(self.data, batch_size) if len(self.data) >= batch_size else self.data\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Trả về số lượng mẫu trong dữ liệu huấn luyện.\n",
    "        \n",
    "        :return: Số lượng mẫu trong dữ liệu.\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "440e09ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 752 samples from 1 classes.\n",
      "Saved models_info to meta_model01/models_info.pkl\n"
     ]
    }
   ],
   "source": [
    "# Nạp dữ liệu huấn luyện vào training_data\n",
    "training_data = TrainingData()\n",
    "\n",
    "label = 0  # Biến nhãn khởi tạo bằng 0\n",
    "models_info = []  # Danh sách chứa thông tin mô hình\n",
    "\n",
    "for model_folder in os.listdir():\n",
    "    if os.path.exists(model_folder + \"/model.pth\") and os.path.exists(model_folder + \"/model_info.pkl\") and os.path.exists(model_folder + \"/dataset\"):\n",
    "        with open(model_folder + \"/model_info.pkl\", 'rb') as f:\n",
    "            model_info = pickle.load(f) \n",
    "            models_info.append(model_info)\n",
    "        for data in os.listdir(model_folder + \"/dataset\"):\n",
    "            data_path = os.path.join(model_folder + \"/dataset\", data)\n",
    "            with open(data_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                data['label'] = label\n",
    "                training_data.add(data)\n",
    "        label += 1  # Tăng nhãn cho thư mục tiếp theo\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(f\"Loaded {len(training_data)} samples from {len(models_info)} classes.\")\n",
    "\n",
    "# Lưu models_info vào file\n",
    "with open(model_name + \"/models_info.pkl\", 'wb') as f:\n",
    "    pickle.dump(models_info, f)\n",
    "print(f\"Saved models_info to {model_name}/models_info.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e879767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mạng nơron\n",
    "\n",
    "class MetaModelNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Mạng nơron cho mô hình MetaModelNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, local_size, num_classes):\n",
    "        super(MetaModelNet, self).__init__()\n",
    "    \n",
    "        # Tầng CNN cho local_obs\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Tầng FC cho position\n",
    "        self.fc_position = nn.Linear(2, 32)\n",
    "\n",
    "        # Tầng FC cuối cùng\n",
    "        self.fc1 = nn.Linear(256 * local_size * local_size + 32, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        \n",
    "        # Dropout trước tầng FC cuối cùng\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # Tầng đầu ra\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, local_obs, position):\n",
    "        \"\"\"\n",
    "        Hàm truyền dữ liệu qua mạng nơron.\n",
    "        \n",
    "        :param local_obs: Dữ liệu đầu vào (local observations).\n",
    "        :param position: Vị trí của dữ liệu đầu vào.\n",
    "        :return: Kết quả đầu ra của mạng nơron.\n",
    "        \"\"\"\n",
    "        # Xử lý local_obs qua các tầng CNN\n",
    "        x_local = F.relu(self.conv1(local_obs))\n",
    "        x_local = F.relu(self.conv2(x_local))\n",
    "        x_local = F.relu(self.conv3(x_local))\n",
    "        x_local = F.relu(self.conv4(x_local))\n",
    "\n",
    "        # Chuyển đổi kích thước tensor\n",
    "        x_local = x_local.view(x_local.size(0), -1)  # Chuyển đổi thành vector 1 chiều\n",
    "\n",
    "        # Xử lý position qua tầng FC\n",
    "        x_position = F.relu(self.fc_position(position))\n",
    "\n",
    "        # Kết hợp dữ liệu từ local_obs và position\n",
    "        x = torch.cat((x_local, x_position), dim=1)\n",
    "\n",
    "        # Truyền qua các tầng FC\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bf5479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu, tải trọng số của mô hình\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Lưu trọng số của mô hình vào file.\n",
    "    \n",
    "    :param model: Mô hình cần lưu.\n",
    "    :param filename: Tên file để lưu trọng số.\n",
    "    \"\"\"\n",
    "    torch.save(model, filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Tải trọng số của mô hình từ file.\n",
    "    \n",
    "    :param model: Mô hình cần tải trọng số.\n",
    "    :param filename: Tên file chứa trọng số.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        model = torch.load(filename, map_location=device, weights_only=False)\n",
    "        print(f\"Model loaded from {filename}\")\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"File {filename} does not exist.\")\n",
    "        return MetaModelNet(local_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7dc439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded from meta_model01/meta_model.pth\n",
      "Initialization complete!\n",
      "Model configuration saved to meta_model01/meta_model.info\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo các siêu tham số\n",
    "\n",
    "local_size = 7  # Kích thước của local_obs\n",
    "num_classes = len(models_info)  # Số lớp (số nhãn)\n",
    "\n",
    "max_episodes = 1000  # Số lượng episode tối đa\n",
    "\n",
    "batch_size = 32  # Kích thước batch\n",
    "\n",
    "learning_rate = 0.001  # Tốc độ học\n",
    "weight_decay = 0.0001  # Tham số weight decay\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Kiểm tra xem có GPU hay không\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Khởi tạo mô hình, hàm mất mát và bộ tối ưu hóa\n",
    "model = load_model(model_name + \"/meta_model.pth\").to(device)  # Tải mô hình từ file\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "print(\"Initialization complete!\")\n",
    "\n",
    "# Lưu thông tin các tham số vào file\n",
    "with open(model_name + \"/meta_model.info\", 'w') as f:\n",
    "    f.write(f\"Hyperparameters:\\n\")\n",
    "    f.write(f\"Max episodes: {max_episodes}\\n\")\n",
    "    f.write(f\"Batch size: {batch_size}\\n\")\n",
    "    f.write(f\"Number of samples: {len(training_data)}\\n\")\n",
    "    f.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "    f.write(f\"Weight decay: {weight_decay}\\n\")\n",
    "    f.write(f\"Device: {device}\\n\")\n",
    "\n",
    "    f.write(f\"\\nModel configuration:\\n\")\n",
    "    f.write(f\"Local size: {local_size}\\n\")\n",
    "    f.write(f\"Number of classes: {num_classes}\\n\")\n",
    "\n",
    "print(\"Model configuration saved to \" + model_name + \"/meta_model.info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7385759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Loss: 1.2105677127838135\n",
      "Episode 2/1000, Loss: 6.6858649253845215\n",
      "Episode 3/1000, Loss: 1.6057493686676025\n",
      "Episode 4/1000, Loss: 1.862868309020996\n",
      "Episode 5/1000, Loss: 4.909915447235107\n",
      "Episode 6/1000, Loss: 2.425429344177246\n",
      "Episode 7/1000, Loss: 0.9840264320373535\n",
      "Episode 8/1000, Loss: 1.9969865083694458\n",
      "Episode 9/1000, Loss: 3.993884563446045\n",
      "Episode 10/1000, Loss: 2.935074806213379\n",
      "Episode 11/1000, Loss: 1.505255103111267\n",
      "Episode 12/1000, Loss: 0.8316145539283752\n",
      "Episode 13/1000, Loss: 2.8453547954559326\n",
      "Episode 14/1000, Loss: 2.3615403175354004\n",
      "Episode 15/1000, Loss: 1.7382316589355469\n",
      "Episode 16/1000, Loss: 0.9634455442428589\n",
      "Episode 17/1000, Loss: 1.0263638496398926\n",
      "Episode 18/1000, Loss: 1.213560938835144\n",
      "Episode 19/1000, Loss: 1.828571081161499\n",
      "Episode 20/1000, Loss: 2.012481212615967\n",
      "Episode 21/1000, Loss: 1.2321057319641113\n",
      "Episode 22/1000, Loss: 0.9586285352706909\n",
      "Episode 23/1000, Loss: 2.7903056144714355\n",
      "Episode 24/1000, Loss: 1.8464345932006836\n",
      "Episode 25/1000, Loss: 1.4315308332443237\n",
      "Episode 26/1000, Loss: 1.1681089401245117\n",
      "Episode 27/1000, Loss: 2.1929478645324707\n",
      "Episode 28/1000, Loss: 2.2801008224487305\n",
      "Episode 29/1000, Loss: 2.4649109840393066\n",
      "Episode 30/1000, Loss: 1.8824279308319092\n",
      "Episode 31/1000, Loss: 0.6341934204101562\n",
      "Episode 32/1000, Loss: 1.5618685483932495\n",
      "Episode 33/1000, Loss: 1.8223971128463745\n",
      "Episode 34/1000, Loss: 1.5755207538604736\n",
      "Episode 35/1000, Loss: 1.3519861698150635\n",
      "Episode 36/1000, Loss: 1.2133713960647583\n",
      "Episode 37/1000, Loss: 0.8046091794967651\n",
      "Episode 38/1000, Loss: 0.8675893545150757\n",
      "Episode 39/1000, Loss: 1.4278745651245117\n",
      "Episode 40/1000, Loss: 1.1233707666397095\n",
      "Episode 41/1000, Loss: 1.2100130319595337\n",
      "Episode 42/1000, Loss: 1.1224913597106934\n",
      "Episode 43/1000, Loss: 1.2836577892303467\n",
      "Episode 44/1000, Loss: 0.9824970960617065\n",
      "Episode 45/1000, Loss: 1.1708276271820068\n",
      "Episode 46/1000, Loss: 1.8068594932556152\n",
      "Episode 47/1000, Loss: 0.9914402961730957\n",
      "Episode 48/1000, Loss: 0.4594535231590271\n",
      "Episode 49/1000, Loss: 0.6313676834106445\n",
      "Episode 50/1000, Loss: 1.858096957206726\n",
      "Episode 51/1000, Loss: 0.9519544839859009\n",
      "Episode 52/1000, Loss: 1.6939095258712769\n",
      "Episode 53/1000, Loss: 0.7952297925949097\n",
      "Episode 54/1000, Loss: 1.0913264751434326\n",
      "Episode 55/1000, Loss: 0.8479770421981812\n",
      "Episode 56/1000, Loss: 1.2311755418777466\n",
      "Episode 57/1000, Loss: 0.8368206024169922\n",
      "Episode 58/1000, Loss: 1.0069535970687866\n",
      "Episode 59/1000, Loss: 1.7810564041137695\n",
      "Episode 60/1000, Loss: 1.005473017692566\n",
      "Episode 61/1000, Loss: 0.8103584051132202\n",
      "Episode 62/1000, Loss: 0.8523840308189392\n",
      "Episode 63/1000, Loss: 0.6418917775154114\n",
      "Episode 64/1000, Loss: 0.973098874092102\n",
      "Episode 65/1000, Loss: 0.9441999197006226\n",
      "Episode 66/1000, Loss: 0.7129635214805603\n",
      "Episode 67/1000, Loss: 0.8663188219070435\n",
      "Episode 68/1000, Loss: 1.1786247491836548\n",
      "Episode 69/1000, Loss: 2.15918231010437\n",
      "Episode 70/1000, Loss: 1.0114164352416992\n",
      "Episode 71/1000, Loss: 0.9196213483810425\n",
      "Episode 72/1000, Loss: 1.007077932357788\n",
      "Episode 73/1000, Loss: 1.7623279094696045\n",
      "Episode 74/1000, Loss: 1.4570720195770264\n",
      "Episode 75/1000, Loss: 1.2920347452163696\n",
      "Episode 76/1000, Loss: 0.837967574596405\n",
      "Episode 77/1000, Loss: 0.8865748643875122\n",
      "Episode 78/1000, Loss: 1.2944562435150146\n",
      "Episode 79/1000, Loss: 1.769865870475769\n",
      "Episode 80/1000, Loss: 1.42811918258667\n",
      "Episode 81/1000, Loss: 0.6363365650177002\n",
      "Episode 82/1000, Loss: 1.0198092460632324\n",
      "Episode 83/1000, Loss: 0.8593778610229492\n",
      "Episode 84/1000, Loss: 1.139832615852356\n",
      "Episode 85/1000, Loss: 0.9587675333023071\n",
      "Episode 86/1000, Loss: 0.5301038026809692\n",
      "Episode 87/1000, Loss: 0.8413872718811035\n",
      "Episode 88/1000, Loss: 1.0148706436157227\n",
      "Episode 89/1000, Loss: 1.4267709255218506\n",
      "Episode 90/1000, Loss: 0.9223867654800415\n",
      "Episode 91/1000, Loss: 1.0317630767822266\n",
      "Episode 92/1000, Loss: 0.7996338605880737\n",
      "Episode 93/1000, Loss: 1.56809401512146\n",
      "Episode 94/1000, Loss: 0.5336035490036011\n",
      "Episode 95/1000, Loss: 0.9379748106002808\n",
      "Episode 96/1000, Loss: 0.9321765303611755\n",
      "Episode 97/1000, Loss: 0.8330785036087036\n",
      "Episode 98/1000, Loss: 0.6273247599601746\n",
      "Episode 99/1000, Loss: 0.7352696657180786\n",
      "Episode 100/1000, Loss: 0.9028118848800659\n",
      "Episode 101/1000, Loss: 1.3657081127166748\n",
      "Episode 102/1000, Loss: 0.9475441575050354\n",
      "Episode 103/1000, Loss: 1.3992043733596802\n",
      "Episode 104/1000, Loss: 1.0065252780914307\n",
      "Episode 105/1000, Loss: 0.5892409086227417\n",
      "Episode 106/1000, Loss: 1.2653697729110718\n",
      "Episode 107/1000, Loss: 0.8152294158935547\n",
      "Episode 108/1000, Loss: 2.034775733947754\n",
      "Episode 109/1000, Loss: 1.8088304996490479\n",
      "Episode 110/1000, Loss: 1.4377217292785645\n",
      "Episode 111/1000, Loss: 1.5722167491912842\n",
      "Episode 112/1000, Loss: 1.205114722251892\n",
      "Episode 113/1000, Loss: 1.4287495613098145\n",
      "Episode 114/1000, Loss: 1.447688341140747\n",
      "Episode 115/1000, Loss: 1.068023920059204\n",
      "Episode 116/1000, Loss: 1.1284775733947754\n",
      "Episode 117/1000, Loss: 2.061530113220215\n",
      "Episode 118/1000, Loss: 0.988472580909729\n",
      "Episode 119/1000, Loss: 1.1305313110351562\n",
      "Episode 120/1000, Loss: 0.6793631315231323\n",
      "Episode 121/1000, Loss: 1.0039639472961426\n",
      "Episode 122/1000, Loss: 1.3067631721496582\n",
      "Episode 123/1000, Loss: 0.9813328981399536\n",
      "Episode 124/1000, Loss: 1.2729108333587646\n",
      "Episode 125/1000, Loss: 0.9835681915283203\n",
      "Episode 126/1000, Loss: 1.2464995384216309\n",
      "Episode 127/1000, Loss: 1.6382088661193848\n",
      "Episode 128/1000, Loss: 1.6825560331344604\n",
      "Episode 129/1000, Loss: 0.9338489770889282\n",
      "Episode 130/1000, Loss: 0.529183566570282\n",
      "Episode 131/1000, Loss: 1.3699665069580078\n",
      "Episode 132/1000, Loss: 2.0121161937713623\n",
      "Episode 133/1000, Loss: 0.8120795488357544\n",
      "Episode 134/1000, Loss: 0.773149847984314\n",
      "Episode 135/1000, Loss: 1.5119953155517578\n",
      "Episode 136/1000, Loss: 1.4430148601531982\n",
      "Episode 137/1000, Loss: 1.488274097442627\n",
      "Episode 138/1000, Loss: 1.185561180114746\n",
      "Episode 139/1000, Loss: 1.3402217626571655\n",
      "Episode 140/1000, Loss: 0.8876084089279175\n",
      "Episode 141/1000, Loss: 1.5323801040649414\n",
      "Episode 142/1000, Loss: 1.1281139850616455\n",
      "Episode 143/1000, Loss: 1.0622444152832031\n",
      "Episode 144/1000, Loss: 1.1187083721160889\n",
      "Episode 145/1000, Loss: 1.1179147958755493\n",
      "Episode 146/1000, Loss: 1.6384658813476562\n",
      "Episode 147/1000, Loss: 1.5751280784606934\n",
      "Episode 148/1000, Loss: 0.6830737590789795\n",
      "Episode 149/1000, Loss: 1.8387563228607178\n",
      "Episode 150/1000, Loss: 0.747382640838623\n",
      "Episode 151/1000, Loss: 1.2719740867614746\n",
      "Episode 152/1000, Loss: 1.3035972118377686\n",
      "Episode 153/1000, Loss: 1.0786564350128174\n",
      "Episode 154/1000, Loss: 0.8724758625030518\n",
      "Episode 155/1000, Loss: 1.326117753982544\n",
      "Episode 156/1000, Loss: 0.7570242285728455\n",
      "Episode 157/1000, Loss: 0.9727328419685364\n",
      "Episode 158/1000, Loss: 1.6440188884735107\n",
      "Episode 159/1000, Loss: 0.6556393504142761\n",
      "Episode 160/1000, Loss: 0.8313620090484619\n",
      "Episode 161/1000, Loss: 0.7380332946777344\n",
      "Episode 162/1000, Loss: 0.6465495228767395\n",
      "Episode 163/1000, Loss: 1.577846646308899\n",
      "Episode 164/1000, Loss: 0.7197045087814331\n",
      "Episode 165/1000, Loss: 0.9175493717193604\n",
      "Episode 166/1000, Loss: 0.7323046922683716\n",
      "Episode 167/1000, Loss: 0.8690628409385681\n",
      "Episode 168/1000, Loss: 1.6502705812454224\n",
      "Episode 169/1000, Loss: 0.909300684928894\n",
      "Episode 170/1000, Loss: 1.3615167140960693\n",
      "Episode 171/1000, Loss: 1.3771920204162598\n",
      "Episode 172/1000, Loss: 0.9794523119926453\n",
      "Episode 173/1000, Loss: 1.7471650838851929\n",
      "Episode 174/1000, Loss: 0.6928418278694153\n",
      "Episode 175/1000, Loss: 0.5225658416748047\n",
      "Episode 176/1000, Loss: 1.1207948923110962\n",
      "Episode 177/1000, Loss: 1.4405403137207031\n",
      "Episode 178/1000, Loss: 1.2626738548278809\n",
      "Episode 179/1000, Loss: 1.0880931615829468\n",
      "Episode 180/1000, Loss: 0.9607378244400024\n",
      "Episode 181/1000, Loss: 1.348226547241211\n",
      "Episode 182/1000, Loss: 1.200234055519104\n",
      "Episode 183/1000, Loss: 1.0817211866378784\n",
      "Episode 184/1000, Loss: 0.864092230796814\n",
      "Episode 185/1000, Loss: 1.6379096508026123\n",
      "Episode 186/1000, Loss: 1.4193717241287231\n",
      "Episode 187/1000, Loss: 1.7513048648834229\n",
      "Episode 188/1000, Loss: 0.8804930448532104\n",
      "Episode 189/1000, Loss: 0.7607296705245972\n",
      "Episode 190/1000, Loss: 2.0436151027679443\n",
      "Episode 191/1000, Loss: 1.4841176271438599\n",
      "Episode 192/1000, Loss: 0.6504804491996765\n",
      "Episode 193/1000, Loss: 1.5023893117904663\n",
      "Episode 194/1000, Loss: 1.4799766540527344\n",
      "Episode 195/1000, Loss: 0.8023239970207214\n",
      "Episode 196/1000, Loss: 1.1553304195404053\n",
      "Episode 197/1000, Loss: 0.7457060813903809\n",
      "Episode 198/1000, Loss: 1.53654944896698\n",
      "Episode 199/1000, Loss: 1.7072968482971191\n",
      "Episode 200/1000, Loss: 0.864374041557312\n",
      "Episode 201/1000, Loss: 0.7985587120056152\n",
      "Episode 202/1000, Loss: 1.0015138387680054\n",
      "Episode 203/1000, Loss: 1.6657986640930176\n",
      "Episode 204/1000, Loss: 1.3477907180786133\n",
      "Episode 205/1000, Loss: 0.6952803730964661\n",
      "Episode 206/1000, Loss: 0.8689526915550232\n",
      "Episode 207/1000, Loss: 1.0260499715805054\n",
      "Episode 208/1000, Loss: 0.6205878257751465\n",
      "Episode 209/1000, Loss: 1.2913285493850708\n",
      "Episode 210/1000, Loss: 1.3938859701156616\n",
      "Episode 211/1000, Loss: 0.994204044342041\n",
      "Episode 212/1000, Loss: 1.3736486434936523\n",
      "Episode 213/1000, Loss: 0.5579444766044617\n",
      "Episode 214/1000, Loss: 2.1541292667388916\n",
      "Episode 215/1000, Loss: 1.102314829826355\n",
      "Episode 216/1000, Loss: 0.7715708017349243\n",
      "Episode 217/1000, Loss: 0.8955427408218384\n",
      "Episode 218/1000, Loss: 0.9377015829086304\n",
      "Episode 219/1000, Loss: 1.0101325511932373\n",
      "Episode 220/1000, Loss: 1.7267159223556519\n",
      "Episode 221/1000, Loss: 1.1934189796447754\n",
      "Episode 222/1000, Loss: 0.371689110994339\n",
      "Episode 223/1000, Loss: 0.7144399881362915\n",
      "Episode 224/1000, Loss: 1.0876755714416504\n",
      "Episode 225/1000, Loss: 1.122406244277954\n",
      "Episode 226/1000, Loss: 0.8336063027381897\n",
      "Episode 227/1000, Loss: 1.0802897214889526\n",
      "Episode 228/1000, Loss: 1.5757873058319092\n",
      "Episode 229/1000, Loss: 0.6114424467086792\n",
      "Episode 230/1000, Loss: 1.0097150802612305\n",
      "Episode 231/1000, Loss: 0.7778927683830261\n",
      "Episode 232/1000, Loss: 1.405634880065918\n",
      "Episode 233/1000, Loss: 1.338263750076294\n",
      "Episode 234/1000, Loss: 0.6769882440567017\n",
      "Episode 235/1000, Loss: 0.512675404548645\n",
      "Episode 236/1000, Loss: 1.8174433708190918\n",
      "Episode 237/1000, Loss: 1.2841548919677734\n",
      "Episode 238/1000, Loss: 1.0019898414611816\n",
      "Episode 239/1000, Loss: 0.7512449026107788\n",
      "Episode 240/1000, Loss: 0.9446119070053101\n",
      "Episode 241/1000, Loss: 1.0805410146713257\n",
      "Episode 242/1000, Loss: 1.3340964317321777\n",
      "Episode 243/1000, Loss: 1.2883570194244385\n",
      "Episode 244/1000, Loss: 1.2893381118774414\n",
      "Episode 245/1000, Loss: 0.9605309367179871\n",
      "Episode 246/1000, Loss: 0.6336038112640381\n",
      "Episode 247/1000, Loss: 1.6772184371948242\n",
      "Episode 248/1000, Loss: 1.4840710163116455\n",
      "Episode 249/1000, Loss: 1.2207398414611816\n",
      "Episode 250/1000, Loss: 1.5172055959701538\n",
      "Episode 251/1000, Loss: 0.8834006190299988\n",
      "Episode 252/1000, Loss: 1.9011598825454712\n",
      "Episode 253/1000, Loss: 1.0864487886428833\n",
      "Episode 254/1000, Loss: 1.3622989654541016\n",
      "Episode 255/1000, Loss: 1.3764065504074097\n",
      "Episode 256/1000, Loss: 1.7061614990234375\n",
      "Episode 257/1000, Loss: 1.1823513507843018\n",
      "Episode 258/1000, Loss: 1.998151421546936\n",
      "Episode 259/1000, Loss: 1.3099095821380615\n",
      "Episode 260/1000, Loss: 0.9726948738098145\n",
      "Episode 261/1000, Loss: 1.129361867904663\n",
      "Episode 262/1000, Loss: 1.4501219987869263\n",
      "Episode 263/1000, Loss: 0.6631485819816589\n",
      "Episode 264/1000, Loss: 1.3699487447738647\n",
      "Episode 265/1000, Loss: 1.5274301767349243\n",
      "Episode 266/1000, Loss: 1.0987892150878906\n",
      "Episode 267/1000, Loss: 0.8430983424186707\n",
      "Episode 268/1000, Loss: 0.9473608732223511\n",
      "Episode 269/1000, Loss: 1.3418093919754028\n",
      "Episode 270/1000, Loss: 0.904081404209137\n",
      "Episode 271/1000, Loss: 1.2273004055023193\n",
      "Episode 272/1000, Loss: 1.0294817686080933\n",
      "Episode 273/1000, Loss: 1.1450607776641846\n",
      "Episode 274/1000, Loss: 0.5155482292175293\n",
      "Episode 275/1000, Loss: 1.0760481357574463\n",
      "Episode 276/1000, Loss: 1.1026463508605957\n",
      "Episode 277/1000, Loss: 0.8624964952468872\n",
      "Episode 278/1000, Loss: 0.9580110311508179\n",
      "Episode 279/1000, Loss: 1.3109164237976074\n",
      "Episode 280/1000, Loss: 0.9639002680778503\n",
      "Episode 281/1000, Loss: 1.6227914094924927\n",
      "Episode 282/1000, Loss: 1.4665822982788086\n",
      "Episode 283/1000, Loss: 0.7069371342658997\n",
      "Episode 284/1000, Loss: 1.2348637580871582\n",
      "Episode 285/1000, Loss: 0.5831907391548157\n",
      "Episode 286/1000, Loss: 0.5957655310630798\n",
      "Episode 287/1000, Loss: 0.8506648540496826\n",
      "Episode 288/1000, Loss: 1.2123149633407593\n",
      "Episode 289/1000, Loss: 1.3574199676513672\n",
      "Episode 290/1000, Loss: 0.9074535965919495\n",
      "Episode 291/1000, Loss: 1.1760317087173462\n",
      "Episode 292/1000, Loss: 1.193524718284607\n",
      "Episode 293/1000, Loss: 0.9670791625976562\n",
      "Episode 294/1000, Loss: 1.0645222663879395\n",
      "Episode 295/1000, Loss: 1.1228132247924805\n",
      "Episode 296/1000, Loss: 2.2469115257263184\n",
      "Episode 297/1000, Loss: 1.6372812986373901\n",
      "Episode 298/1000, Loss: 0.7379426956176758\n",
      "Episode 299/1000, Loss: 2.897136688232422\n",
      "Episode 300/1000, Loss: 1.3695629835128784\n",
      "Episode 301/1000, Loss: 0.7669577598571777\n",
      "Episode 302/1000, Loss: 1.151821255683899\n",
      "Episode 303/1000, Loss: 1.6285122632980347\n",
      "Episode 304/1000, Loss: 1.6035257577896118\n",
      "Episode 305/1000, Loss: 0.9853991270065308\n",
      "Episode 306/1000, Loss: 1.4066450595855713\n",
      "Episode 307/1000, Loss: 1.4780197143554688\n",
      "Episode 308/1000, Loss: 1.4948567152023315\n",
      "Episode 309/1000, Loss: 1.0941381454467773\n",
      "Episode 310/1000, Loss: 0.7744656801223755\n",
      "Episode 311/1000, Loss: 2.0620646476745605\n",
      "Episode 312/1000, Loss: 1.048952341079712\n",
      "Episode 313/1000, Loss: 1.6441861391067505\n",
      "Episode 314/1000, Loss: 1.210838794708252\n",
      "Episode 315/1000, Loss: 1.082700490951538\n",
      "Episode 316/1000, Loss: 2.724430799484253\n",
      "Episode 317/1000, Loss: 1.4384374618530273\n",
      "Episode 318/1000, Loss: 0.7536522150039673\n",
      "Episode 319/1000, Loss: 1.288773536682129\n",
      "Episode 320/1000, Loss: 1.41546630859375\n",
      "Episode 321/1000, Loss: 1.0035970211029053\n",
      "Episode 322/1000, Loss: 0.6955265998840332\n",
      "Episode 323/1000, Loss: 1.4954010248184204\n",
      "Episode 324/1000, Loss: 1.6273095607757568\n",
      "Episode 325/1000, Loss: 1.3008579015731812\n",
      "Episode 326/1000, Loss: 1.0350959300994873\n",
      "Episode 327/1000, Loss: 1.0896127223968506\n",
      "Episode 328/1000, Loss: 1.2146544456481934\n",
      "Episode 329/1000, Loss: 1.2082010507583618\n",
      "Episode 330/1000, Loss: 0.8277711868286133\n",
      "Episode 331/1000, Loss: 0.7189215421676636\n",
      "Episode 332/1000, Loss: 1.2018370628356934\n",
      "Episode 333/1000, Loss: 0.7390033006668091\n",
      "Episode 334/1000, Loss: 1.078344464302063\n",
      "Episode 335/1000, Loss: 1.1850337982177734\n",
      "Episode 336/1000, Loss: 0.8527209758758545\n",
      "Episode 337/1000, Loss: 0.9255586862564087\n",
      "Episode 338/1000, Loss: 1.7423512935638428\n",
      "Episode 339/1000, Loss: 0.9445713758468628\n",
      "Episode 340/1000, Loss: 1.4647899866104126\n",
      "Episode 341/1000, Loss: 1.4144338369369507\n",
      "Episode 342/1000, Loss: 1.1207444667816162\n",
      "Episode 343/1000, Loss: 1.0221022367477417\n",
      "Episode 344/1000, Loss: 2.745954751968384\n",
      "Episode 345/1000, Loss: 0.6355315446853638\n",
      "Episode 346/1000, Loss: 0.882818341255188\n",
      "Episode 347/1000, Loss: 1.0376863479614258\n",
      "Episode 348/1000, Loss: 1.3213000297546387\n",
      "Episode 349/1000, Loss: 1.611987590789795\n",
      "Episode 350/1000, Loss: 0.9296929240226746\n",
      "Episode 351/1000, Loss: 1.015248417854309\n",
      "Episode 352/1000, Loss: 0.98624587059021\n",
      "Episode 353/1000, Loss: 1.7859641313552856\n",
      "Episode 354/1000, Loss: 1.443465232849121\n",
      "Episode 355/1000, Loss: 1.3381011486053467\n",
      "Episode 356/1000, Loss: 0.8446130752563477\n",
      "Episode 357/1000, Loss: 2.006805896759033\n",
      "Episode 358/1000, Loss: 1.0852521657943726\n",
      "Episode 359/1000, Loss: 0.9806267023086548\n",
      "Episode 360/1000, Loss: 0.956735372543335\n",
      "Episode 361/1000, Loss: 0.9091293811798096\n",
      "Episode 362/1000, Loss: 0.6978772878646851\n",
      "Episode 363/1000, Loss: 1.3394644260406494\n",
      "Episode 364/1000, Loss: 0.9790616035461426\n",
      "Episode 365/1000, Loss: 1.7190968990325928\n",
      "Episode 366/1000, Loss: 0.9781441688537598\n",
      "Episode 367/1000, Loss: 1.0800563097000122\n",
      "Episode 368/1000, Loss: 1.0333682298660278\n",
      "Episode 369/1000, Loss: 1.1387441158294678\n",
      "Episode 370/1000, Loss: 2.5297913551330566\n",
      "Episode 371/1000, Loss: 0.9999781250953674\n",
      "Episode 372/1000, Loss: 1.5097236633300781\n",
      "Episode 373/1000, Loss: 0.7386307120323181\n",
      "Episode 374/1000, Loss: 1.386732816696167\n",
      "Episode 375/1000, Loss: 1.575728416442871\n",
      "Episode 376/1000, Loss: 0.9257316589355469\n",
      "Episode 377/1000, Loss: 1.6780881881713867\n",
      "Episode 378/1000, Loss: 0.8391537070274353\n",
      "Episode 379/1000, Loss: 1.0592823028564453\n",
      "Episode 380/1000, Loss: 0.6570072174072266\n",
      "Episode 381/1000, Loss: 1.0099561214447021\n",
      "Episode 382/1000, Loss: 0.7767360210418701\n",
      "Episode 383/1000, Loss: 2.051227331161499\n",
      "Episode 384/1000, Loss: 2.022675037384033\n",
      "Episode 385/1000, Loss: 0.6706295013427734\n",
      "Episode 386/1000, Loss: 1.274933099746704\n",
      "Episode 387/1000, Loss: 1.7425256967544556\n",
      "Episode 388/1000, Loss: 1.638159990310669\n",
      "Episode 389/1000, Loss: 1.5616250038146973\n",
      "Episode 390/1000, Loss: 0.8656936883926392\n",
      "Episode 391/1000, Loss: 0.8025006055831909\n",
      "Episode 392/1000, Loss: 0.8207018375396729\n",
      "Episode 393/1000, Loss: 0.699958086013794\n",
      "Episode 394/1000, Loss: 0.8821269273757935\n",
      "Episode 395/1000, Loss: 0.9473856687545776\n",
      "Episode 396/1000, Loss: 0.9537041187286377\n",
      "Episode 397/1000, Loss: 0.7831524610519409\n",
      "Episode 398/1000, Loss: 1.020510196685791\n",
      "Episode 399/1000, Loss: 0.6025123596191406\n",
      "Episode 400/1000, Loss: 0.5803819298744202\n",
      "Episode 401/1000, Loss: 1.8101496696472168\n",
      "Episode 402/1000, Loss: 1.896686315536499\n",
      "Episode 403/1000, Loss: 0.8858416676521301\n",
      "Episode 404/1000, Loss: 1.2840576171875\n",
      "Episode 405/1000, Loss: 1.7185742855072021\n",
      "Episode 406/1000, Loss: 0.8918425440788269\n",
      "Episode 407/1000, Loss: 0.844273030757904\n",
      "Episode 408/1000, Loss: 1.259326457977295\n",
      "Episode 409/1000, Loss: 1.2451856136322021\n",
      "Episode 410/1000, Loss: 0.8163227438926697\n",
      "Episode 411/1000, Loss: 0.8539268970489502\n",
      "Episode 412/1000, Loss: 0.44520705938339233\n",
      "Episode 413/1000, Loss: 0.8841655850410461\n",
      "Episode 414/1000, Loss: 1.8172292709350586\n",
      "Episode 415/1000, Loss: 0.6534898281097412\n",
      "Episode 416/1000, Loss: 1.0599045753479004\n",
      "Episode 417/1000, Loss: 0.7545922994613647\n",
      "Episode 418/1000, Loss: 0.7027789354324341\n",
      "Episode 419/1000, Loss: 0.9814608097076416\n",
      "Episode 420/1000, Loss: 1.0010499954223633\n",
      "Episode 421/1000, Loss: 0.8971538543701172\n",
      "Episode 422/1000, Loss: 1.0496397018432617\n",
      "Episode 423/1000, Loss: 1.0989972352981567\n",
      "Episode 424/1000, Loss: 1.20694100856781\n",
      "Episode 425/1000, Loss: 1.519822359085083\n",
      "Episode 426/1000, Loss: 1.9611539840698242\n",
      "Episode 427/1000, Loss: 1.9520773887634277\n",
      "Episode 428/1000, Loss: 1.744201421737671\n",
      "Episode 429/1000, Loss: 1.2838845252990723\n",
      "Episode 430/1000, Loss: 0.7736181020736694\n",
      "Episode 431/1000, Loss: 1.6255466938018799\n",
      "Episode 432/1000, Loss: 1.0750513076782227\n",
      "Episode 433/1000, Loss: 1.5029792785644531\n",
      "Episode 434/1000, Loss: 1.0863598585128784\n",
      "Episode 435/1000, Loss: 0.7676557302474976\n",
      "Episode 436/1000, Loss: 0.8156000375747681\n",
      "Episode 437/1000, Loss: 1.8422110080718994\n",
      "Episode 438/1000, Loss: 1.3003525733947754\n",
      "Episode 439/1000, Loss: 0.8264580368995667\n",
      "Episode 440/1000, Loss: 0.7731472253799438\n",
      "Episode 441/1000, Loss: 1.1547541618347168\n",
      "Episode 442/1000, Loss: 1.5764055252075195\n",
      "Episode 443/1000, Loss: 1.4963006973266602\n",
      "Episode 444/1000, Loss: 1.3118512630462646\n",
      "Episode 445/1000, Loss: 1.7273550033569336\n",
      "Episode 446/1000, Loss: 1.2142080068588257\n",
      "Episode 447/1000, Loss: 0.4061528146266937\n",
      "Episode 448/1000, Loss: 1.0815401077270508\n",
      "Episode 449/1000, Loss: 0.9950522780418396\n",
      "Episode 450/1000, Loss: 1.2386162281036377\n",
      "Episode 451/1000, Loss: 1.1975528001785278\n",
      "Episode 452/1000, Loss: 1.0120514631271362\n",
      "Episode 453/1000, Loss: 1.7746845483779907\n",
      "Episode 454/1000, Loss: 0.8641151189804077\n",
      "Episode 455/1000, Loss: 1.3808057308197021\n",
      "Episode 456/1000, Loss: 1.4349393844604492\n",
      "Episode 457/1000, Loss: 0.8675165772438049\n",
      "Episode 458/1000, Loss: 1.0817127227783203\n",
      "Episode 459/1000, Loss: 0.7253451943397522\n",
      "Episode 460/1000, Loss: 1.950080156326294\n",
      "Episode 461/1000, Loss: 0.9255719184875488\n",
      "Episode 462/1000, Loss: 0.9525958895683289\n",
      "Episode 463/1000, Loss: 0.7406931519508362\n",
      "Episode 464/1000, Loss: 1.4305469989776611\n",
      "Episode 465/1000, Loss: 1.1842055320739746\n",
      "Episode 466/1000, Loss: 0.5952887535095215\n",
      "Episode 467/1000, Loss: 0.9756558537483215\n",
      "Episode 468/1000, Loss: 1.0466665029525757\n",
      "Episode 469/1000, Loss: 0.9826709032058716\n",
      "Episode 470/1000, Loss: 1.38206946849823\n",
      "Episode 471/1000, Loss: 0.669386625289917\n",
      "Episode 472/1000, Loss: 1.2461864948272705\n",
      "Episode 473/1000, Loss: 0.8603783845901489\n",
      "Episode 474/1000, Loss: 0.8707111477851868\n",
      "Episode 475/1000, Loss: 0.8754417896270752\n",
      "Episode 476/1000, Loss: 1.4983365535736084\n",
      "Episode 477/1000, Loss: 1.613206386566162\n",
      "Episode 478/1000, Loss: 0.8977980613708496\n",
      "Episode 479/1000, Loss: 0.84070885181427\n",
      "Episode 480/1000, Loss: 1.1020307540893555\n",
      "Episode 481/1000, Loss: 0.40338075160980225\n",
      "Episode 482/1000, Loss: 1.0923937559127808\n",
      "Episode 483/1000, Loss: 0.7722029685974121\n",
      "Episode 484/1000, Loss: 1.003525733947754\n",
      "Episode 485/1000, Loss: 1.333409309387207\n",
      "Episode 486/1000, Loss: 1.6950181722640991\n",
      "Episode 487/1000, Loss: 0.9946795105934143\n",
      "Episode 488/1000, Loss: 0.8101000189781189\n",
      "Episode 489/1000, Loss: 0.8510985970497131\n",
      "Episode 490/1000, Loss: 0.7903814315795898\n",
      "Episode 491/1000, Loss: 0.7402762174606323\n",
      "Episode 492/1000, Loss: 0.8971018195152283\n",
      "Episode 493/1000, Loss: 1.332977294921875\n",
      "Episode 494/1000, Loss: 0.8510507941246033\n",
      "Episode 495/1000, Loss: 0.7554075717926025\n",
      "Episode 496/1000, Loss: 0.5736042261123657\n",
      "Episode 497/1000, Loss: 1.531559705734253\n",
      "Episode 498/1000, Loss: 0.7602555751800537\n",
      "Episode 499/1000, Loss: 1.0731539726257324\n",
      "Episode 500/1000, Loss: 2.1633951663970947\n",
      "Episode 501/1000, Loss: 0.7271345853805542\n",
      "Episode 502/1000, Loss: 1.7419496774673462\n",
      "Episode 503/1000, Loss: 0.8830469250679016\n",
      "Episode 504/1000, Loss: 0.9754244685173035\n",
      "Episode 505/1000, Loss: 1.5439610481262207\n",
      "Episode 506/1000, Loss: 0.9484591484069824\n",
      "Episode 507/1000, Loss: 1.1750621795654297\n",
      "Episode 508/1000, Loss: 0.9397388696670532\n",
      "Episode 509/1000, Loss: 0.9296895265579224\n",
      "Episode 510/1000, Loss: 1.1015009880065918\n",
      "Episode 511/1000, Loss: 1.84886634349823\n",
      "Episode 512/1000, Loss: 1.0625479221343994\n",
      "Episode 513/1000, Loss: 1.2483612298965454\n",
      "Episode 514/1000, Loss: 1.0624768733978271\n",
      "Episode 515/1000, Loss: 1.4876666069030762\n",
      "Episode 516/1000, Loss: 1.4082609415054321\n",
      "Episode 517/1000, Loss: 0.9835065603256226\n",
      "Episode 518/1000, Loss: 0.504065752029419\n",
      "Episode 519/1000, Loss: 0.7238794565200806\n",
      "Episode 520/1000, Loss: 1.216855525970459\n",
      "Episode 521/1000, Loss: 1.1655932664871216\n",
      "Episode 522/1000, Loss: 1.343870997428894\n",
      "Episode 523/1000, Loss: 0.9959895014762878\n",
      "Episode 524/1000, Loss: 1.2086694240570068\n",
      "Episode 525/1000, Loss: 1.5411171913146973\n",
      "Episode 526/1000, Loss: 1.149394154548645\n",
      "Episode 527/1000, Loss: 0.8153018355369568\n",
      "Episode 528/1000, Loss: 1.6373119354248047\n",
      "Episode 529/1000, Loss: 1.42381751537323\n",
      "Episode 530/1000, Loss: 0.8464210629463196\n",
      "Episode 531/1000, Loss: 1.4031095504760742\n",
      "Episode 532/1000, Loss: 1.4283885955810547\n",
      "Episode 533/1000, Loss: 2.162943124771118\n",
      "Episode 534/1000, Loss: 0.9342921376228333\n",
      "Episode 535/1000, Loss: 1.0350208282470703\n",
      "Episode 536/1000, Loss: 1.4205859899520874\n",
      "Episode 537/1000, Loss: 0.7145239114761353\n",
      "Episode 538/1000, Loss: 1.3225092887878418\n",
      "Episode 539/1000, Loss: 0.7470313310623169\n",
      "Episode 540/1000, Loss: 1.0309360027313232\n",
      "Episode 541/1000, Loss: 1.3992503881454468\n",
      "Episode 542/1000, Loss: 0.9850113391876221\n",
      "Episode 543/1000, Loss: 1.0998419523239136\n",
      "Episode 544/1000, Loss: 1.6355817317962646\n",
      "Episode 545/1000, Loss: 1.4985461235046387\n",
      "Episode 546/1000, Loss: 0.9906351566314697\n",
      "Episode 547/1000, Loss: 0.7354880571365356\n",
      "Episode 548/1000, Loss: 1.3546961545944214\n",
      "Episode 549/1000, Loss: 1.4069654941558838\n",
      "Episode 550/1000, Loss: 1.350062608718872\n",
      "Episode 551/1000, Loss: 0.9141066670417786\n",
      "Episode 552/1000, Loss: 0.7198812365531921\n",
      "Episode 553/1000, Loss: 1.4573489427566528\n",
      "Episode 554/1000, Loss: 1.0434490442276\n",
      "Episode 555/1000, Loss: 1.005644679069519\n",
      "Episode 556/1000, Loss: 0.9903079271316528\n",
      "Episode 557/1000, Loss: 0.7825449705123901\n",
      "Episode 558/1000, Loss: 1.3616986274719238\n",
      "Episode 559/1000, Loss: 1.1417996883392334\n",
      "Episode 560/1000, Loss: 1.0417441129684448\n",
      "Episode 561/1000, Loss: 0.8957476615905762\n",
      "Episode 562/1000, Loss: 1.2337026596069336\n",
      "Episode 563/1000, Loss: 1.3041250705718994\n",
      "Episode 564/1000, Loss: 2.404601812362671\n",
      "Episode 565/1000, Loss: 0.8050954341888428\n",
      "Episode 566/1000, Loss: 0.982881486415863\n",
      "Episode 567/1000, Loss: 0.8936761021614075\n",
      "Episode 568/1000, Loss: 1.0028784275054932\n",
      "Episode 569/1000, Loss: 1.1414841413497925\n",
      "Episode 570/1000, Loss: 0.620091438293457\n",
      "Episode 571/1000, Loss: 1.6131606101989746\n",
      "Episode 572/1000, Loss: 0.8141720294952393\n",
      "Episode 573/1000, Loss: 1.2046581506729126\n",
      "Episode 574/1000, Loss: 1.2219799757003784\n",
      "Episode 575/1000, Loss: 0.9153291583061218\n",
      "Episode 576/1000, Loss: 0.7331253290176392\n",
      "Episode 577/1000, Loss: 1.9042859077453613\n",
      "Episode 578/1000, Loss: 1.3460822105407715\n",
      "Episode 579/1000, Loss: 0.8796595335006714\n",
      "Episode 580/1000, Loss: 1.022956132888794\n",
      "Episode 581/1000, Loss: 0.7677656412124634\n",
      "Episode 582/1000, Loss: 1.56184983253479\n",
      "Episode 583/1000, Loss: 0.7291910648345947\n",
      "Episode 584/1000, Loss: 0.745374321937561\n",
      "Episode 585/1000, Loss: 0.5674630999565125\n",
      "Episode 586/1000, Loss: 0.7017378211021423\n",
      "Episode 587/1000, Loss: 0.9225116968154907\n",
      "Episode 588/1000, Loss: 1.3594529628753662\n",
      "Episode 589/1000, Loss: 0.5790513753890991\n",
      "Episode 590/1000, Loss: 1.1149766445159912\n",
      "Episode 591/1000, Loss: 1.52488374710083\n",
      "Episode 592/1000, Loss: 0.8151014447212219\n",
      "Episode 593/1000, Loss: 0.8276973962783813\n",
      "Episode 594/1000, Loss: 0.9807426929473877\n",
      "Episode 595/1000, Loss: 1.456034541130066\n",
      "Episode 596/1000, Loss: 0.9621486663818359\n",
      "Episode 597/1000, Loss: 0.9782055616378784\n",
      "Episode 598/1000, Loss: 1.211699366569519\n",
      "Episode 599/1000, Loss: 1.085155725479126\n",
      "Episode 600/1000, Loss: 1.1734318733215332\n",
      "Episode 601/1000, Loss: 0.8299475908279419\n",
      "Episode 602/1000, Loss: 1.1975769996643066\n",
      "Episode 603/1000, Loss: 1.10598623752594\n",
      "Episode 604/1000, Loss: 1.1804594993591309\n",
      "Episode 605/1000, Loss: 1.2047134637832642\n",
      "Episode 606/1000, Loss: 0.9671366214752197\n",
      "Episode 607/1000, Loss: 1.0774213075637817\n",
      "Episode 608/1000, Loss: 0.9860520362854004\n",
      "Episode 609/1000, Loss: 0.706149160861969\n",
      "Episode 610/1000, Loss: 0.9056599140167236\n",
      "Episode 611/1000, Loss: 0.8151503801345825\n",
      "Episode 612/1000, Loss: 0.6353834271430969\n",
      "Episode 613/1000, Loss: 1.2669942378997803\n",
      "Episode 614/1000, Loss: 1.4465298652648926\n",
      "Episode 615/1000, Loss: 1.0463544130325317\n",
      "Episode 616/1000, Loss: 0.7908801436424255\n",
      "Episode 617/1000, Loss: 1.5298001766204834\n",
      "Episode 618/1000, Loss: 2.3752024173736572\n",
      "Episode 619/1000, Loss: 1.265037178993225\n",
      "Episode 620/1000, Loss: 1.0896716117858887\n",
      "Episode 621/1000, Loss: 0.527294397354126\n",
      "Episode 622/1000, Loss: 1.4303491115570068\n",
      "Episode 623/1000, Loss: 2.2938802242279053\n",
      "Episode 624/1000, Loss: 1.0735082626342773\n",
      "Episode 625/1000, Loss: 1.018309473991394\n",
      "Episode 626/1000, Loss: 1.2434018850326538\n",
      "Episode 627/1000, Loss: 1.2888383865356445\n",
      "Episode 628/1000, Loss: 0.8893660306930542\n",
      "Episode 629/1000, Loss: 0.4427427351474762\n",
      "Episode 630/1000, Loss: 0.7195184230804443\n",
      "Episode 631/1000, Loss: 0.7841932773590088\n",
      "Episode 632/1000, Loss: 0.9812328815460205\n",
      "Episode 633/1000, Loss: 1.2025320529937744\n",
      "Episode 634/1000, Loss: 1.061801791191101\n",
      "Episode 635/1000, Loss: 1.1540915966033936\n",
      "Episode 636/1000, Loss: 1.2137185335159302\n",
      "Episode 637/1000, Loss: 0.36164790391921997\n",
      "Episode 638/1000, Loss: 0.8631829619407654\n",
      "Episode 639/1000, Loss: 0.9512367248535156\n",
      "Episode 640/1000, Loss: 0.7550441026687622\n",
      "Episode 641/1000, Loss: 1.270131230354309\n",
      "Episode 642/1000, Loss: 1.343134880065918\n",
      "Episode 643/1000, Loss: 1.372246503829956\n",
      "Episode 644/1000, Loss: 1.3058351278305054\n",
      "Episode 645/1000, Loss: 1.1580314636230469\n",
      "Episode 646/1000, Loss: 0.7202460169792175\n",
      "Episode 647/1000, Loss: 0.7423511743545532\n",
      "Episode 648/1000, Loss: 0.523566484451294\n",
      "Episode 649/1000, Loss: 0.8535400032997131\n",
      "Episode 650/1000, Loss: 0.686663031578064\n",
      "Episode 651/1000, Loss: 0.7592979669570923\n",
      "Episode 652/1000, Loss: 0.5659734010696411\n",
      "Episode 653/1000, Loss: 0.7800232172012329\n",
      "Episode 654/1000, Loss: 1.3725844621658325\n",
      "Episode 655/1000, Loss: 0.8416410684585571\n",
      "Episode 656/1000, Loss: 1.043581485748291\n",
      "Episode 657/1000, Loss: 0.920872688293457\n",
      "Episode 658/1000, Loss: 1.1968870162963867\n",
      "Episode 659/1000, Loss: 1.530447006225586\n",
      "Episode 660/1000, Loss: 1.923004388809204\n",
      "Episode 661/1000, Loss: 1.009934663772583\n",
      "Episode 662/1000, Loss: 1.0194883346557617\n",
      "Episode 663/1000, Loss: 1.8959534168243408\n",
      "Episode 664/1000, Loss: 1.6169679164886475\n",
      "Episode 665/1000, Loss: 1.0848044157028198\n",
      "Episode 666/1000, Loss: 1.5336005687713623\n",
      "Episode 667/1000, Loss: 0.9302801489830017\n",
      "Episode 668/1000, Loss: 0.5984674692153931\n",
      "Episode 669/1000, Loss: 1.5832335948944092\n",
      "Episode 670/1000, Loss: 2.420165777206421\n",
      "Episode 671/1000, Loss: 2.5318315029144287\n",
      "Episode 672/1000, Loss: 0.9359115362167358\n",
      "Episode 673/1000, Loss: 1.2617051601409912\n",
      "Episode 674/1000, Loss: 1.822391390800476\n",
      "Episode 675/1000, Loss: 1.332233190536499\n",
      "Episode 676/1000, Loss: 0.9629308581352234\n",
      "Episode 677/1000, Loss: 1.0253422260284424\n",
      "Episode 678/1000, Loss: 1.363301157951355\n",
      "Episode 679/1000, Loss: 1.2388734817504883\n",
      "Episode 680/1000, Loss: 0.8341827392578125\n",
      "Episode 681/1000, Loss: 1.6015297174453735\n",
      "Episode 682/1000, Loss: 1.382002830505371\n",
      "Episode 683/1000, Loss: 0.8489716053009033\n",
      "Episode 684/1000, Loss: 0.4536648988723755\n",
      "Episode 685/1000, Loss: 1.1105852127075195\n",
      "Episode 686/1000, Loss: 0.7149839401245117\n",
      "Episode 687/1000, Loss: 1.3975558280944824\n",
      "Episode 688/1000, Loss: 1.54574453830719\n",
      "Episode 689/1000, Loss: 0.708846926689148\n",
      "Episode 690/1000, Loss: 0.6656830310821533\n",
      "Episode 691/1000, Loss: 0.9671545028686523\n",
      "Episode 692/1000, Loss: 2.3824357986450195\n",
      "Episode 693/1000, Loss: 0.6789376735687256\n",
      "Episode 694/1000, Loss: 0.8421345353126526\n",
      "Episode 695/1000, Loss: 1.6375468969345093\n",
      "Episode 696/1000, Loss: 1.6980535984039307\n",
      "Episode 697/1000, Loss: 1.9922409057617188\n",
      "Episode 698/1000, Loss: 1.3153671026229858\n",
      "Episode 699/1000, Loss: 0.6977525353431702\n",
      "Episode 700/1000, Loss: 1.3825970888137817\n",
      "Episode 701/1000, Loss: 0.8637890219688416\n",
      "Episode 702/1000, Loss: 1.0487010478973389\n",
      "Episode 703/1000, Loss: 1.0375885963439941\n",
      "Episode 704/1000, Loss: 1.2165346145629883\n",
      "Episode 705/1000, Loss: 1.3982043266296387\n",
      "Episode 706/1000, Loss: 1.3039789199829102\n",
      "Episode 707/1000, Loss: 0.9659662246704102\n",
      "Episode 708/1000, Loss: 2.0296804904937744\n",
      "Episode 709/1000, Loss: 1.3195229768753052\n",
      "Episode 710/1000, Loss: 1.0562642812728882\n",
      "Episode 711/1000, Loss: 0.9773765206336975\n",
      "Episode 712/1000, Loss: 0.8505761623382568\n",
      "Episode 713/1000, Loss: 0.9188448786735535\n",
      "Episode 714/1000, Loss: 0.8926434516906738\n",
      "Episode 715/1000, Loss: 0.6579723358154297\n",
      "Episode 716/1000, Loss: 1.3331007957458496\n",
      "Episode 717/1000, Loss: 1.7250460386276245\n",
      "Episode 718/1000, Loss: 0.26829421520233154\n",
      "Episode 719/1000, Loss: 1.5943176746368408\n",
      "Episode 720/1000, Loss: 0.8880329728126526\n",
      "Episode 721/1000, Loss: 0.8700224161148071\n",
      "Episode 722/1000, Loss: 0.985664963722229\n",
      "Episode 723/1000, Loss: 1.4106011390686035\n",
      "Episode 724/1000, Loss: 0.7585045099258423\n",
      "Episode 725/1000, Loss: 0.9245680570602417\n",
      "Episode 726/1000, Loss: 1.2294708490371704\n",
      "Episode 727/1000, Loss: 1.5074286460876465\n",
      "Episode 728/1000, Loss: 0.8855980038642883\n",
      "Episode 729/1000, Loss: 0.9933356046676636\n",
      "Episode 730/1000, Loss: 0.755156934261322\n",
      "Episode 731/1000, Loss: 0.7232416868209839\n",
      "Episode 732/1000, Loss: 1.3356430530548096\n",
      "Episode 733/1000, Loss: 0.8547163009643555\n",
      "Episode 734/1000, Loss: 1.399117112159729\n",
      "Episode 735/1000, Loss: 0.5936035513877869\n",
      "Episode 736/1000, Loss: 0.6051263809204102\n",
      "Episode 737/1000, Loss: 1.157738447189331\n",
      "Episode 738/1000, Loss: 1.3122870922088623\n",
      "Episode 739/1000, Loss: 1.631057858467102\n",
      "Episode 740/1000, Loss: 0.9391040802001953\n",
      "Episode 741/1000, Loss: 0.7359801530838013\n",
      "Episode 742/1000, Loss: 0.612001895904541\n",
      "Episode 743/1000, Loss: 0.9180448055267334\n",
      "Episode 744/1000, Loss: 0.4813906252384186\n",
      "Episode 745/1000, Loss: 0.7744841575622559\n",
      "Episode 746/1000, Loss: 0.762043833732605\n",
      "Episode 747/1000, Loss: 1.5428019762039185\n",
      "Episode 748/1000, Loss: 0.7713806629180908\n",
      "Episode 749/1000, Loss: 0.9781619310379028\n",
      "Episode 750/1000, Loss: 0.6878951787948608\n",
      "Episode 751/1000, Loss: 0.9044397473335266\n",
      "Episode 752/1000, Loss: 0.8798948526382446\n",
      "Episode 753/1000, Loss: 0.5073837041854858\n",
      "Episode 754/1000, Loss: 0.673211932182312\n",
      "Episode 755/1000, Loss: 1.0882357358932495\n",
      "Episode 756/1000, Loss: 1.7903923988342285\n",
      "Episode 757/1000, Loss: 0.8190255165100098\n",
      "Episode 758/1000, Loss: 0.958741307258606\n",
      "Episode 759/1000, Loss: 1.0048458576202393\n",
      "Episode 760/1000, Loss: 1.250680923461914\n",
      "Episode 761/1000, Loss: 0.8334662914276123\n",
      "Episode 762/1000, Loss: 0.6976193189620972\n",
      "Episode 763/1000, Loss: 0.8512939810752869\n",
      "Episode 764/1000, Loss: 1.5124473571777344\n",
      "Episode 765/1000, Loss: 0.7332593202590942\n",
      "Episode 766/1000, Loss: 0.596598207950592\n",
      "Episode 767/1000, Loss: 1.3394784927368164\n",
      "Episode 768/1000, Loss: 1.0334947109222412\n",
      "Episode 769/1000, Loss: 0.9759743213653564\n",
      "Episode 770/1000, Loss: 0.4218207597732544\n",
      "Episode 771/1000, Loss: 0.6611334681510925\n",
      "Episode 772/1000, Loss: 1.0137603282928467\n",
      "Episode 773/1000, Loss: 1.0785638093948364\n",
      "Episode 774/1000, Loss: 1.0040054321289062\n",
      "Episode 775/1000, Loss: 1.5341126918792725\n",
      "Episode 776/1000, Loss: 1.9495469331741333\n",
      "Episode 777/1000, Loss: 1.233134388923645\n",
      "Episode 778/1000, Loss: 0.6801390647888184\n",
      "Episode 779/1000, Loss: 1.2948100566864014\n",
      "Episode 780/1000, Loss: 1.6609351634979248\n",
      "Episode 781/1000, Loss: 0.5812631845474243\n",
      "Episode 782/1000, Loss: 0.8541271090507507\n",
      "Episode 783/1000, Loss: 0.7976897954940796\n",
      "Episode 784/1000, Loss: 2.426452159881592\n",
      "Episode 785/1000, Loss: 1.2809900045394897\n",
      "Episode 786/1000, Loss: 1.1728603839874268\n",
      "Episode 787/1000, Loss: 1.1499857902526855\n",
      "Episode 788/1000, Loss: 1.4709863662719727\n",
      "Episode 789/1000, Loss: 1.1857800483703613\n",
      "Episode 790/1000, Loss: 1.2607197761535645\n",
      "Episode 791/1000, Loss: 1.0065515041351318\n",
      "Episode 792/1000, Loss: 1.4963719844818115\n",
      "Episode 793/1000, Loss: 1.1199016571044922\n",
      "Episode 794/1000, Loss: 1.334540605545044\n",
      "Episode 795/1000, Loss: 1.355986475944519\n",
      "Episode 796/1000, Loss: 0.865065336227417\n",
      "Episode 797/1000, Loss: 0.8331915736198425\n",
      "Episode 798/1000, Loss: 0.9437025785446167\n",
      "Episode 799/1000, Loss: 0.8818455338478088\n",
      "Episode 800/1000, Loss: 0.9309921264648438\n",
      "Episode 801/1000, Loss: 1.0059903860092163\n",
      "Episode 802/1000, Loss: 0.7574155330657959\n",
      "Episode 803/1000, Loss: 1.0925861597061157\n",
      "Episode 804/1000, Loss: 1.8533674478530884\n",
      "Episode 805/1000, Loss: 2.00881028175354\n",
      "Episode 806/1000, Loss: 0.7874191999435425\n",
      "Episode 807/1000, Loss: 0.9847509264945984\n",
      "Episode 808/1000, Loss: 1.0500779151916504\n",
      "Episode 809/1000, Loss: 0.9302946329116821\n",
      "Episode 810/1000, Loss: 1.1441594362258911\n",
      "Episode 811/1000, Loss: 1.2604117393493652\n",
      "Episode 812/1000, Loss: 1.447238802909851\n",
      "Episode 813/1000, Loss: 0.8588078022003174\n",
      "Episode 814/1000, Loss: 0.7869486212730408\n",
      "Episode 815/1000, Loss: 1.220309853553772\n",
      "Episode 816/1000, Loss: 0.5973101854324341\n",
      "Episode 817/1000, Loss: 1.0714560747146606\n",
      "Episode 818/1000, Loss: 0.7679212093353271\n",
      "Episode 819/1000, Loss: 0.8281893730163574\n",
      "Episode 820/1000, Loss: 1.1897393465042114\n",
      "Episode 821/1000, Loss: 0.7205517292022705\n",
      "Episode 822/1000, Loss: 0.888507604598999\n",
      "Episode 823/1000, Loss: 0.9311133623123169\n",
      "Episode 824/1000, Loss: 1.1536414623260498\n",
      "Episode 825/1000, Loss: 1.0541807413101196\n",
      "Episode 826/1000, Loss: 0.6654021143913269\n",
      "Episode 827/1000, Loss: 1.6771607398986816\n",
      "Episode 828/1000, Loss: 0.8687606453895569\n",
      "Episode 829/1000, Loss: 0.6121780276298523\n",
      "Episode 830/1000, Loss: 2.215453624725342\n",
      "Episode 831/1000, Loss: 1.4807357788085938\n",
      "Episode 832/1000, Loss: 1.3690510988235474\n",
      "Episode 833/1000, Loss: 1.0458999872207642\n",
      "Episode 834/1000, Loss: 0.7167799472808838\n",
      "Episode 835/1000, Loss: 0.8077167868614197\n",
      "Episode 836/1000, Loss: 0.7940108776092529\n",
      "Episode 837/1000, Loss: 0.498124361038208\n",
      "Episode 838/1000, Loss: 0.8140615820884705\n",
      "Episode 839/1000, Loss: 1.1875476837158203\n",
      "Episode 840/1000, Loss: 1.0384329557418823\n",
      "Episode 841/1000, Loss: 0.7815144062042236\n",
      "Episode 842/1000, Loss: 1.0812010765075684\n",
      "Episode 843/1000, Loss: 0.75091952085495\n",
      "Episode 844/1000, Loss: 0.9402869939804077\n",
      "Episode 845/1000, Loss: 1.0874675512313843\n",
      "Episode 846/1000, Loss: 1.0859439373016357\n",
      "Episode 847/1000, Loss: 1.2294888496398926\n",
      "Episode 848/1000, Loss: 1.056536316871643\n",
      "Episode 849/1000, Loss: 0.88634192943573\n",
      "Episode 850/1000, Loss: 0.911578357219696\n",
      "Episode 851/1000, Loss: 0.7917479276657104\n",
      "Episode 852/1000, Loss: 1.0056061744689941\n",
      "Episode 853/1000, Loss: 1.4238572120666504\n",
      "Episode 854/1000, Loss: 0.9796782732009888\n",
      "Episode 855/1000, Loss: 1.5567017793655396\n",
      "Episode 856/1000, Loss: 0.8087778091430664\n",
      "Episode 857/1000, Loss: 0.7237988114356995\n",
      "Episode 858/1000, Loss: 1.378615379333496\n",
      "Episode 859/1000, Loss: 0.5548808574676514\n",
      "Episode 860/1000, Loss: 1.3234524726867676\n",
      "Episode 861/1000, Loss: 1.2993069887161255\n",
      "Episode 862/1000, Loss: 0.5188602209091187\n",
      "Episode 863/1000, Loss: 1.087127447128296\n",
      "Episode 864/1000, Loss: 0.913457989692688\n",
      "Episode 865/1000, Loss: 1.073805809020996\n",
      "Episode 866/1000, Loss: 1.3101775646209717\n",
      "Episode 867/1000, Loss: 1.5560939311981201\n",
      "Episode 868/1000, Loss: 0.6226955652236938\n",
      "Episode 869/1000, Loss: 1.1553776264190674\n",
      "Episode 870/1000, Loss: 1.298940658569336\n",
      "Episode 871/1000, Loss: 1.461157202720642\n",
      "Episode 872/1000, Loss: 0.8658204078674316\n",
      "Episode 873/1000, Loss: 0.7294905185699463\n",
      "Episode 874/1000, Loss: 0.8696098923683167\n",
      "Episode 875/1000, Loss: 0.711111843585968\n",
      "Episode 876/1000, Loss: 1.0828320980072021\n",
      "Episode 877/1000, Loss: 0.7717931270599365\n",
      "Episode 878/1000, Loss: 0.758466362953186\n",
      "Episode 879/1000, Loss: 0.5661174058914185\n",
      "Episode 880/1000, Loss: 1.233648419380188\n",
      "Episode 881/1000, Loss: 0.9974083304405212\n",
      "Episode 882/1000, Loss: 0.8910785913467407\n",
      "Episode 883/1000, Loss: 1.0331072807312012\n",
      "Episode 884/1000, Loss: 1.5209252834320068\n",
      "Episode 885/1000, Loss: 1.136560082435608\n",
      "Episode 886/1000, Loss: 0.7274295091629028\n",
      "Episode 887/1000, Loss: 1.335991621017456\n",
      "Episode 888/1000, Loss: 1.7024080753326416\n",
      "Episode 889/1000, Loss: 0.8678829073905945\n",
      "Episode 890/1000, Loss: 1.4919893741607666\n",
      "Episode 891/1000, Loss: 1.3179184198379517\n",
      "Episode 892/1000, Loss: 1.7165147066116333\n",
      "Episode 893/1000, Loss: 0.7406783103942871\n",
      "Episode 894/1000, Loss: 1.0367571115493774\n",
      "Episode 895/1000, Loss: 0.7967897653579712\n",
      "Episode 896/1000, Loss: 1.3746904134750366\n",
      "Episode 897/1000, Loss: 1.1240460872650146\n",
      "Episode 898/1000, Loss: 0.8697913885116577\n",
      "Episode 899/1000, Loss: 0.8146489262580872\n",
      "Episode 900/1000, Loss: 0.8196299076080322\n",
      "Episode 901/1000, Loss: 0.95346599817276\n",
      "Episode 902/1000, Loss: 0.9762397408485413\n",
      "Episode 903/1000, Loss: 1.19330632686615\n",
      "Episode 904/1000, Loss: 1.1231037378311157\n",
      "Episode 905/1000, Loss: 1.474090814590454\n",
      "Episode 906/1000, Loss: 1.1049034595489502\n",
      "Episode 907/1000, Loss: 1.0139185190200806\n",
      "Episode 908/1000, Loss: 1.2338299751281738\n",
      "Episode 909/1000, Loss: 0.5233761072158813\n",
      "Episode 910/1000, Loss: 1.0350897312164307\n",
      "Episode 911/1000, Loss: 0.8445174694061279\n",
      "Episode 912/1000, Loss: 1.5645123720169067\n",
      "Episode 913/1000, Loss: 0.6614784598350525\n",
      "Episode 914/1000, Loss: 0.8280576467514038\n",
      "Episode 915/1000, Loss: 0.8600174188613892\n",
      "Episode 916/1000, Loss: 0.9235865473747253\n",
      "Episode 917/1000, Loss: 0.8163858652114868\n",
      "Episode 918/1000, Loss: 1.2765995264053345\n",
      "Episode 919/1000, Loss: 0.7760702967643738\n",
      "Episode 920/1000, Loss: 1.0601099729537964\n",
      "Episode 921/1000, Loss: 1.0435969829559326\n",
      "Episode 922/1000, Loss: 1.2716853618621826\n",
      "Episode 923/1000, Loss: 1.2060109376907349\n",
      "Episode 924/1000, Loss: 1.1602864265441895\n",
      "Episode 925/1000, Loss: 0.6052122116088867\n",
      "Episode 926/1000, Loss: 0.7923617362976074\n",
      "Episode 927/1000, Loss: 0.7305997610092163\n",
      "Episode 928/1000, Loss: 0.54449063539505\n",
      "Episode 929/1000, Loss: 0.6673398017883301\n",
      "Episode 930/1000, Loss: 0.8488692045211792\n",
      "Episode 931/1000, Loss: 0.8771868944168091\n",
      "Episode 932/1000, Loss: 0.44128483533859253\n",
      "Episode 933/1000, Loss: 0.6944147348403931\n",
      "Episode 934/1000, Loss: 1.2194182872772217\n",
      "Episode 935/1000, Loss: 1.164747714996338\n",
      "Episode 936/1000, Loss: 0.7425267696380615\n",
      "Episode 937/1000, Loss: 1.2033333778381348\n",
      "Episode 938/1000, Loss: 1.3094232082366943\n",
      "Episode 939/1000, Loss: 0.8045797944068909\n",
      "Episode 940/1000, Loss: 0.7538296580314636\n",
      "Episode 941/1000, Loss: 1.1735992431640625\n",
      "Episode 942/1000, Loss: 0.9342013597488403\n",
      "Episode 943/1000, Loss: 0.8976789116859436\n",
      "Episode 944/1000, Loss: 0.72682124376297\n",
      "Episode 945/1000, Loss: 0.5963188409805298\n",
      "Episode 946/1000, Loss: 0.9249570369720459\n",
      "Episode 947/1000, Loss: 0.670026421546936\n",
      "Episode 948/1000, Loss: 1.458824634552002\n",
      "Episode 949/1000, Loss: 0.8283114433288574\n",
      "Episode 950/1000, Loss: 0.8583322763442993\n",
      "Episode 951/1000, Loss: 0.6077691912651062\n",
      "Episode 952/1000, Loss: 1.2231556177139282\n",
      "Episode 953/1000, Loss: 0.9248077869415283\n",
      "Episode 954/1000, Loss: 0.834831953048706\n",
      "Episode 955/1000, Loss: 0.756137490272522\n",
      "Episode 956/1000, Loss: 0.9371954202651978\n",
      "Episode 957/1000, Loss: 0.7575177550315857\n",
      "Episode 958/1000, Loss: 1.7145249843597412\n",
      "Episode 959/1000, Loss: 0.9319994449615479\n",
      "Episode 960/1000, Loss: 0.8251525163650513\n",
      "Episode 961/1000, Loss: 0.706954300403595\n",
      "Episode 962/1000, Loss: 0.5682297945022583\n",
      "Episode 963/1000, Loss: 1.3357172012329102\n",
      "Episode 964/1000, Loss: 0.9752663373947144\n",
      "Episode 965/1000, Loss: 1.497994065284729\n",
      "Episode 966/1000, Loss: 0.8566986322402954\n",
      "Episode 967/1000, Loss: 0.9037624001502991\n",
      "Episode 968/1000, Loss: 1.7079212665557861\n",
      "Episode 969/1000, Loss: 0.725538969039917\n",
      "Episode 970/1000, Loss: 1.0592899322509766\n",
      "Episode 971/1000, Loss: 0.871666669845581\n",
      "Episode 972/1000, Loss: 0.6542896032333374\n",
      "Episode 973/1000, Loss: 1.1981233358383179\n",
      "Episode 974/1000, Loss: 1.050754189491272\n",
      "Episode 975/1000, Loss: 0.86210036277771\n",
      "Episode 976/1000, Loss: 1.7006237506866455\n",
      "Episode 977/1000, Loss: 1.1183319091796875\n",
      "Episode 978/1000, Loss: 1.221461534500122\n",
      "Episode 979/1000, Loss: 0.7262668609619141\n",
      "Episode 980/1000, Loss: 1.2123684883117676\n",
      "Episode 981/1000, Loss: 0.7884646654129028\n",
      "Episode 982/1000, Loss: 1.2312127351760864\n",
      "Episode 983/1000, Loss: 0.9385174512863159\n",
      "Episode 984/1000, Loss: 0.5610889196395874\n",
      "Episode 985/1000, Loss: 0.7441169023513794\n",
      "Episode 986/1000, Loss: 0.8264502882957458\n",
      "Episode 987/1000, Loss: 0.6626913547515869\n",
      "Episode 988/1000, Loss: 1.0531166791915894\n",
      "Episode 989/1000, Loss: 0.7827413082122803\n",
      "Episode 990/1000, Loss: 2.0646982192993164\n",
      "Episode 991/1000, Loss: 0.525049090385437\n",
      "Episode 992/1000, Loss: 2.2940855026245117\n",
      "Episode 993/1000, Loss: 1.1975879669189453\n",
      "Episode 994/1000, Loss: 0.954994261264801\n",
      "Episode 995/1000, Loss: 0.6794337034225464\n",
      "Episode 996/1000, Loss: 1.9449431896209717\n",
      "Episode 997/1000, Loss: 0.9331787824630737\n",
      "Episode 998/1000, Loss: 1.0126197338104248\n",
      "Episode 999/1000, Loss: 1.225667119026184\n",
      "Episode 1000/1000, Loss: 1.02413010597229\n",
      "Model saved to meta_model01/meta_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Vòng lặp huấn luyện\n",
    "for episode in range(max_episodes):\n",
    "    #1. Lấy một batch dữ liệu ngẫu nhiên từ training_data\n",
    "    batch = training_data.sample(batch_size)\n",
    "\n",
    "    #2. Tách dữ liệu thành các phần riêng biệt\n",
    "    local_obs = torch.stack([torch.tensor(sample['local_obs'], dtype=torch.float32) for sample in batch])\n",
    "    agent_start_positions = torch.stack([torch.tensor(sample['agent_start_position'], dtype=torch.float32) for sample in batch])\n",
    "    agent_end_positions = torch.stack([torch.tensor(sample['agent_end_position'], dtype=torch.float32) for sample in batch])\n",
    "    labels = torch.tensor([sample['label'] for sample in batch], dtype=torch.long)\n",
    "\n",
    "    #3. Chuyển dữ liệu sang GPU nếu có\n",
    "    local_obs = local_obs.to(device)\n",
    "    agent_start_positions = agent_start_positions.to(device)\n",
    "    agent_end_positions = agent_end_positions.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    #4. Tính giá trị target dựa vào agent_start_positions và agent_end_positions\n",
    "    target_positions = agent_end_positions - agent_start_positions  \n",
    "    target = torch.sum(target_positions, dim=1)  # Tính tổng các vị trí\n",
    "\n",
    "    #5. Đưa dữ liệu vào mô hình và tính toán dự đoán\n",
    "    local_obs = local_obs.unsqueeze(1)  # Thêm chiều cho local_obs\n",
    "    predictions = model(local_obs, agent_start_positions)\n",
    "    prediction = predictions[torch.arange(batch_size), labels]\n",
    "    \n",
    "    #6. Tính toán hàm mất mát\n",
    "    loss = F.mse_loss(prediction, target)\n",
    "    \n",
    "    #7. Tối ưu hóa mô hình\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #8. In thông tin về quá trình huấn luyện\n",
    "    print(f\"Episode {episode + 1}/{max_episodes}, Loss: {loss.item()}\")\n",
    "\n",
    "save_model(model, model_name + '/meta_model.pth')  # Lưu trọng số của mô hình sau mỗi episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fbf8d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to meta_model01/meta_model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Xuất model sang ONNX\n",
    "def export_model_to_onnx(model, filename):\n",
    "    \"\"\"\n",
    "    Xuất mô hình sang định dạng ONNX.\n",
    "    \n",
    "    :param model: Mô hình cần xuất.\n",
    "    :param filename: Tên file để lưu mô hình ONNX.\n",
    "    \"\"\"\n",
    "    dummy_input = torch.randn(1, 1, local_size, local_size).to(device)  # Tạo đầu vào giả\n",
    "    dummy_position = torch.randn(1, 2).to(device)  # Tạo vị trí giả\n",
    "    torch.onnx.export(model, (dummy_input, dummy_position), filename, export_params=True)\n",
    "    print(f\"Model exported to {filename}\")\n",
    "\n",
    "export_model_to_onnx(model, model_name + '/meta_model.onnx')  # Xuất mô hình sang ONNX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
