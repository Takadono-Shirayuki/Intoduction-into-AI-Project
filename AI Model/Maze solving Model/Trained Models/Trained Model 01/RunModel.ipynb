{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các thư viện cần thiết\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces import Discrete\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e72cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model cho MazeNet\n",
    "\n",
    "class MazeNetCombined(nn.Module):\n",
    "    def __init__(self, local_size=11, global_size=10, num_actions=4):\n",
    "        super(MazeNetCombined, self).__init__()\n",
    "        \n",
    "        # Quan sát cục bộ\n",
    "        self.conv1_local = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2_local = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3_local = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4_local = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Quan sát toàn cục\n",
    "        self.conv1_global = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2_global = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3_global = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4_global = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully Connected cho vị trí hiện tại\n",
    "        self.fc_position = nn.Linear(2, 32)\n",
    "\n",
    "        # Tầng Fully Connected cuối cùng\n",
    "        self.fc1 = nn.Linear(256 * local_size * local_size + 256 * global_size * global_size + 32, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout_fc = nn.Dropout(p=0.5)  # Dropout trước tầng FC3\n",
    "        self.fc3 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, local_obs, global_obs, position):\n",
    "        # Xử lý local_obs\n",
    "        x_local = F.relu(self.conv1_local(local_obs))\n",
    "        x_local = F.relu(self.conv2_local(x_local))\n",
    "        x_local = F.relu(self.conv3_local(x_local))\n",
    "        x_local = F.relu(self.conv4_local(x_local))\n",
    "        x_local = x_local.view(x_local.size(0), -1)\n",
    "    \n",
    "        # Xử lý global_obs\n",
    "        x_global = F.relu(self.conv1_global(global_obs))\n",
    "        x_global = F.relu(self.conv2_global(x_global))\n",
    "        x_global = F.relu(self.conv3_global(x_global))\n",
    "        x_global = F.relu(self.conv4_global(x_global))\n",
    "        x_global = x_global.view(x_global.size(0), -1)\n",
    "        # Xử lý vị trí hiện tại\n",
    "        x_position = F.relu(self.fc_position(position))\n",
    "\n",
    "        # Kết hợp tất cả\n",
    "        x = torch.cat((x_local, x_global, x_position), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout_fc(x)  # Dropout trước FC3\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Môi trường Mê cung\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    # Môi trường mê cung\n",
    "    # Maze: Mê cung được tạo ra ngẫu nhiên với các ô đường và tường\n",
    "    # Với các ô đường được đánh dấu là 0 và các ô tường được đánh dấu là -1, ô đích được đánh dấu là 2\n",
    "    # Discovered_maze: Mê cung đã được khám phá trong quá trình di chuyển của tác tử\n",
    "    # Với các ô đường được đánh dấu là 0, các ô tường được đánh dấu là -5, đích được đánh dấu là 10, ô chưa được khám phá được đánh dấu là 1\n",
    "    # Giá trị các ô đường giảm dần mỗi lần tác tử đi qua\n",
    "    # Agent_position: Vị trí hiện tại của tác tử trong mê cung\n",
    "    # Base_position: Vị trí ban đầu của tác tử trong mê cung\n",
    "    # Goal_position: Vị trí đích trong mê cung\n",
    "    # Buff: Biến để xác định xem tác tử có đang sử dụng buff senrigan hay không\n",
    "    # Debuff: Biến để xác định xem tác tử có đang bị debuff shin no meiro hay không\n",
    "    \n",
    "    def __init__(self, maze_size, max_steps = 15, path_percent = 70):\n",
    "        \"\"\"\n",
    "        Khởi tạo môi trường Mê cung.\n",
    "\n",
    "        Args:\n",
    "        - maze_size (int): Kích thước của mê cung (ví dụ: 50x50).\n",
    "        - max_steps (int): Số bước tối đa cho mỗi tập.\n",
    "        - path_percent (int): Tỷ lệ phần trăm ô đường trong mê cung (0-100).\n",
    "        \"\"\"\n",
    "        # Đặt các thông số của môi trường\n",
    "\n",
    "        super(MazeEnv, self).__init__()  # Kế thừa từ gym.Env\n",
    "        self.data_set_name = \"\"\n",
    "        self.export_data = {}\n",
    "        \n",
    "        self.maze_size = maze_size\n",
    "        self.max_steps = max_steps\n",
    "        self.path_percent = path_percent\n",
    "\n",
    "        self.maze = np.ones((maze_size, maze_size), dtype=int)  # Mặc định là chưa biết (1)\n",
    "        self.goal_position = (maze_size * 5 // 6, maze_size * 5 // 6)  # Đích cố định tại giữa khu vực đích\n",
    "        self.buff = False\n",
    "        self.debuff = False\n",
    "\n",
    "        # Định nghĩa action_space (0: lên, 1: xuống, 2: trái, 3: phải)\n",
    "        self.action_space = Discrete(4)\n",
    "    \n",
    "    # Các phương thức liên quan đến tạo data set\n",
    "    def create_data_set(self, data_set_name):\n",
    "        self.data_set_name = data_set_name\n",
    "        self.data_export_count = 0\n",
    "        os.makedirs(data_set_name, exist_ok=True)\n",
    "        print(f\"Data_set created: {data_set_name}\")\n",
    "    \n",
    "    def export_maze_training_data(self):\n",
    "        if self.data_set_name == \"\":\n",
    "            return;\n",
    "        self.data_export_count += 1\n",
    "        self.export_data['agent_end_position'] = self.agent_position\n",
    "        with open(f\"{self.data_set_name}/maze_data_{self.data_export_count}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.export_data, f)\n",
    "    \n",
    "    # Các phương thức liên quan đến tái tạo mê cung\n",
    "    def reset(self):\n",
    "        self.base_position = (self.maze_size // 6 - 1, self.maze_size // 6 - 1)\n",
    "        self.agent_position = self.base_position\n",
    "        return self.regenerate_maze(0, self.path_percent)\n",
    "    \n",
    "    def regenerate_maze(self, adder = 0, path_percent = 70, ):\n",
    "        self.maze = self.generate_maze(adder, path_percent)\n",
    "        self.create_discovered_maze()\n",
    "        self.discover_maze()\n",
    "        self.export_data['local_obs'] = self.get_observation()[2:9, 2:9]\n",
    "        self.export_data['agent_start_position'] = self.agent_position\n",
    "        return (self.get_observation(), self.downsample(self.discovered_maze), self.agent_position)\n",
    "\n",
    "    def generate_maze(self, adder = 0, path_percent=70):\n",
    "        if self.agent_position[0] >= self.maze_size * 2 // 3 and self.agent_position[1] >= self.maze_size * 2 // 3:\n",
    "            number_of_path = 1 + adder\n",
    "        else:\n",
    "            number_of_path = 2 + adder\n",
    "            \n",
    "        while True:  # Sử dụng vòng lặp để tạo lại mê cung nếu không hợp lệ\n",
    "            total_cells = self.maze_size * self.maze_size\n",
    "            num_paths = int(total_cells * path_percent / 100)\n",
    "            num_walls = total_cells - num_paths\n",
    "    \n",
    "            # Tạo danh sách ngẫu nhiên các giá trị (đường hoặc tường)\n",
    "            maze_values = [0] * num_paths + [-1] * num_walls\n",
    "            random.shuffle(maze_values)  # Xáo trộn các giá trị\n",
    "    \n",
    "            # Điền vào mê cung\n",
    "            self.maze = np.array(maze_values).reshape(self.maze_size, self.maze_size)\n",
    "    \n",
    "            # Đặt điểm bắt đầu và đích\n",
    "            self.maze[self.agent_position] = 0  # Đảm bảo điểm bắt đầu là đường (0)\n",
    "            self.maze[self.goal_position] = 2  # Đảm bảo điểm đích là đường (2)\n",
    "    \n",
    "            # Kiểm tra tính hợp lệ\n",
    "            if self.validate_maze(number_of_path):\n",
    "                break  # Nếu mê cung hợp lệ, thoát vòng lặp\n",
    "        return self.maze\n",
    "\n",
    "    def validate_maze(self, number_of_path = 0):\n",
    "        stack = [self.agent_position]\n",
    "        visited = set()\n",
    "        d = 0\n",
    "        \n",
    "        while stack:\n",
    "            x, y = stack.pop()\n",
    "            if self.debuff:\n",
    "                if (x, y) == self.goal_position:\n",
    "                    return False\n",
    "\n",
    "            if (x, y) == self.goal_position:\n",
    "                d += 1\n",
    "                if d == number_of_path:\n",
    "                    return True\n",
    "                continue\n",
    "            \n",
    "            if (x, y) in visited:\n",
    "                continue\n",
    "            visited.add((x, y))\n",
    "\n",
    "            # Thêm các ô lân cận vào stack\n",
    "            neighbors = self.get_neighbors(x, y)\n",
    "            for nx, ny in neighbors:\n",
    "                if self.maze[nx, ny] > -1 and (nx, ny) not in visited:  # Chỉ đi qua đường\n",
    "                    stack.append((nx, ny))\n",
    "                    \n",
    "        return False or self.debuff  # Không có đường tới đích\n",
    "\n",
    "    def get_neighbors(self, x, y):\n",
    "        \"\"\"\n",
    "        Lấy danh sách các ô lân cận.\n",
    "        \"\"\"\n",
    "        neighbors = []\n",
    "        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.maze_size and 0 <= ny < self.maze_size:\n",
    "                neighbors.append((nx, ny))\n",
    "        return neighbors\n",
    "    \n",
    "    # Các phương thức liên quan đến mê cung được khám phá\n",
    "    def create_discovered_maze(self):\n",
    "        self.discovered_maze = np.ones((self.maze_size + 10, self.maze_size + 10), dtype = int) # Mặc định là chưa biết (1)\n",
    "        self.discovered_maze[0 : 5, :] = -5\n",
    "        self.discovered_maze[:, 0 : 5] = -5\n",
    "        self.discovered_maze[self.maze_size + 5 : self.maze_size + 10, :] = -5\n",
    "        self.discovered_maze[:, self.maze_size + 5 : self.maze_size + 10] = -5\n",
    "        self.discovered_maze[self.goal_position[0] + 5, self.goal_position[1] + 5] = 10  # Đích\n",
    "        \n",
    "    def discover_maze(self):\n",
    "        x, y = self.agent_position\n",
    "        obs_size = 11  # Kích thước quan sát (11x11)\n",
    "        if self.buff:\n",
    "            half_size = 5\n",
    "        else:\n",
    "            half_size = 3\n",
    "\n",
    "        # Xác định giới hạn của vùng quan sát trong mê cung\n",
    "        min_x = max(0, x - half_size)\n",
    "        max_x = min(self.maze_size, x + half_size + 1)\n",
    "        min_y = max(0, y - half_size)\n",
    "        max_y = min(self.maze_size, y + half_size + 1)\n",
    "\n",
    "        # Điền dữ liệu từ mê cung vào vùng khám phá\n",
    "        for x in range(min_x, max_x):\n",
    "            for y in range(min_y, max_y):\n",
    "                if self.discovered_maze[x + 5, y + 5] == 1:\n",
    "                    self.discovered_maze[x + 5, y + 5] = self.maze[x, y] * 5\n",
    "        \n",
    "    # Phương thức chính để thực hiện hành động trong môi trường\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Thực hiện hành động và cập nhật trạng thái của môi trường.\n",
    "    \n",
    "        Args:\n",
    "        - action (int): Hành động (0: lên, 1: xuống, 2: trái, 3: phải)\n",
    "    \n",
    "        Returns:\n",
    "        - observation (np.array): Vùng quan sát quanh tác tử\n",
    "        - done (bool): Trạng thái kết thúc\n",
    "        \"\"\"\n",
    "        # Lấy vị trí hiện tại của tác tử\n",
    "        x, y = self.agent_position\n",
    "    \n",
    "        # Xác định vị trí mới dựa trên hành động\n",
    "        if action == 0:  # Lên\n",
    "            new_x, new_y = x - 1, y\n",
    "        elif action == 1:  # Xuống\n",
    "            new_x, new_y = x + 1, y\n",
    "        elif action == 2:  # Trái\n",
    "            new_x, new_y = x, y - 1\n",
    "        elif action == 3:  # Phải\n",
    "            new_x, new_y = x, y + 1\n",
    "        \n",
    "        # Khởi tạo biến done\n",
    "        done = False\n",
    "\n",
    "        # Cập nhật vị trí tác tử\n",
    "        if self.valid_check((new_x, new_y)):  # Nếu vị trí mới hợp lệ và không phải là tường\n",
    "            # Nếu vị trí mới hợp lệ, cập nhật vị trí tác tử\n",
    "            self.agent_position = (new_x, new_y)\n",
    "            self.discover_maze()\n",
    "\n",
    "            # Kiểm tra nếu đến đích\n",
    "            if self.agent_position == self.goal_position:\n",
    "                done = True\n",
    "            \n",
    "            # Giảm giá trị ô đã khám phá \n",
    "            if self.discovered_maze[new_x + 5, new_y + 5] > -5:\n",
    "                self.discovered_maze[new_x + 5, new_y + 5] -= 1        \n",
    "                \n",
    "        # Tạo quan sát hiện tại\n",
    "        local_obs = self.get_observation()\n",
    "\n",
    "        #Tạo quan sát toàn mê cung\n",
    "        global_obs = self.downsample(self.discovered_maze)\n",
    "\n",
    "        return local_obs, global_obs, self.agent_position, done\n",
    "\n",
    "    # Phương thức để downsample mê cung\n",
    "    def downsample(self, maze, block_size = 5):\n",
    "        \"\"\"\n",
    "        Downsample toàn bộ mê cung bằng cách lấy tổng của các ô.\n",
    "    \n",
    "        Args:\n",
    "        - maze: Mê cung kích thước lớn (ví dụ: 50x50).\n",
    "        - block_size: Kích thước mỗi khối để downsample (ví dụ: 5x5).\n",
    "    \n",
    "        Returns:\n",
    "        - Downsampled maze.\n",
    "        \"\"\"\n",
    "        from skimage.measure import block_reduce\n",
    "        return block_reduce(maze, block_size=(block_size, block_size), func=np.sum)\n",
    "\n",
    "    # Xuất dữ liệu mê cung\n",
    "    def render(self):\n",
    "        render_maze = np.zeros((self.maze_size, self.maze_size), dtype = int) \n",
    "        for i in range(self.maze_size):\n",
    "            for j in range(self.maze_size):\n",
    "                if self.maze[i][j] == -1:\n",
    "                    render_maze[i][j] = 1\n",
    "                    continue\n",
    "                if self.maze[i][j] == 2:\n",
    "                    render_maze[i][j] = 10\n",
    "                    continue\n",
    "                if self.discovered_maze[i + 5, j + 5] < 0:\n",
    "                    render_maze[i][j] = 2\n",
    "        print(render_maze)\n",
    "    \n",
    "    # Phương thức để lấy quan sát hiện tại của tác tử\n",
    "    def get_observation(self):\n",
    "        x, y = self.agent_position\n",
    "        observation = np.zeros((11,11), dtype = int)\n",
    "        observation[0 : 11, 0 : 11] = self.discovered_maze[x: x + 11, y : y + 11]\n",
    "        return observation\n",
    "\n",
    "    # Phương thức để kích hoạt buff\n",
    "    def activate_buff(self, buff):\n",
    "        self.regenerate_maze()\n",
    "        if buff == 'senrigan':\n",
    "            self.buff = True\n",
    "        else:\n",
    "            self.buff = False\n",
    "        if buff == 'slime-san onegai':\n",
    "            self.bfs(self.agent_position, 50)\n",
    "        if buff == 'tou no hikari':\n",
    "            min_x = max(0, self.agent_position[0] - 10)\n",
    "            max_x = min(self.maze_size, self.agent_position[0] + 11)\n",
    "            min_y = max(0, self.agent_position[1] - 10)\n",
    "            max_y = min(self.maze_size, self.agent_position[1] + 11)\n",
    "            for x in range(min_x, max_x):\n",
    "                for y in range(min_y, max_y):\n",
    "                    if self.discovered_maze[x + 5, y + 5] == 1:\n",
    "                        self.discovered_maze[x + 5, y + 5] = 5 * self.maze[x, y] \n",
    "        if buff == 'unmei no michi':\n",
    "            self.regenerate_maze(1)\n",
    "        return self.get_observation(), self.downsample(self.discovered_maze), self.agent_position\n",
    "    \n",
    "    # Phương thức để kích hoạt debuff\n",
    "    def activate_debuff(self, debuff):\n",
    "        if debuff == 'waamu houru':\n",
    "            self.agent_position = (random.randint(self.maze_size // 3, self.maze_size), random.randint(self.maze_size // 3, self.maze_size))\n",
    "        if debuff == 'shin no meiro':\n",
    "            self.debuff = True\n",
    "        else:\n",
    "            self.debuff = False\n",
    "        return self.regenerate_maze()\n",
    "    \n",
    "    # Thuật toán BFS phục vụ buff slime-san onegai\n",
    "    def bfs(self, position, step = 50):\n",
    "        \"\"\"\n",
    "        Thuật toán BFS (Breadth-First Search)\n",
    "    \n",
    "        Args:\n",
    "        - position: Đỉnh bắt đầu tìm kiếm.\n",
    "        - step: số bược di chuyển\n",
    "    \n",
    "        \"\"\"\n",
    "        # Tập các đỉnh đã duyệt\n",
    "        visited = set()\n",
    "        visited_order = []\n",
    "    \n",
    "        # Hàng đợi (FIFO) để quản lý các đỉnh\n",
    "        queue = deque([position])\n",
    "    \n",
    "        # Bắt đầu duyệt đồ thị\n",
    "        while queue:\n",
    "            # Lấy một đỉnh từ hàng đợi\n",
    "            current = queue.popleft()\n",
    "            x,y = current\n",
    "            \n",
    "            # Kiểm tra nếu đỉnh chưa được duyệt\n",
    "            if current not in visited:\n",
    "                visited.add(current)\n",
    "                visited_order.append(current)\n",
    "                if self.discovered_maze[x + 5, y + 5] == 1:\n",
    "                    self.discovered_maze[5 + x, 5 + y] = 5 * self.maze[x, y]\n",
    "                if len(visited_order) >= step:\n",
    "                    break\n",
    "                neighbors = self.get_neighbors(x, y)\n",
    "                for nx, ny in neighbors:\n",
    "                    if self.maze[nx, ny] > -1 and (nx, ny) not in visited:  # Chỉ đi qua đường\n",
    "                        queue.append((nx, ny))\n",
    "\n",
    "    # Phương thức để kiểm tra tính hợp lệ của một vị trí\n",
    "    def valid_check(self, p1):\n",
    "        if 0 <= p1[0] < self.maze_size and 0 <= p1[1] < self.maze_size and self.maze[p1] > -1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Phương thức xác định hệ số tau\n",
    "    def tau_coefficient(self, coefficient = 2):\n",
    "        return ((abs(self.agent_position[0] - self.goal_position[0]) + abs(self.agent_position[1] - self.goal_position[1])) / (4 / 3 * self.maze_size)) ** coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phương thức để tải model\n",
    "def load_model(model_path, device = \"cuda\"):\n",
    "    # Load the model\n",
    "    model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.eval()  # Chuyển model về chế độ đánh giá (evaluation mode)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải model\n",
    "while True:\n",
    "    model_path = input(\"Enter the model name: \")\n",
    "    if os.path.exists(model_path + \"/model.pth\"):\n",
    "        break\n",
    "    else:\n",
    "        print(\"Model not found. Please try again.\")\n",
    "\n",
    "model = load_model(model_path + \"/model.pth\")\n",
    "with open(model_path + \"/model_info.pkl\", \"rb\") as f:\n",
    "    model_info = pickle.load(f)\n",
    "    buff = model_info['buff']\n",
    "    tau_start = model_info['tau_start']\n",
    "    tau_end = model_info['tau_end']\n",
    "    tau_decay = model_info['tau_decay']\n",
    "    tau_decay_exponent = model_info['tau_decay_exponent']\n",
    "\n",
    "print(f\"Model loaded: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66696aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo các siêu tham số\n",
    "maze_size = 30\n",
    "max_steps = 15\n",
    "path_percent = 70\n",
    "num_actions = 4\n",
    "\n",
    "max_episodes = 50000\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769786e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn hành động\n",
    "def select_action(env, policy_net, local_obs, global_obs, position, tau, num_actions, device):\n",
    "    \"\"\"\n",
    "    Chọn hành động dựa trên chiến lược Boltzmann Exploration.\n",
    "\n",
    "    Args:\n",
    "    - policy_net (nn.Module): Mạng chính để dự đoán giá trị Q(s, a).\n",
    "    - local_obs (np.array): Quan sát cục bộ (ví dụ: 11x11).\n",
    "    - global_obs (np.array): Quan sát toàn bộ mê cung đã downsample (ví dụ: 10x10).\n",
    "    - position (list or np.array): Vị trí hiện tại của tác tử (dx, dy).\n",
    "    - tau (float): Xác suất chọn hành động ngẫu nhiên (khám phá).\n",
    "    - num_actions (int): Số lượng hành động (ví dụ: 4: lên, xuống, trái, phải).\n",
    "    - device (torch.device): Thiết bị thực thi (CPU hoặc GPU).\n",
    "\n",
    "    Returns:\n",
    "    - action (int): Hành động được chọn (0, 1, 2, 3).\n",
    "    \"\"\"\n",
    "    p1 = (position[0] - 1, position[1])\n",
    "    p2 = (position[0] + 1, position[1])\n",
    "    p3 = (position[0], position[1] - 1)\n",
    "    p4 = (position[0], position[1] + 1)\n",
    "\n",
    "    # Loại bỏ các hành động không hợp lệ (nếu cần)\n",
    "    valid_actions = []\n",
    "    if env.valid_check(p1): valid_actions.append(0)  # Lên\n",
    "    if env.valid_check(p2): valid_actions.append(1)  # Xuống\n",
    "    if env.valid_check(p3): valid_actions.append(2)  # Trái\n",
    "    if env.valid_check(p4): valid_actions.append(3)  # Phải\n",
    "\n",
    "    # Chuyển đổi các quan sát thành Tensor để đưa vào mạng\n",
    "    local_obs_tensor = torch.tensor(local_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    global_obs_tensor = torch.tensor(global_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    position_tensor = torch.tensor(position, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # Dự đoán giá trị Q(s, a) cho tất cả các hành động\n",
    "    q_values = policy_net(local_obs_tensor, global_obs_tensor, position_tensor)\n",
    "\n",
    "    # Lấy giá trị Q chỉ cho các hành động hợp lệ\n",
    "    valid_q_values = q_values.squeeze()[valid_actions]  # Chỉ giữ Q của các hành động hợp lệ\n",
    "\n",
    "    # Tính xác suất Boltzmann (softmax)\n",
    "    probabilities = torch.softmax(valid_q_values / tau, dim=0)\n",
    "    \n",
    "    # Chọn hành động dựa trên xác suất Boltzmann\n",
    "    selected_index = torch.multinomial(probabilities, num_samples=1).item()\n",
    "    action = valid_actions[selected_index]\n",
    "    \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vòng lặp chạy chương trình\n",
    "counter = 0\n",
    "step_min = 1000\n",
    "step_max = 0\n",
    "done_count = 0\n",
    "env = MazeEnv(maze_size=maze_size, max_steps=max_steps, path_percent=path_percent)\n",
    "env.reset()\n",
    "env.create_data_set(model_path + \"/dataset\")\n",
    "for episode in range(max_episodes):\n",
    "    if counter % env.max_steps == 0:\n",
    "        if counter:\n",
    "            env.export_maze_training_data()\n",
    "        \n",
    "        # Khởi tạo trạng thái môi trường\n",
    "        local_obs, global_obs, position = env.regenerate_maze()\n",
    "        local_obs, global_obs, position = env.activate_buff(buff)\n",
    "        tau = tau_start * env.tau_coefficient(tau_decay_exponent)\n",
    "    counter += 1\n",
    "\n",
    "    # Chọn hành động\n",
    "    action = select_action(env, model, local_obs, global_obs, position, tau, num_actions, device)\n",
    "\n",
    "    # Thực hiện hành động trong môi trường\n",
    "    local_obs, global_obs, position, done = env.step(action)\n",
    "    \n",
    "    # Giảm tau theo thời gian\n",
    "    tau = max(tau_end, tau * tau_decay)\n",
    "    \n",
    "    # reset môi trường nếu đạt được mục tiêu\n",
    "    if done:\n",
    "        env.export_maze_training_data()\n",
    "        done_count += 1\n",
    "        print(f\"Done counter: {done_count}\")\n",
    "        print(f\"Number of steps: {counter}\")\n",
    "        if counter > step_max:\n",
    "            step_max = counter\n",
    "        if counter < step_min: \n",
    "            step_min = counter\n",
    "        counter = 0\n",
    "        local_obs, global_obs, position = env.reset()\n",
    "        local_obs, global_obs, position = env.activate_buff(buff)\n",
    "        tau = tau_start * env.tau_coefficient(env.tau_coefficient())\n",
    "\n",
    "print(f\"average number of steps: {(max_episodes - counter) // done_count}\")\n",
    "print(f\"max: {step_max}\")\n",
    "print(f\"min: {step_min}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27838bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ghi lại thông tin về chạy model\n",
    "with open(\"run_model.txt\", \"a\") as f:\n",
    "    f.write(f\"Model: {model_path}\\n\")\n",
    "    f.write(f\"Number of episodes: {max_episodes}\\n\")\n",
    "    f.write(f\"Average number of steps: {(max_episodes - counter) // done_count}\\n\")\n",
    "    f.write(f\"Max: {step_max}\\n\")\n",
    "    f.write(f\"Min: {step_min}\\n\")\n",
    "    f.write(\"---------------------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
