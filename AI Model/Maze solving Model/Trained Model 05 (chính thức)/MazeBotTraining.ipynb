{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b7dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các thư viện cần thiết\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces import Discrete\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "604b9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các hằng số sử dụng\n",
    "class Action:\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "class Maze:\n",
    "    WALL = -1\n",
    "    UNEXPLORED = 0\n",
    "    PATH = 1\n",
    "    GOAL = 2\n",
    "    AGENT_POSITION = 3\n",
    "class Buff:\n",
    "    NONE = 0\n",
    "    SENRIGAN = 1\n",
    "    TOU_NO_HIKARI = 2\n",
    "    SLIME_SAN_ONEGAI = 3\n",
    "    UNMEI_NO_MICHI = 4\n",
    "    SLIME_STEP = 100\n",
    "    TOU_NO_HIKARI_OBS = 8\n",
    "    UNMEI_NO_MICHI_PATH = 1\n",
    "class Debuff:\n",
    "    NONE = 0\n",
    "    WAAMU_HOURU = 1\n",
    "    SHIN_NO_MEIRO = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b796e787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention data loaded successfully.\n",
      "Attention data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Khai báo tên model\n",
    "def create_attention_data(path, local_size = 7, numbers_of_keys = 10000):\n",
    "    keys = np.zeros((numbers_of_keys, local_size, local_size), dtype=int)\n",
    "    values = np.zeros((numbers_of_keys, 4))\n",
    "    \n",
    "    with open(path + \"/keys.pkl\", 'wb') as f:\n",
    "        pickle.dump(keys, f)\n",
    "    with open(path + \"/values.pkl\", 'wb') as f:\n",
    "        pickle.dump(values, f)\n",
    "    print(\"Attention data created successfully.\")\n",
    "    return keys, values\n",
    "\n",
    "def load_attention_data(path):\n",
    "    with open(path + \"/keys.pkl\", 'rb') as f:\n",
    "        keys = pickle.load(f)\n",
    "    with open(path + \"/values.pkl\", 'rb') as f:\n",
    "        values = pickle.load(f)\n",
    "    print(\"Attention data loaded successfully.\")\n",
    "    return keys, values\n",
    "\n",
    "while True:\n",
    "    model_name = input(\"Enter model name: \")\n",
    "    if model_name == \"\":\n",
    "        print(\"Model name cannot be empty. Please enter a valid name.\")\n",
    "    else:\n",
    "        if os.path.exists(model_name):\n",
    "            keys, values = load_attention_data(model_name + \"/attention_data\")\n",
    "            keys_senrigan, values_senrigan = load_attention_data(model_name + \"/senrigan_attention_data\")\n",
    "        else:\n",
    "            os.makedirs(model_name, exist_ok=True)\n",
    "            os.makedirs(model_name + \"/attention_data\", exist_ok=True)\n",
    "            os.makedirs(model_name + \"/senrigan_attention_data\", exist_ok=True)\n",
    "            keys, values = create_attention_data(model_name + \"/attention_data\")\n",
    "            keys_senrigan, values_senrigan = create_attention_data(model_name + \"/senrigan_attention_data\", 11)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c404baae-6398-4dd8-9e8c-74c0e6c92fc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Môi trường Mê cung\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    # Môi trường mê cung\n",
    "    # Maze: Mê cung được tạo ra ngẫu nhiên với các ô đường và tường\n",
    "    # Với các ô đường được đánh dấu là 0 và các ô tường được đánh dấu là -1, ô đích được đánh dấu là 2\n",
    "    # Discovered_maze: Mê cung đã được khám phá trong quá trình di chuyển của tác tử\n",
    "    # Với các ô đường được đánh dấu là 0, các ô tường được đánh dấu là -5, đích được đánh dấu là 10, ô chưa được khám phá được đánh dấu là 1\n",
    "    # Giá trị các ô đường giảm dần mỗi lần tác tử đi qua\n",
    "    # Agent_position: Vị trí hiện tại của tác tử trong mê cung\n",
    "    # Base_position: Vị trí ban đầu của tác tử trong mê cung\n",
    "    # Goal_position: Vị trí đích trong mê cung\n",
    "    # Buff: Biến để xác định xem tác tử có đang sử dụng buff senrigan hay không\n",
    "    # Debuff: Biến để xác định xem tác tử có đang bị debuff shin no meiro hay không\n",
    "\n",
    "    def __init__(self, maze_size, local_obs_size = 3, max_steps = 15, path_percent = 70):\n",
    "        \"\"\"\n",
    "        Khởi tạo môi trường Mê cung.\n",
    "\n",
    "        Args:\n",
    "        - maze_size (int): Kích thước của mê cung (ví dụ: 50x50).\n",
    "        - max_steps (int): Số bước tối đa cho mỗi tập.\n",
    "        - path_percent (int): Tỷ lệ phần trăm ô đường trong mê cung (0-100).\n",
    "        \"\"\"\n",
    "        # Đặt các thông số của môi trường\n",
    "        super(MazeEnv, self).__init__()  # Kế thừa từ gym.Env\n",
    "        \n",
    "        self.maze_size = maze_size\n",
    "        self.max_steps = max_steps\n",
    "        self.path_percent = path_percent\n",
    "        self.local_obs_size = local_obs_size\n",
    "        \n",
    "        self.maze = np.ones((maze_size, maze_size), dtype=int) * Maze.UNEXPLORED # Tạo mê cung với tất cả ô là chưa khám phá\n",
    "        self.base_position = (maze_size // 6 - 1, maze_size // 6 - 1)  # Vị trí bắt đầu của tác tử\n",
    "        self.goal_position = (maze_size * 5 // 6, maze_size * 5 // 6)  # Đích cố định tại giữa khu vực đích\n",
    "        self.senrigan_buff = False\n",
    "        self.shin_no_meiro_debuff = False\n",
    "\n",
    "        # Định nghĩa action_space (0: lên, 1: xuống, 2: trái, 3: phải)\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "    # Các phương thức liên quan đến tái tạo mê cung\n",
    "    def reset(self, buff = Buff.NONE, debuff = Debuff.NONE):\n",
    "        self.agent_position = self.base_position\n",
    "        return self.regenerate_maze(buff, debuff)\n",
    "    \n",
    "    def regenerate_maze(self, buff = Buff.NONE, debuff = Debuff.NONE):\n",
    "        # Kích hoạt buff\n",
    "        if buff == Buff.SENRIGAN:\n",
    "            self.senrigan_buff = True\n",
    "        else:\n",
    "            self.senrigan_buff = False\n",
    "        if buff == Buff.UNMEI_NO_MICHI:\n",
    "            adder = Buff.UNMEI_NO_MICHI_PATH\n",
    "        else:\n",
    "            adder = 0\n",
    "        # Kích hoạt debuff\n",
    "        if debuff == Debuff.WAAMU_HOURU:\n",
    "            self.agent_position = (random.randint(self.maze_size // 3, self.maze_size), random.randint(self.maze_size // 3, self.maze_size))\n",
    "        if debuff == Debuff.SHIN_NO_MEIRO:\n",
    "            self.shin_no_meiro_debuff = True\n",
    "        else:\n",
    "            self.shin_no_meiro_debuff = False\n",
    "        # Tạo mê cung mới\n",
    "        self.generate_maze(adder)\n",
    "        self.create_discovered_maze()\n",
    "\n",
    "        # Kích hoạt buff\n",
    "        if buff == Buff.SLIME_SAN_ONEGAI:\n",
    "            self.bfs(self.agent_position, Buff.SLIME_STEP)\n",
    "        if buff == Buff.TOU_NO_HIKARI:\n",
    "            local_obs_size = Buff.TOU_NO_HIKARI_OBS\n",
    "        else:\n",
    "            local_obs_size = 3\n",
    "        self.discover_maze(local_obs_size)\n",
    "\n",
    "        return_discovered_maze = self.discovered_maze.copy()\n",
    "        return (self.get_observation(), return_discovered_maze, self.agent_position)\n",
    "\n",
    "    def generate_maze(self, adder = 0):\n",
    "        if self.agent_position[0] >= self.maze_size * 2 // 3 and self.agent_position[1] >= self.maze_size * 2 // 3:\n",
    "            number_of_path = 1 + adder\n",
    "        else:\n",
    "            number_of_path = 2 + adder\n",
    "\n",
    "        total_cells = self.maze_size * self.maze_size\n",
    "        num_paths = int(total_cells * self.path_percent / 100)\n",
    "        num_walls = total_cells - num_paths\n",
    "        maze_values = [Maze.PATH] * num_paths + [Maze.WALL] * num_walls\n",
    "\n",
    "        while True:  # Sử dụng vòng lặp để tạo lại mê cung nếu không hợp lệ\n",
    "            random.shuffle(maze_values)  # Xáo trộn các giá trị\n",
    "            \n",
    "            # Điền vào mê cung\n",
    "            self.maze = np.array(maze_values).reshape(self.maze_size, self.maze_size)\n",
    "            \n",
    "            # Đặt điểm bắt đầu và đích\n",
    "            self.maze[self.agent_position] = Maze.PATH # Đặt vị trí tác tử là đường \n",
    "            self.maze[self.goal_position] = Maze.GOAL # Đặt vị trí đích\n",
    "\n",
    "            # Kiểm tra tính hợp lệ\n",
    "            if self.validate_maze(number_of_path):\n",
    "                break  # Nếu mê cung hợp lệ, thoát vòng lặp\n",
    "\n",
    "    def validate_maze(self, number_of_path = 0):\n",
    "        d = 0\n",
    "        temp_maze = self.maze.copy()  # Tạo bản sao của mê cung\n",
    "        for _ in range(number_of_path):\n",
    "            path = self.shortest_path(temp_maze)\n",
    "            if path is None:  # Nếu không tìm thấy đường đi\n",
    "                break\n",
    "            d += 1  # Tăng số lượng đường đi hợp lệ\n",
    "            for x, y in path:\n",
    "                temp_maze[x, y] = Maze.WALL \n",
    "                # Đánh dấu các ô đã đi qua là tường\n",
    "            temp_maze[self.agent_position] = Maze.PATH  # Đặt lại vị trí tác tử là đường\n",
    "            temp_maze[self.goal_position] = Maze.GOAL  # Đặt lại vị trí đích là đường\n",
    "\n",
    "        if d == 0:\n",
    "            return self.shin_no_meiro_debuff\n",
    "        if d == number_of_path:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def shortest_path(self, maze):\n",
    "        # Hàng đợi để BFS\n",
    "        queue = deque([(self.agent_position, [self.agent_position])])  # Lưu trữ (vị trí hiện tại, đường đi)\n",
    "        visited = set()  # Tập hợp các điểm đã thăm\n",
    "        visited.add(self.agent_position)\n",
    "        \n",
    "        while queue:\n",
    "            current, path = queue.popleft()\n",
    "\n",
    "            # Kiểm tra nếu đã đến điểm end\n",
    "            if current == self.goal_position:\n",
    "                return path  # Trả về đường đi ngắn nhất\n",
    "            \n",
    "            # Lấy các điểm lân cận\n",
    "            for neighbor in self.get_neighbors(current[0], current[1]):\n",
    "                if neighbor not in visited and maze[neighbor] != Maze.WALL: \n",
    "                    visited.add(neighbor)\n",
    "                    queue.append((neighbor, path + [neighbor]))\n",
    "        \n",
    "        return None  # Không tìm thấy đường đi\n",
    "\n",
    "    def get_neighbors(self, x, y):\n",
    "        \"\"\"\n",
    "        Lấy danh sách các ô lân cận.\n",
    "        \"\"\"\n",
    "        neighbors = []\n",
    "        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < self.maze_size and 0 <= ny < self.maze_size:\n",
    "                neighbors.append((nx, ny))\n",
    "        return neighbors\n",
    "    \n",
    "    # Các phương thức liên quan đến mê cung được khám phá\n",
    "    def create_discovered_maze(self):\n",
    "        self.discovered_maze = np.zeros((self.maze_size + 10, self.maze_size + 10), dtype=int)  # Tạo mê cung đã khám phá\n",
    "        self.discovered_maze.fill(Maze.UNEXPLORED)  # Đánh dấu tất cả ô là chưa khám phá (1)\n",
    "        self.discovered_maze[0 : 5, :] = 5 * Maze.WALL\n",
    "        self.discovered_maze[:, 0 : 5] = 5 * Maze.WALL\n",
    "        self.discovered_maze[self.maze_size + 5 : self.maze_size + 10, :] = 5 * Maze.WALL\n",
    "        self.discovered_maze[:, self.maze_size + 5 : self.maze_size + 10] = 5 * Maze.WALL\n",
    "        self.discovered_maze[self.goal_position[0] + 5, self.goal_position[1] + 5] = 10  # Đích được đánh dấu là 10\n",
    "    def discover_maze(self, local_obs_size = 3):\n",
    "        x, y = self.agent_position\n",
    "        if self.senrigan_buff:\n",
    "            half_size = max(5, local_obs_size)  \n",
    "        else:\n",
    "            half_size = max(3, local_obs_size)  \n",
    "\n",
    "        # Xác định giới hạn của vùng quan sát trong mê cung\n",
    "        min_x = max(0, x - half_size)\n",
    "        max_x = min(self.maze_size, x + half_size + 1)\n",
    "        min_y = max(0, y - half_size)\n",
    "        max_y = min(self.maze_size, y + half_size + 1)\n",
    "\n",
    "        # Điền dữ liệu từ mê cung vào vùng khám phá\n",
    "        for x in range(min_x, max_x):\n",
    "            for y in range(min_y, max_y):\n",
    "                if self.discovered_maze[x + 5, y + 5] == Maze.UNEXPLORED:\n",
    "                    if self.maze[x, y] == Maze.PATH:\n",
    "                        self.discovered_maze[x + 5, y + 5] = 2 * Maze.PATH\n",
    "                    else:\n",
    "                        self.discovered_maze[x + 5, y + 5] = 5 * self.maze[x, y]\n",
    "        \n",
    "    # Phương thức chính để thực hiện hành động trong môi trường\n",
    "    def step(self, action, take_action = True):\n",
    "        \"\"\"\n",
    "        Thực hiện hành động và cập nhật trạng thái của môi trường.\n",
    "    \n",
    "        Args:\n",
    "        - action (int): Hành động \n",
    "    \n",
    "        Returns:\n",
    "        - local_obs (np.array): Quan sát hiện tại của tác tử\n",
    "        - global_obs (np.array): Mê cung đã khám phá\n",
    "        - reward (float): Phần thưởng\n",
    "        - done (bool): Trạng thái kết thúc\n",
    "        \"\"\"\n",
    "        # Lưu lại trạng thái nếu không thực hiện hành động\n",
    "        if not take_action:\n",
    "            saved_position = self.agent_position\n",
    "            saved_discovered_maze = self.discovered_maze.copy()\n",
    "\n",
    "        # Lấy vị trí hiện tại của tác tử\n",
    "        x, y = self.agent_position\n",
    "    \n",
    "        # Xác định vị trí mới dựa trên hành động\n",
    "        if action == Action.UP:  # Lên\n",
    "            new_x, new_y = x - 1, y\n",
    "        elif action == Action.DOWN:  # Xuống\n",
    "            new_x, new_y = x + 1, y\n",
    "        elif action == Action.LEFT:  # Trái\n",
    "            new_x, new_y = x, y - 1\n",
    "        elif action == Action.RIGHT:  # Phải\n",
    "            new_x, new_y = x, y + 1\n",
    "    \n",
    "        # Khởi tạo biến phần thưởng\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Cập nhật vị trí tác tử\n",
    "        if self.valid_check((new_x, new_y)) and self.discovered_maze[new_x + 5, new_y + 5] != 5 * Maze.WALL:  # Nếu vị trí mới hợp lệ và không phải là tường\n",
    "            # Nếu vị trí mới hợp lệ, cập nhật vị trí tác tử\n",
    "            self.agent_position = (new_x, new_y)\n",
    "            self.discover_maze()\n",
    "\n",
    "        if self.discovered_maze[new_x + 5, new_y + 5] == 5 * Maze.WALL:\n",
    "            reward -= 2 * self.maze_size  # Phạt lớn khi va chạm với tường\n",
    "        elif self.agent_position == self.goal_position:\n",
    "            reward += 3 * self.maze_size  # Thưởng lớn khi đến đích\n",
    "            done = True\n",
    "        else:\n",
    "            # Tính khoảng cách Manhattan từ vị trí hiện tại tới đích\n",
    "            current_distance = abs(self.agent_position[0] - self.goal_position[0]) + abs(self.agent_position[1] - self.goal_position[1])\n",
    "            \n",
    "            # Tính khoảng cách trước đó\n",
    "            previous_distance = abs(x - self.goal_position[0]) + abs(y - self.goal_position[1])\n",
    "\n",
    "            # Tính khoảng cách đã đi được\n",
    "            moved_distance = 1 + abs(new_x - self.agent_position[0]) + abs(new_y - self.agent_position[1])\n",
    "\n",
    "            # Tăng thưởng nếu đến gần đích hơn\n",
    "            if self.discovered_maze[new_x + 5, new_y + 5] == 2 * Maze.PATH:  # Nếu ô chưa được khám phá\n",
    "                if current_distance < previous_distance:\n",
    "                    reward += 2 * self.maze_size // current_distance  # Thưởng khi di chuyển đến gần đích hơn\n",
    "                else:\n",
    "                    reward -= 2 * self.maze_size // moved_distance # Phạt nếu tác tử đi xa hơn\n",
    "                self.discovered_maze[new_x + 5, new_y + 5] = -1  \n",
    "            else:\n",
    "                # Phạt khi đi vào ô đã khám phá\n",
    "                reward += self.discovered_maze[new_x + 5, new_y + 5] * self.maze_size // current_distance + self.discovered_maze[new_x + 5, new_y + 5] * self.maze_size // moved_distance\n",
    "                # Giảm giá trị ô đã khám phá\n",
    "                self.discovered_maze[new_x + 5, new_y + 5] -= 1\n",
    "        \n",
    "        # Tạo quan sát hiện tại\n",
    "        local_obs = self.get_observation()\n",
    "\n",
    "        #Tạo quan sát toàn mê cung\n",
    "        return_discovered_maze = self.discovered_maze.copy()\n",
    "        return_agent_position = self.agent_position\n",
    "        # Trả lại các giá trị đã lưu nếu không thực hiện hành động\n",
    "        if not take_action:\n",
    "            self.agent_position = saved_position\n",
    "            self.discovered_maze = saved_discovered_maze.copy()\n",
    "        return local_obs, return_discovered_maze, return_agent_position, reward, done\n",
    "\n",
    "    # Xuất dữ liệu mê cung\n",
    "    def render(self):\n",
    "        render_maze = np.zeros((self.maze_size, self.maze_size), dtype = int) \n",
    "        for i in range(self.maze_size):\n",
    "            for j in range(self.maze_size):\n",
    "                if self.maze[i][j] == Maze.WALL:\n",
    "                    render_maze[i][j] = 1\n",
    "                    continue\n",
    "                if self.maze[i][j] == Maze.GOAL:\n",
    "                    render_maze[i][j] = 10\n",
    "                    continue\n",
    "        print(render_maze)\n",
    "    \n",
    "    # Phương thức để lấy quan sát hiện tại của tác tử\n",
    "    def get_observation(self):\n",
    "        x, y = self.agent_position\n",
    "        observation = np.zeros((2 * self.local_obs_size + 1, 2*self.local_obs_size + 1), dtype = int)\n",
    "        observation[:, :] = self.discovered_maze[x + 5 - self.local_obs_size: x + 6 + self.local_obs_size, y + 5 - self.local_obs_size : y + 6 + self.local_obs_size]\n",
    "        return observation\n",
    "\n",
    "    def bfs(self, position, step):\n",
    "        \"\"\"\n",
    "        Thuật toán BFS (Breadth-First Search)\n",
    "    \n",
    "        Args:\n",
    "        - position: Đỉnh bắt đầu tìm kiếm.\n",
    "        - step: số bược di chuyển\n",
    "    \n",
    "        \"\"\"\n",
    "        # Tập các đỉnh đã duyệt\n",
    "        visited = set()\n",
    "        visited_counter = 0\n",
    "    \n",
    "        # Hàng đợi (FIFO) để quản lý các đỉnh\n",
    "        queue = deque([position])\n",
    "    \n",
    "        # Bắt đầu duyệt đồ thị\n",
    "        while queue:\n",
    "            # Lấy một đỉnh từ hàng đợi\n",
    "            current = queue.popleft()\n",
    "            x,y = current\n",
    "            \n",
    "            # Kiểm tra nếu đỉnh chưa được duyệt\n",
    "            if current not in visited:\n",
    "                visited.add(current)\n",
    "                visited_counter += 1\n",
    "                if self.discovered_maze[x + 5, y + 5] == Maze.UNEXPLORED:\n",
    "                    self.discovered_maze[5 + x, 5 + y] = 5 * self.maze[x, y]\n",
    "                    \n",
    "                # Nếu đã duyệt đủ số bước, dừng lại\n",
    "                if visited_counter >= step:\n",
    "                    break\n",
    "                neighbors = self.get_neighbors(x, y)\n",
    "                for nx, ny in neighbors:\n",
    "                    if self.maze[nx, ny] != Maze.WALL and (nx, ny) not in visited:  # Chỉ đi qua đường\n",
    "                        queue.append((nx, ny))\n",
    "    # Phương thức để kiểm tra tính hợp lệ của một vị trí\n",
    "    def valid_check(self, p1):\n",
    "        if 0 <= p1[0] < self.maze_size and 0 <= p1[1] < self.maze_size and self.maze[p1] != Maze.WALL:\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d000311c-41eb-4e8c-9b70-d640e978f328",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Mạng Nơ ron \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def format(self, x):\n",
    "        return F.relu(-x / 5)\n",
    "    \n",
    "    def forward(self, queries, keys, values):\n",
    "        x_queries = self.format(queries).expand(queries.size(0), keys.size(0), queries.size(2), queries.size(3))\n",
    "        x_keys = keys.unsqueeze(0).expand(queries.size(0), keys.size(0), keys.size(1), keys.size(2))\n",
    "        x_values = values.unsqueeze(0).expand(queries.size(0), keys.size(0), values.size(1))\n",
    "        simularity = self.calculate_similarity(x_queries, x_keys)\n",
    "        attention_weights = self.softmax(simularity)\n",
    "        attention_weights = attention_weights.unsqueeze(2).expand(queries.size(0), keys.size(0), values.size(1))\n",
    "        output = (attention_weights * x_values).sum(dim = (1))\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def calculate_similarity(self, queries, keys):\n",
    "        similarity = (queries * keys).sum(dim = (-2, -1))\n",
    "        similarity = similarity / (((queries.size(2) * queries.size(3)) ** 0.5))  # Chia cho kích thước của các chiều\n",
    "        return similarity\n",
    "    \n",
    "class MazeNetCombined(nn.Module):\n",
    "    def __init__(self, maze_size, goal_position):\n",
    "        super(MazeNetCombined, self).__init__()\n",
    "        self.maze_size = maze_size\n",
    "        self.goal_position = goal_position\n",
    "\n",
    "        # Quan sát cục bộ\n",
    "        self.conv1_local = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2_local = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3_local = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4_local = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Quan sát toàn cục\n",
    "        self.conv1_global = nn.Conv2d(1, 32, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2_global = nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Xử lý vị trí\n",
    "        self.fc_position = nn.Linear(2, 32)  # Vị trí hiện tại\n",
    "\n",
    "        # Cơ chế attention ánh xạ đến các trường hợp đặc biệt\n",
    "        self.attention_block = AttentionBlock()\n",
    "\n",
    "        # Tầng Fully Connected cuối cùng\n",
    "        self.fc1 = nn.LazyLinear(256)\n",
    "        self.fc2 = nn.LazyLinear(128) \n",
    "        self.dropout_fc = nn.Dropout(p=0.5)  # Dropout trước tầng FC3\n",
    "        self.fc3 = nn.LazyLinear(4)  # Đầu ra cho 4 hành động (lên, xuống, trái, phải)\n",
    "    \n",
    "    def forward(self, local_obs, global_obs, position, keys, values):\n",
    "        # Xử lý local_obs\n",
    "        x_local = F.relu(self.conv1_local(local_obs))\n",
    "        x_local = F.relu(self.conv2_local(x_local))\n",
    "        x_local = F.relu(self.conv3_local(x_local))\n",
    "        x_local = F.relu(self.conv4_local(x_local))\n",
    "        x_local = x_local.view(x_local.size(0), -1)\n",
    "        \n",
    "        # Xử lý global_obs\n",
    "        x_global = F.relu(self.conv1_global(global_obs))\n",
    "        x_global = F.relu(self.conv2_global(x_global))\n",
    "        x_global = x_global.view(x_global.size(0), -1)\n",
    "\n",
    "        # Xử lý vị trí hiện tại\n",
    "        x_position = F.relu(self.fc_position(position))\n",
    "        \n",
    "        # Kết hợp tất cả\n",
    "        x = torch.cat((x_local, x_global, x_position), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout_fc(x)  # Dropout trước FC3\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # Tính toán attention\n",
    "        attention_output, attention_weights = self.attention_block(local_obs, keys, values)\n",
    "        return x + attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c3e95ff-7bb2-422c-9292-206a310eb14f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, path, save_prob=0.1):\n",
    "        \"\"\"\n",
    "        Khởi tạo Replay Buffer.\n",
    "        Args:\n",
    "            capacity (int): Sức chứa của buffer chính.\n",
    "            path (str): Đường dẫn để load/lưu buffer (saved_buffer).\n",
    "            save_prob (float): Xác suất lưu trải nghiệm mới vào saved_buffer.\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.saved_buffer = []  # Buffer lưu từ tệp\n",
    "        self.path = path\n",
    "        self.save_prob = save_prob\n",
    "        \n",
    "        # Tải replay buffer đã lưu nếu tồn tại\n",
    "        self._load_saved_buffer()\n",
    "\n",
    "    def _load_saved_buffer(self):\n",
    "        \"\"\"Tải saved_buffer từ tệp.\"\"\"\n",
    "        try:\n",
    "            with open(self.path, 'rb') as f:\n",
    "                self.saved_buffer = pickle.load(f)\n",
    "            with open(self.path, 'rb') as f:\n",
    "                self.buffer = pickle.load(f)\n",
    "            print(f\"Saved buffer loaded successfully from {self.path}.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No saved buffer found at {self.path}. Starting with an empty saved buffer.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading saved buffer: {e}. Starting with an empty saved buffer.\")\n",
    "\n",
    "    def save_saved_buffer(self):\n",
    "        \"\"\"Lưu saved_buffer vào tệp tại path.\"\"\"\n",
    "        try:\n",
    "            with open(self.path, 'wb') as f:\n",
    "                pickle.dump(self.saved_buffer, f)\n",
    "            print(f\"Saved buffer successfully saved to {self.path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving saved buffer: {e}.\")\n",
    "\n",
    "    def push(self, experience):\n",
    "        \"\"\"Thêm một trải nghiệm vào buffer chính và có xác suất thêm vào saved_buffer.\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "        # Với xác suất save_prob, thêm trải nghiệm vào saved_buffer\n",
    "        if np.random.rand() < self.save_prob:\n",
    "            if len(self.saved_buffer) >= self.capacity:\n",
    "                # Nếu saved_buffer đã đầy, xóa trải nghiệm cũ nhất\n",
    "                self.saved_buffer.pop(0)\n",
    "            self.saved_buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Trích xuất batch mẫu từ buffer chính.\"\"\"\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        # Tách dữ liệu thành các phần riêng biệt\n",
    "        local_obs, global_obs, position, actions, rewards, next_local_obs, next_global_obs, next_position, dones = zip(*batch)\n",
    "        return (np.array(local_obs), np.array(global_obs), np.array(position), \n",
    "                np.array(actions), np.array(rewards), np.array(next_local_obs), \n",
    "                np.array(next_global_obs), np.array(next_position), np.array(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c6359cf-4e38-4694-8f99-cb823c37e1c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cập nhật model\n",
    "def update_model(policy_net, target_net, replay_buffer, optimizer, batch_size, gamma, device, goal_position):\n",
    "    \"\"\"\n",
    "    Cập nhật mô hình chính (policy network) cho mạng có đầu vào đa dạng (local_obs, global_obs, position).\n",
    "\n",
    "    Args:\n",
    "    - policy_net (nn.Module): Mạng chính dự đoán giá trị Q(s, a).\n",
    "    - target_net (nn.Module): Mạng mục tiêu dùng để tính Q_target.\n",
    "    - replay_buffer (ReplayBuffer): Bộ nhớ replay chứa các kinh nghiệm (local_obs, global_obs, position, action, reward, next_local_obs, next_global_obs, next_position, done).\n",
    "    - optimizer (torch.optim.Optimizer): Trình tối ưu hóa (Adam, SGD, ...).\n",
    "    - batch_size (int): Kích thước batch mẫu từ replay buffer.\n",
    "    - gamma (float): Hệ số chiết khấu (discount factor).\n",
    "    - device (torch.device): Thiết bị thực thi (CPU hoặc GPU).\n",
    "\n",
    "    Returns:\n",
    "    - loss (float): Giá trị mất mát (loss) sau khi cập nhật.\n",
    "    \"\"\"\n",
    "    # Kiểm tra nếu replay buffer chưa đủ dữ liệu\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # 1. Lấy mẫu từ replay buffer\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    (local_obs, global_obs, position, actions, rewards, \n",
    "     next_local_obs, next_global_obs, next_position, dones) = batch\n",
    "\n",
    "    # 2. Chuyển đổi dữ liệu sang Tensor và đưa vào thiết bị (CPU/GPU)\n",
    "    local_obs = torch.tensor(local_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh cho CNN\n",
    "    global_obs = torch.tensor(global_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh\n",
    "    position = torch.tensor(position, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_local_obs = torch.tensor(next_local_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh\n",
    "    next_global_obs = torch.tensor(next_global_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh\n",
    "    next_position = torch.tensor(next_position, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 3. Dự đoán Q(s, a) từ policy_net\n",
    "    q_values = policy_net(local_obs, global_obs, position, keys, values)  # Đầu ra: (batch_size, num_actions)\n",
    "    q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Lấy giá trị Q(s, a) cho hành động đã thực hiện\n",
    "\n",
    "    # 4. Tính toán Q_target bằng target_net\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_local_obs, next_global_obs, next_position, keys, values)  # Dự đoán Q(s', a') từ target_net\n",
    "        max_next_q_values = next_q_values.max(1)[0]  # Lấy giá trị lớn nhất Q(s', a')\n",
    "        q_targets = rewards + gamma * max_next_q_values * (1 - dones)  # Hàm Bellman\n",
    "\n",
    "    # 5. Tính hàm mất mát   \n",
    "    loss = F.mse_loss(q_values, q_targets)\n",
    "\n",
    "    # 6. Tối ưu hóa mô hình\n",
    "    optimizer.zero_grad()  # Xóa gradient cũ\n",
    "    loss.backward()  # Lan truyền ngược (backpropagation)\n",
    "    optimizer.step()  # Cập nhật trọng số\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "682b6dba-bdca-4cc4-a4fb-b21725ba7e30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Chọn hành động\n",
    "def select_action(env, policy_net, local_obs, global_obs, position, device):\n",
    "    \"\"\"\n",
    "    Chọn hành động dựa trên chiến lược Boltzmann Exploration.\n",
    "\n",
    "    Args:\n",
    "    - policy_net (nn.Module): Mạng chính để dự đoán giá trị Q(s, a).\n",
    "    - local_obs (np.array): Quan sát cục bộ (ví dụ: 11x11).\n",
    "    - global_obs (np.array): Quan sát toàn bộ mê cung (ví dụ: 50x50).\n",
    "    - position (list or np.array): Vị trí hiện tại của tác tử (dx, dy).\n",
    "    - device (torch.device): Thiết bị thực thi (CPU hoặc GPU).\n",
    "\n",
    "    Returns:\n",
    "    - action (int): Hành động được chọn (0, 1, 2, 3).\n",
    "    \"\"\"\n",
    "    p1 = (position[0] - 1, position[1])\n",
    "    p2 = (position[0] + 1, position[1])\n",
    "    p3 = (position[0], position[1] - 1)\n",
    "    p4 = (position[0], position[1] + 1)\n",
    "\n",
    "    # Loại bỏ các hành động không hợp lệ (nếu cần)\n",
    "    valid_actions = []\n",
    "    if env.valid_check(p1): valid_actions.append(Action.UP)  # Lên\n",
    "    if env.valid_check(p2): valid_actions.append(Action.DOWN)  # Xuống\n",
    "    if env.valid_check(p3): valid_actions.append(Action.LEFT)  # Trái\n",
    "    if env.valid_check(p4): valid_actions.append(Action.RIGHT)  # Phải\n",
    "\n",
    "    # Chuyển đổi các quan sát thành Tensor để đưa vào mạng\n",
    "    local_obs_tensor = torch.tensor(local_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    global_obs_tensor = torch.tensor(global_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    agent_position_tensor = torch.tensor(position, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # Dự đoán giá trị Q(s, a) cho tất cả các hành động\n",
    "    q_values = policy_net(local_obs_tensor, global_obs_tensor, agent_position_tensor, keys, values) \n",
    "\n",
    "    valid_q_values = q_values.squeeze()[valid_actions]  # Chỉ giữ Q của các hành động hợp lệ\n",
    "    selected_index = valid_q_values.argmax().item()  # Chọn hành động có giá trị Q lớn nhất\n",
    "    action = valid_actions[selected_index]\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b4b6527-7fe7-4c1b-bcbc-894d259e3afd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Đồng bộ với mạng mục tiêu\n",
    "def sync_target_network(q_network, target_network):\n",
    "    target_network.load_state_dict(q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5a797c7-f96e-4e97-a03b-824daaae83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu, tải trọng số của mô hình\n",
    "\n",
    "# Lưu trọng số của mô hình\n",
    "def save_model(model, path):\n",
    "    torch.save(model, path)\n",
    "    print(f\"Model saved to: {path}\")\n",
    "# Tải trọng số của mô hình\n",
    "def load_model(path, device = \"cuda\"):\n",
    "    policy_model = torch.load(path, map_location=device, weights_only=False)  # Tải mô hình từ file\n",
    "    target_model = policy_model\n",
    "    target_model.eval() # Đặt target_model ở chế độ đánh giá\n",
    "    print(F\"Model loaded from: {path}\")\n",
    "    return policy_model, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83b2070f-2f24-41e0-bd60-d400df0fbe81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Model loaded from: model01/model.pth\n",
      "Saved buffer loaded successfully from model01/replay_buffer.pkl.\n",
      "Initialization complete!\n",
      "Bot info file created!\n",
      "Model info file created!\n",
      "Training info file created!\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo các siêu tham số\n",
    "\n",
    "# Thiết bị thực thi (CPU hoặc GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Chuyển attention_data sang device\n",
    "attention_data = torch.tensor([0, 0], dtype=torch.float32).to(device)\n",
    "senrigan_attention_data = torch.tensor([0, 0], dtype=torch.float32).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.8  # Hệ số chiết khấu (discount factor)\n",
    "learning_rate = 1e-3  # Learning rate cho optimizer\n",
    "weight_decay = 1e-3 # weight decay cho optimizer\n",
    "batch_size = 64  # Kích thước batch khi lấy mẫu từ replay buffer\n",
    "target_update_frequency = 100  # Số vòng lặp huấn luyện trước khi đồng bộ target_net với policy_net\n",
    "max_episodes = 10000  # Số lượng tập (episodes) tối đa\n",
    "replay_buffer_capacity = 20000  # Dung lượng bộ nhớ replay buffer\n",
    "\n",
    "# Thông tin về môi trường\n",
    "maze_size = 20 # Kích thước mê cung (30x30)\n",
    "max_steps = 15  # Số bước tối đa trong mỗi tập (episode)\n",
    "path_percent = 70  # Tỷ lệ phần trăm ô đường đi trong mê cung (70% đường đi, 30% tường)\n",
    "buff = Buff.NONE  # Buff cho tác tử (nếu có)\n",
    "local_obs_size = 3\n",
    "env = MazeEnv(maze_size=maze_size, local_obs_size=local_obs_size, max_steps=max_steps, path_percent=path_percent)\n",
    "env.reset()\n",
    "\n",
    "# Mạng chính và mạng mục tiêu\n",
    "load = True and os.path.exists(model_name + \"/model.pth\")\n",
    "# Nếu có trọng số đã lưu, tải chúng vào mô hình\n",
    "if load:\n",
    "    policy_net, target_net = load_model(model_name + \"/model.pth\", device)\n",
    "else:\n",
    "    # Khởi tạo mạng chính (policy_net) và mạng mục tiêu (target_net)\n",
    "    policy_net = MazeNetCombined(maze_size, env.goal_position).to(device)\n",
    "    target_net = MazeNetCombined(maze_size, env.goal_position).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())  # Đồng bộ hóa trọng số ban đầu\n",
    "    target_net.eval()  # Đặt target_net ở chế độ đánh giá\n",
    "\n",
    "# Chuyển keys và values sang device\n",
    "if buff == Buff.SENRIGAN:\n",
    "    keys = torch.tensor(keys_senrigan, dtype=torch.float32).to(device)\n",
    "    values = torch.tensor(values_senrigan, dtype=torch.float32).to(device)\n",
    "else:\n",
    "    keys = torch.tensor(keys, dtype=torch.float32).to(device)\n",
    "    values = torch.tensor(values, dtype=torch.float32).to(device)\n",
    "    \n",
    "\n",
    "save_rate = 0.15  # Tỷ lệ lưu replay buffer (15%)\n",
    "replay_buffer = ReplayBuffer(replay_buffer_capacity, model_name + \"/replay_buffer.pkl\", save_rate)  # Khởi tạo replay buffer\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "print(\"Initialization complete!\")\n",
    "\n",
    "# Lưu thông tin các siêu tham số vào file\n",
    "with open(model_name + \"/bot.info\", \"w\") as file:\n",
    "    file.write(\"Maze Environment Configuration:\\n\")\n",
    "    file.write(f\"maze_size: {maze_size}\\n\")\n",
    "    file.write(f\"max_steps: {max_steps}\\n\")\n",
    "    file.write(f\"path_percent: {path_percent}\\n\")\n",
    "    file.write(f\"buff: {buff}\\n\")\n",
    "    \n",
    "    file.write(\"\\nNeural Network Configuration:\\n\")\n",
    "    file.write(f\"local_obs_size: {local_obs_size}\\n\")\n",
    "\n",
    "    file.write(\"\\nTraining Configuration:\\n\")\n",
    "    file.write(f\"gamma: {gamma}\\n\")\n",
    "    file.write(f\"learning_rate: {learning_rate}\\n\")\n",
    "    file.write(f\"weight_decay: {weight_decay}\\n\")\n",
    "    file.write(f\"batch_size: {batch_size}\\n\")\n",
    "    file.write(f\"target_update_frequency: {target_update_frequency}\\n\")\n",
    "    file.write(f\"max_episodes: {max_episodes}\\n\")\n",
    "    file.write(f\"replay_buffer_capacity: {replay_buffer_capacity}\\n\")\n",
    "\n",
    "    file.write(\"\\nReplay Buffer Configuration:\\n\")\n",
    "    file.write(f\"save_rate: {save_rate}\\n\")\n",
    "print(\"Bot info file created!\")\n",
    "\n",
    "# Tạo model info\n",
    "model_info = {}\n",
    "model_info['model_path'] = model_name + \"/model.pth\"\n",
    "model_info['buff'] = buff\n",
    "\n",
    "with open(model_name + \"/model_info.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model_info, file)\n",
    "print(\"Model info file created!\")\n",
    "\n",
    "# Tạo training_info\n",
    "training_info = {}\n",
    "training_info['gamma'] = gamma\n",
    "training_info['learning_rate'] = learning_rate\n",
    "training_info['weight_decay'] = weight_decay\n",
    "\n",
    "with open(model_name + \"/training_info.pkl\", \"wb\") as file:\n",
    "    pickle.dump(training_info, file)\n",
    "print(\"Training info file created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9db2b6af-b40a-41ae-81d5-8f390bc365ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done counter: 1\n",
      "Number of steps: 30\n",
      "Done counter: 2\n",
      "Number of steps: 30\n",
      "Done counter: 3\n",
      "Number of steps: 32\n",
      "Done counter: 4\n",
      "Number of steps: 60\n",
      "Done counter: 5\n",
      "Number of steps: 30\n",
      "Done counter: 6\n",
      "Number of steps: 46\n",
      "Done counter: 7\n",
      "Number of steps: 52\n",
      "Done counter: 8\n",
      "Number of steps: 36\n",
      "Done counter: 9\n",
      "Number of steps: 48\n",
      "Done counter: 10\n",
      "Number of steps: 38\n",
      "Done counter: 11\n",
      "Number of steps: 30\n",
      "Done counter: 12\n",
      "Number of steps: 48\n",
      "Done counter: 13\n",
      "Number of steps: 28\n",
      "Done counter: 14\n",
      "Number of steps: 28\n",
      "Done counter: 15\n",
      "Number of steps: 30\n",
      "Done counter: 16\n",
      "Number of steps: 30\n",
      "Done counter: 17\n",
      "Number of steps: 48\n",
      "Done counter: 18\n",
      "Number of steps: 34\n",
      "Done counter: 19\n",
      "Number of steps: 34\n",
      "Done counter: 20\n",
      "Number of steps: 40\n",
      "Done counter: 21\n",
      "Number of steps: 44\n",
      "Done counter: 22\n",
      "Number of steps: 34\n",
      "Done counter: 23\n",
      "Number of steps: 38\n",
      "Done counter: 24\n",
      "Number of steps: 48\n",
      "Done counter: 25\n",
      "Number of steps: 34\n",
      "Done counter: 26\n",
      "Number of steps: 44\n",
      "Done counter: 27\n",
      "Number of steps: 28\n",
      "Done counter: 28\n",
      "Number of steps: 50\n",
      "Done counter: 29\n",
      "Number of steps: 34\n",
      "Done counter: 30\n",
      "Number of steps: 30\n",
      "Done counter: 31\n",
      "Number of steps: 34\n",
      "Done counter: 32\n",
      "Number of steps: 32\n",
      "Done counter: 33\n",
      "Number of steps: 42\n",
      "Done counter: 34\n",
      "Number of steps: 30\n",
      "Done counter: 35\n",
      "Number of steps: 32\n",
      "Done counter: 36\n",
      "Number of steps: 30\n",
      "Done counter: 37\n",
      "Number of steps: 32\n",
      "Done counter: 38\n",
      "Number of steps: 38\n",
      "Done counter: 39\n",
      "Number of steps: 46\n",
      "Done counter: 40\n",
      "Number of steps: 28\n",
      "Done counter: 41\n",
      "Number of steps: 32\n",
      "Done counter: 42\n",
      "Number of steps: 42\n",
      "Done counter: 43\n",
      "Number of steps: 30\n",
      "Done counter: 44\n",
      "Number of steps: 28\n",
      "Done counter: 45\n",
      "Number of steps: 28\n",
      "Done counter: 46\n",
      "Number of steps: 34\n",
      "Done counter: 47\n",
      "Number of steps: 40\n",
      "Done counter: 48\n",
      "Number of steps: 38\n",
      "Done counter: 49\n",
      "Number of steps: 45\n",
      "Done counter: 50\n",
      "Number of steps: 79\n",
      "Done counter: 51\n",
      "Number of steps: 32\n",
      "Done counter: 52\n",
      "Number of steps: 30\n",
      "Done counter: 53\n",
      "Number of steps: 36\n",
      "Done counter: 54\n",
      "Number of steps: 42\n",
      "Done counter: 55\n",
      "Number of steps: 40\n",
      "Done counter: 56\n",
      "Number of steps: 34\n",
      "Done counter: 57\n",
      "Number of steps: 28\n",
      "Done counter: 58\n",
      "Number of steps: 40\n",
      "Done counter: 59\n",
      "Number of steps: 36\n",
      "Done counter: 60\n",
      "Number of steps: 36\n",
      "Done counter: 61\n",
      "Number of steps: 42\n",
      "Done counter: 62\n",
      "Number of steps: 36\n",
      "Done counter: 63\n",
      "Number of steps: 46\n",
      "Done counter: 64\n",
      "Number of steps: 42\n",
      "Done counter: 65\n",
      "Number of steps: 34\n",
      "Done counter: 66\n",
      "Number of steps: 32\n",
      "Done counter: 67\n",
      "Number of steps: 42\n",
      "Done counter: 68\n",
      "Number of steps: 38\n",
      "Done counter: 69\n",
      "Number of steps: 38\n",
      "Done counter: 70\n",
      "Number of steps: 32\n",
      "Done counter: 71\n",
      "Number of steps: 48\n",
      "Done counter: 72\n",
      "Number of steps: 32\n",
      "Done counter: 73\n",
      "Number of steps: 30\n",
      "Done counter: 74\n",
      "Number of steps: 30\n",
      "Done counter: 75\n",
      "Number of steps: 42\n",
      "Done counter: 76\n",
      "Number of steps: 28\n",
      "Done counter: 77\n",
      "Number of steps: 32\n",
      "Done counter: 78\n",
      "Number of steps: 30\n",
      "Done counter: 79\n",
      "Number of steps: 36\n",
      "Done counter: 80\n",
      "Number of steps: 30\n",
      "Done counter: 81\n",
      "Number of steps: 40\n",
      "Done counter: 82\n",
      "Number of steps: 32\n",
      "Done counter: 83\n",
      "Number of steps: 32\n",
      "Done counter: 84\n",
      "Number of steps: 30\n",
      "Done counter: 85\n",
      "Number of steps: 28\n",
      "Done counter: 86\n",
      "Number of steps: 32\n",
      "Done counter: 87\n",
      "Number of steps: 36\n",
      "Done counter: 88\n",
      "Number of steps: 40\n",
      "Done counter: 89\n",
      "Number of steps: 30\n",
      "Done counter: 90\n",
      "Number of steps: 28\n",
      "Done counter: 91\n",
      "Number of steps: 30\n",
      "Done counter: 92\n",
      "Number of steps: 38\n",
      "Done counter: 93\n",
      "Number of steps: 36\n",
      "Done counter: 94\n",
      "Number of steps: 32\n",
      "Done counter: 95\n",
      "Number of steps: 28\n",
      "Done counter: 96\n",
      "Number of steps: 30\n",
      "Done counter: 97\n",
      "Number of steps: 28\n",
      "Done counter: 98\n",
      "Number of steps: 48\n",
      "Done counter: 99\n",
      "Number of steps: 50\n",
      "Done counter: 100\n",
      "Number of steps: 32\n",
      "Done counter: 101\n",
      "Number of steps: 52\n",
      "Done counter: 102\n",
      "Number of steps: 36\n",
      "Done counter: 103\n",
      "Number of steps: 40\n",
      "Done counter: 104\n",
      "Number of steps: 30\n",
      "Done counter: 105\n",
      "Number of steps: 30\n",
      "Done counter: 106\n",
      "Number of steps: 42\n",
      "Done counter: 107\n",
      "Number of steps: 34\n",
      "Done counter: 108\n",
      "Number of steps: 46\n",
      "Done counter: 109\n",
      "Number of steps: 32\n",
      "Done counter: 110\n",
      "Number of steps: 58\n",
      "Done counter: 111\n",
      "Number of steps: 30\n",
      "Done counter: 112\n",
      "Number of steps: 38\n",
      "Done counter: 113\n",
      "Number of steps: 66\n",
      "Done counter: 114\n",
      "Number of steps: 42\n",
      "Done counter: 115\n",
      "Number of steps: 36\n",
      "Done counter: 116\n",
      "Number of steps: 36\n",
      "Done counter: 117\n",
      "Number of steps: 30\n",
      "Done counter: 118\n",
      "Number of steps: 42\n",
      "Done counter: 119\n",
      "Number of steps: 42\n",
      "Done counter: 120\n",
      "Number of steps: 51\n",
      "Done counter: 121\n",
      "Number of steps: 30\n",
      "Done counter: 122\n",
      "Number of steps: 30\n",
      "Done counter: 123\n",
      "Number of steps: 38\n",
      "Done counter: 124\n",
      "Number of steps: 40\n",
      "Done counter: 125\n",
      "Number of steps: 34\n",
      "Done counter: 126\n",
      "Number of steps: 32\n",
      "Done counter: 127\n",
      "Number of steps: 28\n",
      "Done counter: 128\n",
      "Number of steps: 34\n",
      "Done counter: 129\n",
      "Number of steps: 44\n",
      "Done counter: 130\n",
      "Number of steps: 34\n",
      "Done counter: 131\n",
      "Number of steps: 32\n",
      "Done counter: 132\n",
      "Number of steps: 28\n",
      "Done counter: 133\n",
      "Number of steps: 34\n",
      "Done counter: 134\n",
      "Number of steps: 30\n",
      "Done counter: 135\n",
      "Number of steps: 50\n",
      "Done counter: 136\n",
      "Number of steps: 34\n",
      "Done counter: 137\n",
      "Number of steps: 36\n",
      "Done counter: 138\n",
      "Number of steps: 54\n",
      "Done counter: 139\n",
      "Number of steps: 44\n",
      "Done counter: 140\n",
      "Number of steps: 54\n",
      "Done counter: 141\n",
      "Number of steps: 34\n",
      "Done counter: 142\n",
      "Number of steps: 30\n",
      "Done counter: 143\n",
      "Number of steps: 36\n",
      "Done counter: 144\n",
      "Number of steps: 28\n",
      "Done counter: 145\n",
      "Number of steps: 32\n",
      "Done counter: 146\n",
      "Number of steps: 36\n",
      "Done counter: 147\n",
      "Number of steps: 34\n",
      "Done counter: 148\n",
      "Number of steps: 32\n",
      "Done counter: 149\n",
      "Number of steps: 34\n",
      "Done counter: 150\n",
      "Number of steps: 46\n",
      "Done counter: 151\n",
      "Number of steps: 36\n",
      "Done counter: 152\n",
      "Number of steps: 34\n",
      "Done counter: 153\n",
      "Number of steps: 40\n",
      "Done counter: 154\n",
      "Number of steps: 40\n",
      "Done counter: 155\n",
      "Number of steps: 36\n",
      "Done counter: 156\n",
      "Number of steps: 40\n",
      "Done counter: 157\n",
      "Number of steps: 34\n",
      "Done counter: 158\n",
      "Number of steps: 52\n",
      "Done counter: 159\n",
      "Number of steps: 36\n",
      "Done counter: 160\n",
      "Number of steps: 30\n",
      "Done counter: 161\n",
      "Number of steps: 40\n",
      "Done counter: 162\n",
      "Number of steps: 40\n",
      "Done counter: 163\n",
      "Number of steps: 36\n",
      "Done counter: 164\n",
      "Number of steps: 46\n",
      "Done counter: 165\n",
      "Number of steps: 36\n",
      "Done counter: 166\n",
      "Number of steps: 34\n",
      "Done counter: 167\n",
      "Number of steps: 30\n",
      "Done counter: 168\n",
      "Number of steps: 44\n",
      "Done counter: 169\n",
      "Number of steps: 36\n",
      "Done counter: 170\n",
      "Number of steps: 47\n",
      "Done counter: 171\n",
      "Number of steps: 30\n",
      "Done counter: 172\n",
      "Number of steps: 32\n",
      "Done counter: 173\n",
      "Number of steps: 34\n",
      "Done counter: 174\n",
      "Number of steps: 36\n",
      "Done counter: 175\n",
      "Number of steps: 46\n",
      "Done counter: 176\n",
      "Number of steps: 46\n",
      "Done counter: 177\n",
      "Number of steps: 32\n",
      "Done counter: 178\n",
      "Number of steps: 46\n",
      "Done counter: 179\n",
      "Number of steps: 36\n",
      "Done counter: 180\n",
      "Number of steps: 54\n",
      "Done counter: 181\n",
      "Number of steps: 36\n",
      "Done counter: 182\n",
      "Number of steps: 38\n",
      "Done counter: 183\n",
      "Number of steps: 36\n",
      "Done counter: 184\n",
      "Number of steps: 42\n",
      "Done counter: 185\n",
      "Number of steps: 40\n",
      "Done counter: 186\n",
      "Number of steps: 42\n",
      "Done counter: 187\n",
      "Number of steps: 48\n",
      "Done counter: 188\n",
      "Number of steps: 48\n",
      "Done counter: 189\n",
      "Number of steps: 40\n",
      "Done counter: 190\n",
      "Number of steps: 28\n",
      "Done counter: 191\n",
      "Number of steps: 46\n",
      "Done counter: 192\n",
      "Number of steps: 32\n",
      "Done counter: 193\n",
      "Number of steps: 36\n",
      "Done counter: 194\n",
      "Number of steps: 38\n",
      "Done counter: 195\n",
      "Number of steps: 44\n",
      "Done counter: 196\n",
      "Number of steps: 38\n",
      "Done counter: 197\n",
      "Number of steps: 30\n",
      "Done counter: 198\n",
      "Number of steps: 28\n",
      "Done counter: 199\n",
      "Number of steps: 40\n",
      "Done counter: 200\n",
      "Number of steps: 30\n",
      "Done counter: 201\n",
      "Number of steps: 30\n",
      "Done counter: 202\n",
      "Number of steps: 30\n",
      "Done counter: 203\n",
      "Number of steps: 46\n",
      "Done counter: 204\n",
      "Number of steps: 40\n",
      "Done counter: 205\n",
      "Number of steps: 42\n",
      "Done counter: 206\n",
      "Number of steps: 32\n",
      "Done counter: 207\n",
      "Number of steps: 38\n",
      "Done counter: 208\n",
      "Number of steps: 32\n",
      "Done counter: 209\n",
      "Number of steps: 34\n",
      "Done counter: 210\n",
      "Number of steps: 40\n",
      "Done counter: 211\n",
      "Number of steps: 36\n",
      "Done counter: 212\n",
      "Number of steps: 30\n",
      "Done counter: 213\n",
      "Number of steps: 28\n",
      "Done counter: 214\n",
      "Number of steps: 40\n",
      "Done counter: 215\n",
      "Number of steps: 44\n",
      "Done counter: 216\n",
      "Number of steps: 38\n",
      "Done counter: 217\n",
      "Number of steps: 32\n",
      "Done counter: 218\n",
      "Number of steps: 32\n",
      "Done counter: 219\n",
      "Number of steps: 34\n",
      "Done counter: 220\n",
      "Number of steps: 38\n",
      "Done counter: 221\n",
      "Number of steps: 32\n",
      "Done counter: 222\n",
      "Number of steps: 34\n",
      "Done counter: 223\n",
      "Number of steps: 34\n",
      "Done counter: 224\n",
      "Number of steps: 44\n",
      "Done counter: 225\n",
      "Number of steps: 32\n",
      "Done counter: 226\n",
      "Number of steps: 38\n",
      "Done counter: 227\n",
      "Number of steps: 32\n",
      "Done counter: 228\n",
      "Number of steps: 38\n",
      "Done counter: 229\n",
      "Number of steps: 32\n",
      "Done counter: 230\n",
      "Number of steps: 30\n",
      "Done counter: 231\n",
      "Number of steps: 48\n",
      "Done counter: 232\n",
      "Number of steps: 36\n",
      "Done counter: 233\n",
      "Number of steps: 36\n",
      "Done counter: 234\n",
      "Number of steps: 28\n",
      "Done counter: 235\n",
      "Number of steps: 40\n",
      "Done counter: 236\n",
      "Number of steps: 34\n",
      "Done counter: 237\n",
      "Number of steps: 28\n",
      "Done counter: 238\n",
      "Number of steps: 28\n",
      "Done counter: 239\n",
      "Number of steps: 42\n",
      "Done counter: 240\n",
      "Number of steps: 48\n",
      "Done counter: 241\n",
      "Number of steps: 48\n",
      "Done counter: 242\n",
      "Number of steps: 28\n",
      "Done counter: 243\n",
      "Number of steps: 38\n",
      "Done counter: 244\n",
      "Number of steps: 32\n",
      "Done counter: 245\n",
      "Number of steps: 58\n",
      "Done counter: 246\n",
      "Number of steps: 38\n",
      "Done counter: 247\n",
      "Number of steps: 34\n",
      "Done counter: 248\n",
      "Number of steps: 30\n",
      "Done counter: 249\n",
      "Number of steps: 28\n",
      "Done counter: 250\n",
      "Number of steps: 30\n",
      "Done counter: 251\n",
      "Number of steps: 36\n",
      "Done counter: 252\n",
      "Number of steps: 30\n",
      "Done counter: 253\n",
      "Number of steps: 32\n",
      "Done counter: 254\n",
      "Number of steps: 54\n",
      "Done counter: 255\n",
      "Number of steps: 40\n",
      "Done counter: 256\n",
      "Number of steps: 40\n",
      "Done counter: 257\n",
      "Number of steps: 32\n",
      "Done counter: 258\n",
      "Number of steps: 30\n",
      "Done counter: 259\n",
      "Number of steps: 40\n",
      "Done counter: 260\n",
      "Number of steps: 40\n",
      "Done counter: 261\n",
      "Number of steps: 42\n",
      "Done counter: 262\n",
      "Number of steps: 30\n",
      "Done counter: 263\n",
      "Number of steps: 30\n",
      "Done counter: 264\n",
      "Number of steps: 32\n",
      "Done counter: 265\n",
      "Number of steps: 44\n",
      "Done counter: 266\n",
      "Number of steps: 38\n",
      "Done counter: 267\n",
      "Number of steps: 60\n",
      "Done counter: 268\n",
      "Number of steps: 42\n",
      "Done counter: 269\n",
      "Number of steps: 44\n",
      "average number of steps: 37\n",
      "max: 79\n",
      "min: 28\n",
      "Model saved to: model01/model.pth\n"
     ]
    }
   ],
   "source": [
    "# Huấn luyện mô hình\n",
    "#Khởi tạo môi trường \n",
    "counter = 0\n",
    "step_min = 1000\n",
    "step_max = 0\n",
    "done_count = 0\n",
    "done_statistical = []\n",
    "# Vòng lặp huấn luyện\n",
    "for episode in range(max_episodes):\n",
    "    if counter % env.max_steps == 0:\n",
    "        # Khởi tạo trạng thái môi trường\n",
    "        local_obs, global_obs, position = env.regenerate_maze(buff)\n",
    "    counter += 1\n",
    "    \n",
    "    # Chọn hành động\n",
    "    action = select_action(env, policy_net, local_obs, global_obs, position, device)\n",
    "    \n",
    "    # Tạo buffer cho các hành động\n",
    "    for i in range(4):\n",
    "        next_local_obs, next_global_obs, next_position, reward, d = env.step(i, (action==i))\n",
    "\n",
    "        # Lưu trải nghiệm vào replay buffer\n",
    "        replay_buffer.push((local_obs, global_obs, position, i, reward,\n",
    "                            next_local_obs, next_global_obs, next_position, d))\n",
    "        \n",
    "        # Cập nhật trạng thái\n",
    "        if i == action:\n",
    "            save_local_obs, save_global_obs, save_position = next_local_obs, next_global_obs, next_position\n",
    "            save_done = d\n",
    "\n",
    "    local_obs, global_obs, position = save_local_obs, save_global_obs, save_position\n",
    "    done = save_done\n",
    "\n",
    "    # Huấn luyện mô hình nếu buffer đủ dữ liệu\n",
    "    if len(replay_buffer) >= batch_size:\n",
    "        update_model(policy_net, target_net, replay_buffer, optimizer, batch_size, gamma, device, env.goal_position)\n",
    "\n",
    "    # reset môi trường nếu đạt được mục tiêu\n",
    "    if done:\n",
    "        done_count += 1\n",
    "        print(f\"Done counter: {done_count}\")\n",
    "        print(f\"Number of steps: {counter}\")\n",
    "        if counter > step_max:\n",
    "            step_max = counter\n",
    "        if counter < step_min: \n",
    "            step_min = counter\n",
    "        done_statistical.append(counter)\n",
    "        counter = 0\n",
    "        local_obs, global_obs, position = env.reset(buff)\n",
    "    # Đồng bộ target_net định kỳ\n",
    "    if (episode + 1) % target_update_frequency == 0:\n",
    "        sync_target_network(policy_net, target_net)\n",
    "\n",
    "print(f\"average number of steps: {(max_episodes - counter) // done_count}\")\n",
    "print(f\"max: {step_max}\")\n",
    "print(f\"min: {step_min}\")\n",
    "save_model(policy_net, model_name + \"/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0392cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved buffer successfully saved to model01/replay_buffer.pkl.\n",
      "Replay buffer saved successfully!\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Lưu replay buffer sau khi huấn luyện\n",
    "replay_buffer.save_saved_buffer()  \n",
    "print(\"Replay buffer saved successfully!\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e36787ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hiện biểu đồ thống kê\n",
    "done_statistical = Counter(done_statistical)\n",
    "values = list(done_statistical.keys())  # Các giá trị duy nhất\n",
    "frequencies = list(done_statistical.values())\n",
    "\n",
    "# Vẽ biểu đồ cột\n",
    "plt.bar(values, frequencies, color=\"skyblue\")\n",
    "\n",
    "# Thêm nhãn và tiêu đề\n",
    "plt.xlabel(\"Số bước\")\n",
    "plt.ylabel(\"Tần suất\")\n",
    "plt.title(\"\")\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
