{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ec959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo các thư viện cần thiết\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000311c-41eb-4e8c-9b70-d640e978f328",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Mạng Nơ ron \n",
    "\n",
    "class MazeNetCombined(nn.Module):\n",
    "    def __init__(self, local_size=11, global_size=10, num_actions=4):\n",
    "        super(MazeNetCombined, self).__init__()\n",
    "        \n",
    "        # Quan sát cục bộ\n",
    "        self.conv1_local = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2_local = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3_local = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4_local = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Quan sát toàn cục\n",
    "        self.conv1_global = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2_global = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3_global = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4_global = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully Connected cho vị trí hiện tại\n",
    "        self.fc_position = nn.Linear(2, 32)\n",
    "\n",
    "        # Tầng Fully Connected cuối cùng\n",
    "        self.fc1 = nn.Linear(256 * local_size * local_size + 256 * global_size * global_size + 32, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout_fc = nn.Dropout(p=0.5)  # Dropout trước tầng FC3\n",
    "        self.fc3 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, local_obs, global_obs, position):\n",
    "        # Xử lý local_obs\n",
    "        x_local = F.relu(self.conv1_local(local_obs))\n",
    "        x_local = F.relu(self.conv2_local(x_local))\n",
    "        x_local = F.relu(self.conv3_local(x_local))\n",
    "        x_local = F.relu(self.conv4_local(x_local))\n",
    "        x_local = x_local.view(x_local.size(0), -1)\n",
    "    \n",
    "        # Xử lý global_obs\n",
    "        x_global = F.relu(self.conv1_global(global_obs))\n",
    "        x_global = F.relu(self.conv2_global(x_global))\n",
    "        x_global = F.relu(self.conv3_global(x_global))\n",
    "        x_global = F.relu(self.conv4_global(x_global))\n",
    "        x_global = x_global.view(x_global.size(0), -1)\n",
    "        # Xử lý vị trí hiện tại\n",
    "        x_position = F.relu(self.fc_position(position))\n",
    "\n",
    "        # Kết hợp tất cả\n",
    "        x = torch.cat((x_local, x_global, x_position), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout_fc(x)  # Dropout trước FC3\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e95ff-7bb2-422c-9292-206a310eb14f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "\n",
    "    def push(self, experience):\n",
    "        \"\"\"Thêm một trải nghiệm (local_obs, global_obs, position, action, reward, next_local_obs, next_global_obs, next_position, done) vào replay buffer.\"\"\"\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)  # Loại bỏ trải nghiệm cũ nhất nếu đầy\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Trích xuất batch mẫu từ replay buffer.\"\"\"\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        # Tách dữ liệu thành các phần riêng biệt\n",
    "        local_obs, global_obs, position, actions, rewards, next_local_obs, next_global_obs, next_position, dones = zip(*batch)\n",
    "        return (np.array(local_obs), np.array(global_obs), np.array(position), \n",
    "                np.array(actions), np.array(rewards), np.array(next_local_obs), \n",
    "                np.array(next_global_obs), np.array(next_position), np.array(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6359cf-4e38-4694-8f99-cb823c37e1c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cập nhật model\n",
    "def update_model(policy_net, target_net, replay_buffer, optimizer, batch_size, gamma, device):\n",
    "    \"\"\"\n",
    "    Cập nhật mô hình chính (policy network) cho mạng có đầu vào đa dạng (local_obs, global_obs, position).\n",
    "\n",
    "    Args:\n",
    "    - policy_net (nn.Module): Mạng chính dự đoán giá trị Q(s, a).\n",
    "    - target_net (nn.Module): Mạng mục tiêu dùng để tính Q_target.\n",
    "    - replay_buffer (ReplayBuffer): Bộ nhớ replay chứa các kinh nghiệm (local_obs, global_obs, position, action, reward, next_local_obs, next_global_obs, next_position, done).\n",
    "    - optimizer (torch.optim.Optimizer): Trình tối ưu hóa (Adam, SGD, ...).\n",
    "    - batch_size (int): Kích thước batch mẫu từ replay buffer.\n",
    "    - gamma (float): Hệ số chiết khấu (discount factor).\n",
    "    - device (torch.device): Thiết bị thực thi (CPU hoặc GPU).\n",
    "\n",
    "    Returns:\n",
    "    - loss (float): Giá trị mất mát (loss) sau khi cập nhật.\n",
    "    \"\"\"\n",
    "    # Kiểm tra nếu replay buffer chưa đủ dữ liệu\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # 1. Lấy mẫu từ replay buffer\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    (local_obs, global_obs, position, actions, rewards, \n",
    "     next_local_obs, next_global_obs, next_position, dones) = batch\n",
    "\n",
    "    # 2. Chuyển đổi dữ liệu sang Tensor và đưa vào thiết bị (CPU/GPU)\n",
    "    local_obs = torch.tensor(local_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh cho CNN\n",
    "    global_obs = torch.tensor(global_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh\n",
    "    position = torch.tensor(position, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    next_local_obs = torch.tensor(next_local_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh\n",
    "    next_global_obs = torch.tensor(next_global_obs, dtype=torch.float32).to(device).unsqueeze(1)  # Thêm chiều kênh\n",
    "    next_position = torch.tensor(next_position, dtype=torch.float32).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 3. Dự đoán Q(s, a) từ policy_net\n",
    "    q_values = policy_net(local_obs, global_obs, position)  # Đầu ra: (batch_size, num_actions)\n",
    "    q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Lấy giá trị Q(s, a) cho hành động đã thực hiện\n",
    "\n",
    "    # 4. Tính toán Q_target bằng target_net\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_local_obs, next_global_obs, next_position)  # Dự đoán Q(s', a') từ target_net\n",
    "        max_next_q_values = next_q_values.max(1)[0]  # Lấy giá trị lớn nhất Q(s', a')\n",
    "        q_targets = rewards + gamma * max_next_q_values * (1 - dones)  # Hàm Bellman\n",
    "\n",
    "    # 5. Tính hàm mất mát\n",
    "    loss = F.mse_loss(q_values, q_targets)\n",
    "\n",
    "    # 6. Tối ưu hóa mô hình\n",
    "    optimizer.zero_grad()  # Xóa gradient cũ\n",
    "    loss.backward()  # Lan truyền ngược (backpropagation)\n",
    "    optimizer.step()  # Cập nhật trọng số\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b6527-7fe7-4c1b-bcbc-894d259e3afd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Đồng bộ với mạng mục tiêu\n",
    "def sync_target_network(q_network, target_network):\n",
    "    target_network.load_state_dict(q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2070f-2f24-41e0-bd60-d400df0fbe81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Khởi tạo các siêu tham số\n",
    "import torch.optim as optim\n",
    "\n",
    "# Thiết bị thực thi (CPU hoặc GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.8  # Hệ số chiết khấu (discount factor)\n",
    "learning_rate = 1e-3  # Learning rate cho optimizer\n",
    "weight_decay = 1e-3 # weight decay cho optimizer\n",
    "batch_size = 64  # Kích thước batch khi lấy mẫu từ replay buffer\n",
    "num_epochs = 10000             # Số epoch huấn luyện\n",
    "replay_buffer_capacity = 5000  # Dung lượng bộ nhớ replay buffer\n",
    "\n",
    "# Mạng chính và mạng mục tiêu\n",
    "local_size = 11  # Kích thước quan sát cục bộ\n",
    "global_size = 8  # Kích thước quan sát toàn cục (downsample)\n",
    "num_actions = 4  # Số hành động (lên, xuống, trái, phải)\n",
    "\n",
    "# Khởi tạo mạng chính (policy_net) và mạng mục tiêu (target_net)\n",
    "policy_net = MazeNetCombined(local_size=local_size, global_size=global_size, num_actions=num_actions).to(device)\n",
    "target_net = MazeNetCombined(local_size=local_size, global_size=global_size, num_actions=num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())  # Đồng bộ hóa trọng số ban đầu\n",
    "target_net.eval()  # Đặt target_net ở chế độ đánh giá\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "print(\"Initialization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo replay buffer\n",
    "\n",
    "with open(\"replay_buffer.pkl\", \"rb\") as file:\n",
    "    replay_buffer = pickle.load(file)\n",
    "\n",
    "print(f\"Replay buffer loaded with {len(replay_buffer)} experiences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a797c7-f96e-4e97-a03b-824daaae83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu, tải trọng số của mô hình\n",
    "\n",
    "# Lưu trọng số của mô hình\n",
    "def save_state_dict(model, path=\"model.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model weight was saved to: {path}\")\n",
    "\n",
    "# Tải trọng số của mô hình\n",
    "def load_state_dict(model, target_model, path = \"model.pth\"):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    target_model.load_state_dict(model.state_dict())  # Đồng bộ hóa trọng số ban đầu\n",
    "    target_model.eval() # Đặt target_model ở chế độ đánh giá\n",
    "    print(F\"Model weight was loaded from: {path}\")\n",
    "\n",
    "load_state_dict(policy_net, target_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15819ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện mô hình trên replay buffer\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    loss = update_model(policy_net, target_net, replay_buffer, optimizer, batch_size, gamma, device)\n",
    "    \n",
    "    if loss is not None:\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "    else:\n",
    "        print(\"Replay buffer không đủ dữ liệu để cập nhật mô hình.\")\n",
    "\n",
    "    # Đồng bộ hóa mạng mục tiêu\n",
    "    if epoch % 100 == 0:\n",
    "        sync_target_network(policy_net, target_net)\n",
    "\n",
    "# Lưu trọng số của mô hình\n",
    "save_state_dict(policy_net, path=f\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eff7ae-de11-4018-a5e0-fcf428a9e7a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Xuất mô hình sang định dạng ONNX\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "def export_to_onnx(trained_model, device, local_size=11, global_size=10, num_actions=4, output_file=\"model.onnx\"):\n",
    "    \"\"\"\n",
    "    Xuất mô hình PyTorch đã huấn luyện sang định dạng ONNX.\n",
    "\n",
    "    Args:\n",
    "    - trained_model (nn.Module): Mô hình PyTorch đã huấn luyện.\n",
    "    - local_size (int): Kích thước quan sát cục bộ (ví dụ: 11x11).\n",
    "    - global_size (int): Kích thước quan sát toàn cục (ví dụ: 10x10).\n",
    "    - num_actions (int): Số lượng hành động trong mê cung.\n",
    "    - output_file (str): Đường dẫn tệp để lưu mô hình ONNX.\n",
    "\n",
    "    Returns:\n",
    "    - None: Mô hình ONNX được lưu vào file.\n",
    "    \"\"\"\n",
    "    # Đặt mô hình ở chế độ đánh giá\n",
    "    trained_model.to(device)\n",
    "    trained_model.eval()\n",
    "\n",
    "    # Tạo dữ liệu đầu vào giả (dummy input) để định hình đầu vào\n",
    "    dummy_local_obs = torch.randn(1, 1, local_size, local_size).to(device)  # Local observation\n",
    "    dummy_global_obs = torch.randn(1, 1, global_size, global_size).to(device)  # Global observation\n",
    "    dummy_position = torch.randn(1, 2).to(device)  # Vị trí hiện tại\n",
    "    \n",
    "    # Xuất mô hình sang định dạng ONNX\n",
    "    torch.onnx.export(\n",
    "        trained_model,  # Mô hình đã huấn luyện\n",
    "        (dummy_local_obs, dummy_global_obs, dummy_position),  # Đầu vào\n",
    "        output_file,  # Tên tệp ONNX\n",
    "        input_names=[\"local_obs\", \"global_obs\", \"position\"],  # Tên các đầu vào\n",
    "        output_names=[\"action\"],  # Tên đầu ra\n",
    "        dynamic_axes={\n",
    "            \"local_obs\": {0: \"batch_size\"}, \n",
    "            \"global_obs\": {0: \"batch_size\"}, \n",
    "            \"position\": {0: \"batch_size\"}, \n",
    "            \"action\": {0: \"batch_size\"}\n",
    "        }\n",
    "    )\n",
    "    print(f\"Model exported to ONNX format: {output_file}\")\n",
    "\n",
    "# Xuất mô hình sang ONNX\n",
    "export_to_onnx(target_net, device, local_size, global_size, num_actions, \"maze_net_combined.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
